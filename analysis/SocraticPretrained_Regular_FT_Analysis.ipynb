{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "sGahQhf6_-jY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "# For Preprocessing\n",
        "!pip install -q -U datasets\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import json\n",
        "import random\n",
        "from datasets import Dataset, load_dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "!pip install -q -U torch torchvision torchaudio fastai\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U transformers\n",
        "!pip install -q -U peft\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U tokenizers\n",
        "!pip install -q -U datasets\n",
        "!pip install -q -U evaluate\n",
        "!pip install -q -U rouge_score\n",
        "!pip install -q -U bert_score\n",
        "!pip install -q -U loralib einops xformers\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "import os\n",
        "import bitsandbytes as bnb\n",
        "import pandas as pd\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    PeftConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        ")\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "import evaluate\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from textwrap import fill\n",
        "from transformers import pipeline\n",
        "from sortedcontainers import SortedList\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "pAHIvOeYeE9v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff4367fc-9178-447e-eec0-6bd566fec2fd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m132.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m108.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m133.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.3/235.3 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m121.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m104.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.9/503.9 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.1/117.1 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Prepared Documents\n",
        "\n",
        "These are 10 randomly chosen short stories from the SQuALITY test set, with 2 questions each, for 20 total samples."
      ],
      "metadata": {
        "id": "MI52o9aNnFpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "retrieve_test_samples = []\n",
        "\n",
        "try:\n",
        "  if not retrieve_test_samples:\n",
        "    file_name = \"hallucination_study_test_samples-2025-08-02_171427.pkl\"\n",
        "    with open(f\"/content/drive/MyDrive/DS266/project/analysis/data/{file_name}\", \"rb\") as file:  # \"rb\" for read binary\n",
        "      retrieve_test_samples = pickle.load(file)\n",
        "except Exception as e:\n",
        "  print(f\"Error: {e}\")\n",
        "\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Test samples:\", len(retrieve_test_samples), \"\\n\")\n",
        "retrieve_test_document = retrieve_test_samples[0]\n",
        "retrieve_document_length = len(retrieve_test_document[0].split())\n",
        "retrieve_document_text = fill(retrieve_test_document[0], width=80)\n",
        "retrieve_question_length = len(retrieve_test_document[1].split())\n",
        "retrieve_question_text = retrieve_test_document[1]\n",
        "print(f\"\\033[1mDocument ({retrieve_document_length} words):\\033[0m {retrieve_document_text}\", \"\\n\")\n",
        "print(f\"\\033[1mQuestion ({retrieve_question_length} words):\\033[0m {retrieve_question_text}\", \"\\n\")\n"
      ],
      "metadata": {
        "id": "U6B1mGK4Io6S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a99e6ce3-dfa9-417d-fe74-a50e8e6d3b3d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "\n",
            "Test samples: 5 \n",
            "\n",
            "\u001b[1mDocument (401 words):\u001b[0m HOME IS WHERE YOU LEFT IT By ADAM CHASE [Transcriber Note: This etext was\n",
            "produced from Amazing Stories February 1957. Extensive research did not uncover\n",
            "any evidence that the U.S. copyright on this publication was renewed.] The\n",
            "chance of mass slaughter was their eternal nightmare. How black is the blackest\n",
            "treachery? Is the most callous traitor entitled to mercy? Steve pondered these\n",
            "questions. His decision? That at times the villain should possibly be spoken of\n",
            "as a hero. Only the shells of deserted mud-brick houses greeted Steve Cantwell\n",
            "when he reached the village. He poked around in them for a while. The desert\n",
            "heat was searing, parching, and the Sirian sun gleamed balefully off the blades\n",
            "of Steve's unicopter, which had brought him from Oasis City, almost five hundred\n",
            "miles away. He had remembered heat from his childhood here on Sirius' second\n",
            "planet with the Earth colony, but not heat like this. It was like a magnet\n",
            "drawing all the moisture out of his body. He walked among the buildings,\n",
            "surprise and perhaps sadness etched on his gaunt, weather-beaten face. Childhood\n",
            "memories flooded back: the single well from which all the families drew their\n",
            "water, the mud-brick house, hardly different from the others and just four walls\n",
            "and a roof now, in which he'd lived with his aunt after his parents had been\n",
            "killed in a Kumaji raid, the community center where he'd spent his happiest time\n",
            "as a boy. He went to the well and hoisted up a pailful of water. The winch\n",
            "creaked as he remembered. He ladled out the water, suddenly very thirsty, and\n",
            "brought the ladle to his lips. He hurled the ladle away. The water was bitter.\n",
            "Not brackish. Poisoned. He spat with fury, then kneeled and stuffed his mouth\n",
            "with sand, almost gagging. After a while he spat out the sand too and opened his\n",
            "canteen and rinsed his mouth. His lips and mouth were paralyzed by contact with\n",
            "the poison. He walked quickly across the well-square to his aunt's house.\n",
            "Inside, it was dim but hardly cooler. Steve was sweating, the saline sweat\n",
            "making him blink. He scowled, not understanding. The table was set in his aunt's\n",
            "house. A coffeepot was on the stove and last night's partially-consumed dinner\n",
            "still on the table. The well had been poisoned, the town had been deserted on\n",
            "the spur of the moment, and Steve had returned t \n",
            "\n",
            "\u001b[1mQuestion (5 words):\u001b[0m What did steve cantwell remember? \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    load_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model_name = \"socraticpretraining_baseline-2025-07-26_230206\"\n",
        "\n",
        "model_quantized = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    f\"/content/drive/MyDrive/DS266/project/models/{model_name}\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0})\n",
        "\n",
        "# Tokenizer was unchanged.\n",
        "tokenizer_quantized = AutoTokenizer.from_pretrained(\"Salesforce/squality-socratic-books-30M\")\n",
        "model_quantized = prepare_model_for_kbit_training(model_quantized)\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"k_proj\", \"v_proj\", \"q_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_2_SEQ_LM\",\n",
        "    modules_to_save=[\"lm_head\"]\n",
        ")\n",
        "\n",
        "model_quantized = get_peft_model(model_quantized, config)\n",
        "print(model_quantized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "aa1885fbd62b49249b2521b76807cc46",
            "452eadfcac9a4ef7829e0d8896cdf0b9",
            "1422f4c35be84fd88d12f22e368adb25",
            "5fd5fe743b3a439ea22b335282489abe",
            "a6593a7ed99b4da59966a251fde9d8cf",
            "bc263c141cde48188d4986dc43316180",
            "cd79aa0bd9ba40fd9028e25036d614db",
            "5db62ac537fc4db6bfdd6427d46b7aae",
            "a27bfb63d9c24ea6b5f774474fee04fa",
            "4c3577843d61481b8f87a02b10eef1de",
            "5146fd87137c447a85ded556f1c6ddd0",
            "4e362b2d430a4b1b91a4d84fa5aee4ec",
            "522007e5b6b74e48bf1cc4e4cc26e345",
            "27611472487746b394dcab0708dd99fa",
            "5d5de882fa76432d92dd499cb28172f9",
            "c24b11ec3b6c4be2b96d5cefc4570e20",
            "60c77a2e5f144717aec698d0a651dde4",
            "5ce1e06b52154e03992675cbb233c5ac",
            "dbbd26d1e66f440ca484c2481a51a6eb",
            "d817d4899dac49d9ab520c62cbd09ec5",
            "c57f31c4b20e4d66a676f2b0a4b6ef06",
            "11f66230cb274b249b280cd7ca275180",
            "f6068e002e9b42319c7928527ae6a02b",
            "507348e2ec5b4064acbae455fc1d34cb",
            "45791837a1de41f1b3aac9fb46cbfd0e",
            "2f3795ce24474ea9b94dbe759f23bac0",
            "98b271175cba46f1b599a1013697b6f4",
            "27cc654c421f4e48a7123115abe5c8cf",
            "245e56fe012b4701ba10617f124feb39",
            "5a0c689d3c004caa80369ae07537f8ca",
            "d4d489e41495464986b1fe34e3ea665d",
            "83065a997248452696559d6a7f53a6b1",
            "631fb40e13fa45f3b7b1a9df9c2f9aa3",
            "0cbe852c111b4b43bc8b9e74a98ef0b5",
            "6b0213c1d1f841429765b9a6971625dc",
            "621c14015781454c96f7e9ca776ec8c2",
            "81a50d4f5af749e986e949cdf77911d6",
            "ff617e179549427fa7c918991da600c8",
            "b1fbb7454bda4ac7a052ed10b1032a42",
            "f248b4b59bbc49c6a1873712d470bc35",
            "c4014f5f6e8041e787a6ca5952fa16bb",
            "48eb7e65592f489eab1250e1b8aebc33",
            "5004a4e255404ab5a6c744ca1c36fb4f",
            "6048f39df8ef4d29b58804351c06c089",
            "4c79ee3cf6b345cab33d6bfaad3f0a53",
            "f32c586d3c6544b08bd6009f34140feb",
            "da6f6bce008d48de8ab03779a5f1befc",
            "fd7bc33861674d4db74815ad23a26fb9",
            "7d775e1c682b42f5a9c50236f96ce85d",
            "46448e319b1440cab1f85dd745c36126",
            "30c93f81e702444c830de26a2dd6f2c7",
            "7d90ab938ff24c2ea406b7bac188ea52",
            "ecd5a283edcc4a8c8e689a1496097174",
            "91ba0abf3e55452d992626aea1bc3bdc",
            "03207013ba204cf4a82fe8cba07e77d5",
            "14c4b408c05048a1bece48a23e4c6d12",
            "fcc72ef3b78f4e5b9b689a638c8e8a37",
            "43bcab31d53142b2abf0de21b2f3ffe0",
            "6a380edbf07e42248ba084b5dc6f0b9f",
            "9a2ed5f6e65b4f18b63396e935a5647f",
            "66b9a0008ca64b598553dc0d30fe33ac",
            "5a5439dd97764a7f8d6f29a7807e2cc7",
            "81aea83c7af94e00832449cb6fd2e6b7",
            "b5e62e2b528d4b99a580d8e829bcee67",
            "258dfcbe05ab4abbbd5b5487a522fb44",
            "84c4f735f9714e2ba68ff0fcacef05fc",
            "74fc5cc4fd3c4f24944a8397e81ac746",
            "e00b3b13fbbd40bf9b4f917242f502f3",
            "6fc7e97b6f3b4624a827d22829c362f9",
            "6509e84a41cc42d1b9dbe37cca99ba27",
            "5e89a342cf8940e5970f37484b65444d",
            "5188eb18032848fb9ff31cff8f866495",
            "c82a9c6ae73b43fdb66eb3973c092c41",
            "8f2ede3dacc1450abc02df16d9bd3747",
            "4f5c3ee7352747198fc3426c1a7cf279",
            "1ee8de73622e494989b7f7a4ee7c94dc",
            "3f0dbed3f57a4b1daa8ffb84ad8a1632",
            "9af781ee19cc4206bdbb302eaefc713d",
            "f4363bf646de42beb3413996474a0f0a",
            "361c32568732482bb64698f97c77f987",
            "d4b8836f79b04860bfbce8855d77db58",
            "2381eab66da348baa813b0f3280132dc",
            "489ae5b7d48744a5bdd6a222a3c73c42",
            "04053e0a62fb467baea18c98e8d639d3",
            "83976416a4f2463daacb37ad148509c2",
            "f6194e987f37412c8ea6238745859f58",
            "e40fa5cb21974779956ac4d978f02d61",
            "e34a3741d3024880878d883b5436c64d",
            "4eedc0e3c3374bca9d3dd0815bda92bd",
            "8d84271c9d3c49e1af04f1e4bc0088c9",
            "041a10f77ddc434281ae1f5b82ebfcac",
            "fb7f316988bf400089d379d645ce11d0",
            "135968ff33c74217b924c7cd5aacf8a3",
            "8ee77fe8922e450e89110c9f82c93003",
            "b46854a7c2234584b405b3aaeb158f0f",
            "eba7287a3b7e42b4bd70c0a202adbe47",
            "9e037cdf4f5140bcbccaf996b47702f4",
            "fc32048ab2724e7fbd7686ee9ee3d79b",
            "dd8581d7df1b4d038cdecaed2ea6e1e7"
          ]
        },
        "id": "M3McbIl93ATA",
        "outputId": "98f83f78-ef96-41dc-ce75-91550a6cb88c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa1885fbd62b49249b2521b76807cc46"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e362b2d430a4b1b91a4d84fa5aee4ec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f6068e002e9b42319c7928527ae6a02b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/384 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0cbe852c111b4b43bc8b9e74a98ef0b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c79ee3cf6b345cab33d6bfaad3f0a53"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "14c4b408c05048a1bece48a23e4c6d12"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74fc5cc4fd3c4f24944a8397e81ac746"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9af781ee19cc4206bdbb302eaefc713d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4eedc0e3c3374bca9d3dd0815bda92bd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PeftModelForSeq2SeqLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): BartForConditionalGeneration(\n",
            "      (model): BartModel(\n",
            "        (shared): BartScaledWordEmbedding(50274, 1024, padding_idx=1)\n",
            "        (encoder): BartEncoder(\n",
            "          (embed_tokens): BartScaledWordEmbedding(50274, 1024, padding_idx=1)\n",
            "          (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
            "          (layers): ModuleList(\n",
            "            (0-11): 12 x BartEncoderLayer(\n",
            "              (self_attn): BartAttention(\n",
            "                (k_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (v_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (q_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (activation_fn): GELUActivation()\n",
            "              (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
            "              (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
            "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "          (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (decoder): BartDecoder(\n",
            "          (embed_tokens): BartScaledWordEmbedding(50274, 1024, padding_idx=1)\n",
            "          (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
            "          (layers): ModuleList(\n",
            "            (0-11): 12 x BartDecoderLayer(\n",
            "              (self_attn): BartAttention(\n",
            "                (k_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (v_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (q_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (activation_fn): GELUActivation()\n",
            "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (encoder_attn): BartAttention(\n",
            "                (k_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (v_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (q_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
            "              (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
            "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "          (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (lm_head): ModulesToSaveWrapper(\n",
            "        (original_module): Linear(in_features=1024, out_features=50274, bias=False)\n",
            "        (modules_to_save): ModuleDict(\n",
            "          (default): Linear(in_features=1024, out_features=50274, bias=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions to generate output.\n",
        "from textwrap import fill\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 1024\n",
        "\n",
        "# Hyperparameters\n",
        "generate_kwargs = {\n",
        "    \"num_beams\": 3,\n",
        "    \"do_sample\": True,\n",
        "    \"no_repeat_ngram_size\": 4,\n",
        "    \"max_length\": 512\n",
        "}\n",
        "\n",
        "def get_sentences(text):\n",
        "  return nltk.sent_tokenize(text)\n",
        "\n",
        "def make_question_document_pairs(dataset):\n",
        "    question_document_pairs = []\n",
        "    for document, question in zip(dataset[\"document\"], dataset[\"question\"]):\n",
        "        question_document_pairs.append(f\"<ask&answer> {question} <qsep> {document}\")\n",
        "\n",
        "    return question_document_pairs\n",
        "\n",
        "def generate_output_quantized(model, tokenizer, dataset, batch_size):\n",
        "    input_sentences = make_question_document_pairs(dataset)\n",
        "\n",
        "    all_outputs = []\n",
        "    #pbar = tqdm(range(int(len(input_sentences) / batch_size) + 1))\n",
        "    for i in range(int(len(input_sentences) / batch_size) + 1):\n",
        "        start_i, end_i = i * batch_size, (i + 1) * batch_size\n",
        "        if start_i >= len(input_sentences):\n",
        "            break\n",
        "\n",
        "        #pbar.set_description_str(f\"Samples {start_i} to {end_i}\")\n",
        "\n",
        "        inputs_encoded = tokenizer(\n",
        "            input_sentences[start_i:end_i],\n",
        "            max_length=MAX_SEQUENCE_LENGTH,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors='pt')\n",
        "\n",
        "        output_ids = model.cuda().generate(\n",
        "            input_ids = inputs_encoded['input_ids'].cuda(),\n",
        "            **generate_kwargs)\n",
        "\n",
        "        generated_sentences = tokenizer.batch_decode(output_ids,\n",
        "                                                     skip_special_tokens=True,\n",
        "                                                     clean_up_tokenization_spaces=False)\n",
        "        all_outputs.extend(generated_sentences)\n",
        "\n",
        "    return all_outputs\n",
        "\n",
        "def generate_and_print_output(samples):\n",
        "  for i, (document, question) in enumerate(samples, start=1):\n",
        "    document = fill(document, width=80)\n",
        "    summary = generate_output_quantized(\n",
        "      model_quantized,\n",
        "      tokenizer_quantized,\n",
        "      { \"document\": [document], \"question\": [question] },\n",
        "      16)\n",
        "    response = fill(summary[0], width=80)\n",
        "    d_sentence_count = len(get_sentences(document))\n",
        "    r_sentence_count = len(get_sentences(response))\n",
        "    print(\"\\n\")\n",
        "    print(\"----------------------------------------------------------------\\n\")\n",
        "    print(f\"\\033[1mDocument {i} ({d_sentence_count} sentences):\\033[0m {document}\", \"\\n\")\n",
        "    print(f\"\\033[1mQuestion {i}:\\033[0m {question}\", \"\\n\")\n",
        "    print(f\"\\033[1mResponse {i} ({r_sentence_count} sentences):\\033[0m {response}\", \"\\n\")\n"
      ],
      "metadata": {
        "id": "FggrV0XA5wkO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_and_print_output(retrieve_test_samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5qy5cS6Wlxo",
        "outputId": "c785bec3-8e85-48df-ae64-89a8cbf61af9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "\n",
            "\u001b[1mDocument 1 (32 sentences):\u001b[0m HOME IS WHERE YOU LEFT IT By ADAM CHASE [Transcriber Note: This etext was\n",
            "produced from Amazing Stories February 1957. Extensive research did not uncover\n",
            "any evidence that the U.S. copyright on this publication was renewed.] The\n",
            "chance of mass slaughter was their eternal nightmare. How black is the blackest\n",
            "treachery? Is the most callous traitor entitled to mercy? Steve pondered these\n",
            "questions. His decision? That at times the villain should possibly be spoken of\n",
            "as a hero. Only the shells of deserted mud-brick houses greeted Steve Cantwell\n",
            "when he reached the village. He poked around in them for a while. The desert\n",
            "heat was searing, parching, and the Sirian sun gleamed balefully off the blades\n",
            "of Steve's unicopter, which had brought him from Oasis City, almost five hundred\n",
            "miles away. He had remembered heat from his childhood here on Sirius' second\n",
            "planet with the Earth colony, but not heat like this. It was like a magnet\n",
            "drawing all the moisture out of his body. He walked among the buildings,\n",
            "surprise and perhaps sadness etched on his gaunt, weather-beaten face. Childhood\n",
            "memories flooded back: the single well from which all the families drew their\n",
            "water, the mud-brick house, hardly different from the others and just four walls\n",
            "and a roof now, in which he'd lived with his aunt after his parents had been\n",
            "killed in a Kumaji raid, the community center where he'd spent his happiest time\n",
            "as a boy. He went to the well and hoisted up a pailful of water. The winch\n",
            "creaked as he remembered. He ladled out the water, suddenly very thirsty, and\n",
            "brought the ladle to his lips. He hurled the ladle away. The water was bitter.\n",
            "Not brackish. Poisoned. He spat with fury, then kneeled and stuffed his mouth\n",
            "with sand, almost gagging. After a while he spat out the sand too and opened his\n",
            "canteen and rinsed his mouth. His lips and mouth were paralyzed by contact with\n",
            "the poison. He walked quickly across the well-square to his aunt's house.\n",
            "Inside, it was dim but hardly cooler. Steve was sweating, the saline sweat\n",
            "making him blink. He scowled, not understanding. The table was set in his aunt's\n",
            "house. A coffeepot was on the stove and last night's partially-consumed dinner\n",
            "still on the table. The well had been poisoned, the town had been deserted on\n",
            "the spur of the moment, and Steve had returned t \n",
            "\n",
            "\u001b[1mQuestion 1:\u001b[0m What did steve cantwell remember? \n",
            "\n",
            "\u001b[1mResponse 1 (18 sentences):\u001b[0m Home is where you left it by Adam Chase is where Steve Cantwell left it. When he\n",
            "reached the village where he left it, memories flooded back, childhood memories\n",
            "flooded back: the single well from which all the families drew their water, the\n",
            "mud-brick house that all the families draw their water from, the one well from\n",
            "which everyone drew their water. The town that all families drew water from, and\n",
            "the well poisoned with poison. He went to the well and drank water from the\n",
            "well, and the water was bitter and bitter. The well was poisoned, the town had\n",
            "been deserted on the spur of the moment, and his lips and mouth were paralyzed\n",
            "by contact with the poison. When he returned to his aunt's house, the table was\n",
            "set in last night's partially-consumed dinner still on the stove and last\n",
            "night’s partially consumed dinner was still on the table. He walked to his\n",
            "aunt’s house, but there was a coffeepot and a partially cooked dinner still in\n",
            "the stove. He went back to the community center where he spent his happiest time\n",
            "as a boy, where he spent most of his happiest time. When he got thirsty, he\n",
            "drank the water, the water tasted bitter and bitter, and he threw the ladle away\n",
            "and filled his mouth with sand, almost gagging with sand. When he went back to\n",
            "his village, the well had been poisoned, the water was poisoned, and the town\n",
            "was deserted. When he walked among the deserted houses, he looked around in them\n",
            "for a while, he was surprised and sadness. The desert heat was searing, parching\n",
            "and parching, and the Sirian sun gleamed balefully off the blades of his\n",
            "unicopter, which brought him from Oasis City, almost five hundred miles away. He\n",
            "remembered heat from his childhood here on Sirius' second planet with the Earth\n",
            "colony, but not this kind of heat like this. He walked among the buildings,\n",
            "shocked and sadness, and walked among the mud brick houses. He went among the\n",
            "buildings. Childhood memories flooded back to the single well where all the\n",
            "families lived with his aunt after his parents had been killed in a Kumaji raid,\n",
            "the community center which he'd spent his happiest times as a boy. When he\n",
            "visited the village, the water well was poisoned with poison, the town deserted,\n",
            "and he went to the town’s well-square to get water, the well was poisoned. When\n",
            "he tried to drink it, his mouth was paralyzed by \n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "\n",
            "\u001b[1mDocument 2 (42 sentences):\u001b[0m Wanderers of the Wolf Moon By NELSON S. BOND They were marooned on Titan, their\n",
            "ship wrecked, the radio smashed. Yet they had to exist, had to build a new life\n",
            "on a hostile world. And the man who assumed command was Gregory Malcolm, the\n",
            "bespectacled secretary—whose only adventures had come through the pages of a\n",
            "book. [Transcriber's Note: This etext was produced from Planet Stories Spring\n",
            "1944. Extensive research did not uncover any evidence that the U.S. copyright on\n",
            "this publication was renewed.] Sparks snapped off the switches and followed him\n",
            "to the door of the radio turret. Sparks was a stunted, usually-grinning, little\n",
            "redhead named Hannigan. But he wasn't grinning now. He laid an anxious hand on\n",
            "Greg's arm. \"If I was you,\" he said, \"if I was you, Malcolm, I don't think I'd\n",
            "say nothing to the boss about this. Not just yet, anyhow.\" Greg said, \"Why not?\"\n",
            "Sparks spluttered and fussed and made heavy weather of answering. \"Well, for one\n",
            "thing, it ain't important. It would only worry him. And then there's the\n",
            "womenfolks, they scare easy. Which of course they ain't no cause to.\n",
            "Atmospherics don't mean nothing. I've rode out worse storms than this—plenty of\n",
            "times. And in worse crates than the Carefree .\" Greg studied him carefully from\n",
            "behind trim plasta-rimmed spectacles. He drew a deep breath. He said levelly,\n",
            "\"So it's that bad, eh, Sparks?\" \"What bad? I just told you—\" \"I know. Sparks,\n",
            "I'm not a professional spaceman. But I've studied astrogation as few\n",
            "Earthlubbers have. It's been my hobby for years. And I think I know what we're\n",
            "up against. \"We hit a warp-eddy last night. We've been trapped in a vortex for\n",
            "more than eight hours. Lord only knows how many hundreds of thousands of miles\n",
            "we've been borne off our course. And now we've blasted into a super-ionized belt\n",
            "of atmospherics. Your radio signals are blanketed. You can't get signals in or\n",
            "out. We're a deaf-mute speck of metal being whirled headlong through space.\n",
            "Isn't that it?\" \"I don't know what—\" began Sparks hotly. Then he stopped,\n",
            "studied his companion thoughtfully, nodded. \"O.Q.,\" he confessed, \"that's it.\n",
            "But we ain't licked yet. We got three good men on \n",
            "\n",
            "\u001b[1mQuestion 2:\u001b[0m What did malcolm say? \n",
            "\n",
            "\u001b[1mResponse 2 (20 sentences):\u001b[0m Gregory Malcolm, the man who assumed command of the Wanderers of the Wolf Moon,\n",
            "is a bespectacled secretary whose only adventures had come through the pages of\n",
            "books. Gregory has spent years studying astrogation as few Earthlubbers have. He\n",
            "knows what they are up against, but he doesn't want to tell the boss about it\n",
            "because it would only worry him. Sparks and Hannigan, the other two men on the\n",
            "ship, are worried about the women on the ship because they scare easy. Hannigan\n",
            "says that he doesn't think he would tell his boss about what they are going\n",
            "through because he knows astrogation better than any Earthlubber. He says that\n",
            "atmosphereherics don’t mean nothing, and then there’s womenfolks, who scare\n",
            "easy. He says he knows what they’re up against because he’s studied astrogation\n",
            "for years. He also says that he knows how many hundreds of thousands of miles\n",
            "they’ve been borne off their course. He admits that they haven’t licked yet, but\n",
            "they are a deaf-mute speck of metal being whirled headlong through space. He\n",
            "says they can’t get signals in or out because the super-ionized belt of\n",
            "atmospherics is blanketed. He admits to Sparks that they have three good men on\n",
            "board, but the radio signals are blanketed because of super-ionsized\n",
            "atmospherics. He says the men are afraid of the women because they scare easily.\n",
            "He says to Sparks, “Don’t worry about it, Sparks, it’s not important.” He says\n",
            "that he has been through worse storms than this many times, and in worse crates\n",
            "than the Carefree. He says astrogation is a hobby for years, and he knows what\n",
            "astrogation means. He admits he knows what their situation is up against, and he\n",
            "doesn’t want to say anything to the boss because it would worry the men. He\n",
            "says, “I know what we’ve hit a warp-eddy last night, but I know how many\n",
            "hundreds, thousands of miles we've been borne off our course. The\n",
            "atmosphereherics mean nothing, but they scare easy women. They scare easy\n",
            "because they have no cause to. He says it is important not to say anything about\n",
            "this because the men on board are good men. He explains astrogation and\n",
            "astrogation isn’t important. \n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "\n",
            "\u001b[1mDocument 3 (40 sentences):\u001b[0m THE EXPENDABLES BY JIM HARMON It was just a little black box, useful for getting\n",
            "rid of things. Trouble was, it worked too well! [Transcriber's Note: This etext\n",
            "was produced from Worlds of If Science Fiction, May 1962. Extensive research did\n",
            "not uncover any evidence that the U.S. copyright on this publication was\n",
            "renewed.] \"You see my problem, Professor?\" Tony Carmen held his pinkly\n",
            "manicured, flashily ringed hands wide. I saw his problem and it was warmly\n",
            "embarrassing. \"Really, Mr. Carmen,\" I said, \"this isn't the sort of thing you\n",
            "discuss with a total stranger. I'm not a doctor—not of medicine, anyway—or a\n",
            "lawyer.\" \"They can't help me. I need an operator in your line.\" \"I work for the\n",
            "United States government. I can't become involved in anything illegal.\" Carmen\n",
            "smoothed down the front of his too-tight midnight blue suit and touched the\n",
            "diamond sticking in his silver tie. \"You can't, Professor Venetti? Ever hear of\n",
            "the Mafia?\" \"I've heard of it,\" I said uneasily. \"An old fraternal organization\n",
            "something like the Moose or Rosicrucians, founded in Sicily. It allegedly\n",
            "controls organized crime in the U.S. But that is a responsibility-eluding myth\n",
            "that honest Italian-Americans are stamping out. We don't even like to see the\n",
            "word in print.\" \"I can understand honest Italian-Americans feeling that way. But\n",
            "guys like me know the Mafia is still with it. We can put the squeeze on marks\n",
            "like you pretty easy.\" You don't have to tell even a third generation American\n",
            "about the Mafia. Maybe that was the trouble. I had heard too much and for too\n",
            "long. All the stories I had ever heard about the Mafia, true or false, built up\n",
            "an unendurable threat. \"All right, I'll try to help you, Carmen. But ... that\n",
            "is, you didn't kill any of these people?\" He snorted. \"I haven't killed anybody\n",
            "since early 1943.\" \"Please,\" I said weakly. \"You needn't incriminate yourself\n",
            "with me.\" \"I was in the Marines,\" Carmen said hotly. \"Listen, Professor, these\n",
            "aren't no Prohibition times. Not many people get made for a hit these days.\n",
            "Mother, most of these bodies they keep ditching at my club haven't been murdered\n",
            "by anybody. They're accident victims. Rumbums with \n",
            "\n",
            "\u001b[1mQuestion 3:\u001b[0m What did carmen do? \n",
            "\n",
            "\u001b[1mResponse 3 (27 sentences):\u001b[0m TheTony Carmen is the owner of Tony Carmen's club where bodies of accident\n",
            "victims are dumped at his club, but most of the bodies they keep ditching at his\n",
            "club haven't been murdered by accident victims since early 1943. The bodies they\n",
            "keep dumping at his club are accident victims. The carmen do not kill any of the\n",
            "bodies because they haven't killed anybody since early 1943 since early 1943,\n",
            "and most of the people they keep dumping bodies at his club have been accident\n",
            "victims. They haven't killed any of these people since early 1943 because most\n",
            "of them are accident victims, accident victims. Mr. Tony Carmen claims he hasn't\n",
            "killed anyone since early 1943 and says he hasn't been made for a hit these\n",
            "days. Professor Venetti has heard too much about the Mafia. All the stories\n",
            "about the Mafia, true or false, built up an unendurable threat. He had heard too\n",
            "much and for too long about the Mafia because honest Italian-Americans don't\n",
            "like to see the word in print. The Mafia is still with it, but honest Italian-\n",
            "American people don't even like to see it in print. Mr. Carmen is still with the\n",
            "Mafia. He has been in the Marines, and was in the Marines. He hasn't killed\n",
            "anybody in the Marines since early 1943; most of these bodies aren't accidents.\n",
            "The bodies aren't made for hit these days, most of them get made for hit. Not\n",
            "many people get made for hits these days. He says they get made for accidents\n",
            "victims. He has only accident victims. Carmen do accident victims. He says he\n",
            "haven't killed nobody since early 1943 but has been making hit since early 1943\n",
            "so they haven’t been murdered by anybody. They don’t ditching bodies. He says\n",
            "the bodies aren’t he hasn’t killed anybody since “prohibition times”. He says\n",
            "that most of the body they keep dumping the bodies haven't been ditching at the\n",
            "club haven” since early 1943 “since early 1943. Carmen’s club hasn” since ”not\n",
            "many people get hit these days”. They’re accident victims. Carmen says they\n",
            "haven” accident victims”. When Venetti tries to help Mr. Carmen, he can’t become\n",
            "involved in anything illegal. He doesn’t kill any of these bodies since early\n",
            "1943”. The Professor tries to help him, but Carmen says he hasn”t killed anybody \n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "\n",
            "\u001b[1mDocument 4 (28 sentences):\u001b[0m DEATH STAR By TOM PACE Trapped by the most feared of space pirates Devil\n",
            "Garrett, Starrett Blade was fighting for his life. Weaponless, his ship gone, he\n",
            "was pinning his hopes on a girl—who wanted him dead. [Transcriber's Note: This\n",
            "etext was produced from Planet Stories Spring 1945. Extensive research did not\n",
            "uncover any evidence that the U.S. copyright on this publication was renewed.]\n",
            "Starrett Blade crouched in the rocks by the tiny Centaurian lake. It was only\n",
            "about two or three hundred feet across, but probably thousands of feet deep.\n",
            "This lake, and hundreds of others like it, were the only things to break the\n",
            "monotony of the flat, rocky surface of Alpha Centauri III—called the most barren\n",
            "planet in space. Ten minutes ago, Star Blade's ship had spun into the stagnant\n",
            "waters before him. An emergency release had flung the air-lock doors open, and\n",
            "the air pressure had flung Star out. And now he was waiting for Devil Garrett to\n",
            "come down to the water's edge to search for him. For eight years, Devil Garrett\n",
            "had been the top space pirate in the void. For a year, Star himself had\n",
            "personally been hunting him. And on a tour over Alpha III, a Barden energy-beam\n",
            "had stabbed up at Blade's ship, and Star Blade had crashed into the lake. That\n",
            "Barden Beam had Star worried and puzzled. It took a million volts of power for a\n",
            "split-second flash of the beam. Garrett didn't have an atomics plant on Alpha\n",
            "III—if he had, escaping rays would point it out, no matter how well it was\n",
            "camouflaged. There was no water power, for there was no running water. There\n",
            "were only the lakes ... and tidal power was out, for Alpha III had no moon.\n",
            "However, that could wait. Star slid the electron knife from his water-proof\n",
            "sheath, gripped it firmly. He could hear quick footsteps as a man came down the\n",
            "trail that led directly past his hiding place. It wasn't Garrett, which was\n",
            "disappointing. But it was one of his men, and he was heavily armed. That didn't\n",
            "worry Star. His fighting had earned Starrett Blade the nickname of \"Death Star.\"\n",
            "The man walked to the water's edge, and peered out over the pool. He saw the\n",
            "bubbles that were coming up from the sinking ship, and he nodded, grunted in\n",
            "satisfaction, and started to turn back. S \n",
            "\n",
            "\u001b[1mQuestion 4:\u001b[0m What is the most barren planet in space? \n",
            "\n",
            "\u001b[1mResponse 4 (16 sentences):\u001b[0m The surface of Alpha Centauri III is called the most barren planet in space\n",
            "because it has no running water, no running water power, no moon, tidal power,\n",
            "and tidal power is out. There is no moon on Alpha Centauri III either, for it\n",
            "has no moon. There is a Centaurian lake that is only about two or three hundred\n",
            "feet across, but probably thousands feet deep. This lake and hundreds of others\n",
            "like it are the only things that break the monotony of the flat, rocky surface\n",
            "of the flat flat, rocky Alpha Centauri III surface. The Centaurian lakes break\n",
            "the monotonony of the Centaurian surface. This lake is the only thing that\n",
            "Starrett Blade is fighting for his life against space pirate Devil Garrett.\n",
            "After his ship crashed into the Centaurians lake, Star is fighting for the life\n",
            "of a girl who wants him dead. He has been hunted by Devil Garrett for eight\n",
            "years, and Star himself has personally been hunting him for a year. Devil\n",
            "Garrett has been the top space pirate in the void for eight years. He has no\n",
            "atomics plant on Alpha III, for there is no running water. There is only the\n",
            "lakes and tidal power for Alpha Centauri III to run on. There is also no moon,\n",
            "for Alpha III has no moon at all. There are no running water and tidal power,\n",
            "for there are only lakes. The only running water is on the lakes, for there was\n",
            "no running water on Alpha Centauri II. There are only tidal power and tidal\n",
            "power on Alpha III for there is only tidal power. There is still no moon on the\n",
            "planet. \n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "\n",
            "\u001b[1mDocument 5 (19 sentences):\u001b[0m THE FIRST MAN INTO SPACE Cadet Marshall Farnsworth woke from a nightmare of\n",
            "exploding novae and fouling rockets. After recovering from his fright, he\n",
            "laughed contemptuously at himself. “Here I was picked as the most stable of a\n",
            "group of two hundred cadets,” he thought, “and chosen to make man’s first trip\n",
            "into space, yet I’m shaking like a leaf.” He got out of bed and went over to the\n",
            "window. From his father’s temporary apartment, he could see distant Skyharbor,\n",
            "the scene of the plunge into space tomorrow night. He had been awarded the\n",
            "frightening honor of making that trip. 10 As he watched teardrop cars whip along\n",
            "Phoenix, Arizona’s, double-decked streets, elevated over one another to avoid\n",
            "dangerous intersections and delaying stop lights, he thought back over the\n",
            "years; to the 1950’s, when mice and monkeys were sent up in Vikings to launch\n",
            "mankind’s first probing of the mysterious space beyond Earth, and the first\n",
            "satellites were launched; to the 1960’s, when huger, multiple-stage rockets\n",
            "finally conquered the problem of escape velocity; to 1975—today—when man was\n",
            "finally ready to send one of his own kind into the uninhabited deeps. Marsh\n",
            "climbed back into bed, but sleep would not come. In the adjoining room, he could\n",
            "hear the footsteps of mother and father. By their sound he knew they were the\n",
            "footsteps of worried people. This hurt Marsh more than his own uneasiness. The\n",
            "anxiety had begun for them, he knew, when he had first signed up for space-cadet\n",
            "training. They had known there was an extremely high percentage of washouts, and\n",
            "after each test he passed, they had pretended to be glad. But Marsh knew that\n",
            "inwardly they had hoped he would fail, for they were aware of the ultimate goal\n",
            "that the space scientists were working for—the goal that had just now been\n",
            "reached. Marsh finally fell into a troubled sleep that lasted until morning. He\n",
            "woke early, before the alarm rang. He got up, showered, pulled on his blue-\n",
            "corded cadet uniform, and tugged on the polished gray boots. He took one final\n",
            "look around his room as though in farewell, then went out to the kitchen. 11 His\n",
            "folks were up ahead of time too, trying to act as though it were just another\n",
            "day. Dad was pretending to enjoy his morning paper, nodding only casually to\n",
            "Marsh as he came in. \n",
            "\n",
            "\u001b[1mQuestion 5:\u001b[0m When did marsh think back to? \n",
            "\n",
            "\u001b[1mResponse 5 (19 sentences):\u001b[0m Marsh thinks back over the years when mice and monkeys were sent up in Vikings\n",
            "to launch mankind’s first probing of the mysterious space beyond Earth, and the\n",
            "first probes of space beyond Earth. In 1950’s and 1960’s, monkeys and mice were\n",
            "sent up to explore space beyond Earth in Vikings to probe the unknown space\n",
            "beyond Earth and the first satellites were launched. In the 1960’ s, multiple-\n",
            "stage rockets finally conquered the problem of escape velocity. In 1975, man was\n",
            "finally ready to send one of his own kind into the uninhabited deeps. Marsh’s\n",
            "mother and father were worried about him, and he knew that they were hoping that\n",
            "he would fail. They had known there was an extremely high percentage of\n",
            "washouts, and they pretended to be glad after each test he passed, pretending to\n",
            "be glad. After each test, they pretended to act as though it were just another\n",
            "day. 11 As he watched teardrop cars whip along Phoenix, Arizona’s double-decked\n",
            "Phoenix, Arizona, double decked streets, elevated over one another to avoid\n",
            "dangerous intersections and delaying stop lights, he thinks back over years,\n",
            "when cars were sent to Phoenix, Arizona double decked cars were sent up into the\n",
            "skies to avoid dangerous accidents and delays stop lights. In the 1950's and\n",
            "1960's, mice and monkeys are sent up to launch in Vikings to explore the\n",
            "mysterious space far beyond Earth. The 1960's and the 1960's, multiple stage\n",
            "rockets finally conquered escape velocity, and huger, multiple- stage rockets\n",
            "conquered escape velocity. Today, man is ready to send a cadet into space. He\n",
            "thinks back to 1975, when man is finally ready to go into the unknown deeps.\n",
            "When he wakes up in the morning, he hears his mother and father’s footsteps, and\n",
            "he can hear them worried about him. After he goes back to bed, he goes to the\n",
            "window to see distant Skyharbor, the scene of the plunge into space tomorrow\n",
            "night. He wakes up contemptuously at himself. After hearing his parents’\n",
            "footsteps, he knows that they are worried because they were hoping he would\n",
            "fail, and they are aware of the ultimate goal that the space scientists are\n",
            "working for. He goes back to sleep, but he cannot sleep because he hears the\n",
            "footsteps of worried people. He hears his parents pretending to enjoy their\n",
            "morning paper, but he knows that their anxiety had begun when he signed up for\n",
            "space-cadet training. He goes to his \n",
            "\n"
          ]
        }
      ]
    }
  ]
}
