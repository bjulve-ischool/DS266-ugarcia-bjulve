{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "Da756-v4AEsW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pAHIvOeYeE9v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6daea03e-5f17-4e0c-9fd7-7af9e08406f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m140.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m126.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m112.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.3/235.3 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m136.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m135.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.9/503.9 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.1/117.1 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCK ING'] = \"1\"\n",
        "\n",
        "# For Preprocessing\n",
        "!pip install -q -U datasets\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import random\n",
        "from datasets import Dataset, load_dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "!pip install -q -U torch torchvision torchaudio fastai\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U transformers\n",
        "!pip install -q -U peft\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U tokenizers\n",
        "!pip install -q -U datasets\n",
        "!pip install -q -U evaluate\n",
        "!pip install -q -U rouge_score\n",
        "!pip install -q -U bert_score\n",
        "!pip install -q -U loralib einops xformers\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "import os\n",
        "import bitsandbytes as bnb\n",
        "import pandas as pd\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    PeftConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        ")\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "import evaluate\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from textwrap import fill\n",
        "from transformers import pipeline\n",
        "from sortedcontainers import SortedList\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI52o9aNnFpz"
      },
      "source": [
        "# Load Data\n",
        "\n",
        "Load stories from the test set (SQuALITY) to conduct a manual analysis of hallucinations generated by each model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZX-4RLQM3gc",
        "outputId": "086acf0c-ba76-42b2-cc57-0d3417fc1399"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'DS266-ugarcia-bjulve'...\n",
            "remote: Enumerating objects: 38, done.\u001b[K\n",
            "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 38 (delta 13), reused 4 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (38/38), 3.42 MiB | 4.73 MiB/s, done.\n",
            "Resolving deltas: 100% (13/13), done.\n",
            "DS266-ugarcia-bjulve  sample_data\n",
            "\n",
            "\n",
            "Test documents: 5 \n",
            "\n",
            "\u001b[1mDocument (401 words):\u001b[0m HOME IS WHERE YOU LEFT IT By ADAM CHASE [Transcriber Note: This etext was\n",
            "produced from Amazing Stories February 1957. Extensive research did not uncover\n",
            "any evidence that the U.S. copyright on this publication was renewed.] The\n",
            "chance of mass slaughter was their eternal nightmare. How black is the blackest\n",
            "treachery? Is the most callous traitor entitled to mercy? Steve pondered these\n",
            "questions. His decision? That at times the villain should possibly be spoken of\n",
            "as a hero. Only the shells of deserted mud-brick houses greeted Steve Cantwell\n",
            "when he reached the village. He poked around in them for a while. The desert\n",
            "heat was searing, parching, and the Sirian sun gleamed balefully off the blades\n",
            "of Steve's unicopter, which had brought him from Oasis City, almost five hundred\n",
            "miles away. He had remembered heat from his childhood here on Sirius' second\n",
            "planet with the Earth colony, but not heat like this. It was like a magnet\n",
            "drawing all the moisture out of his body. He walked among the buildings,\n",
            "surprise and perhaps sadness etched on his gaunt, weather-beaten face. Childhood\n",
            "memories flooded back: the single well from which all the families drew their\n",
            "water, the mud-brick house, hardly different from the others and just four walls\n",
            "and a roof now, in which he'd lived with his aunt after his parents had been\n",
            "killed in a Kumaji raid, the community center where he'd spent his happiest time\n",
            "as a boy. He went to the well and hoisted up a pailful of water. The winch\n",
            "creaked as he remembered. He ladled out the water, suddenly very thirsty, and\n",
            "brought the ladle to his lips. He hurled the ladle away. The water was bitter.\n",
            "Not brackish. Poisoned. He spat with fury, then kneeled and stuffed his mouth\n",
            "with sand, almost gagging. After a while he spat out the sand too and opened his\n",
            "canteen and rinsed his mouth. His lips and mouth were paralyzed by contact with\n",
            "the poison. He walked quickly across the well-square to his aunt's house.\n",
            "Inside, it was dim but hardly cooler. Steve was sweating, the saline sweat\n",
            "making him blink. He scowled, not understanding. The table was set in his aunt's\n",
            "house. A coffeepot was on the stove and last night's partially-consumed dinner\n",
            "still on the table. The well had been poisoned, the town had been deserted on\n",
            "the spur of the moment, and Steve had returned t \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Set random seed for reproducibility.\n",
        "RANDOM_SEED = 33\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "#tf.random.set_seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "torch.cuda.manual_seed(RANDOM_SEED)\n",
        "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "\n",
        "# Prep for download.\n",
        "%cd /content/\n",
        "!rm -rf DS266-ugarcia-bjulve\n",
        "!git clone https://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@github.com/bjulve-ischool/DS266-ugarcia-bjulve.git\n",
        "!ls .\n",
        "\n",
        "test_file = 'DS266-ugarcia-bjulve/data/v1-3/test.jsonl'\n",
        "\n",
        "def truncate_tokens(text, max):\n",
        "  tokens = word_tokenize(text)\n",
        "  num_tokens = len(tokens)\n",
        "  truncated = tokens[:max] if num_tokens > max else tokens\n",
        "  reconstructed_text = \" \".join(truncated)\n",
        "  reconstructed_text_length = len(reconstructed_text)\n",
        "  truncated_text = text[:reconstructed_text_length] if reconstructed_text_length < len(text) else text\n",
        "\n",
        "  return truncated_text\n",
        "\n",
        "  # tokens = word_tokenize(text)\n",
        "  # num_tokens = len(tokens)\n",
        "  # truncated = tokens[:max] if num_tokens > max else tokens\n",
        "  # reconstructed_text = \" \".join(truncated)\n",
        "\n",
        "  #return reconstructed_text\n",
        "\n",
        "# Helper to load the data into memory.\n",
        "# NOTE: This method is different from the rest of the other notebooks.\n",
        "#   - First, it will only grab the document column to be used in the manual\n",
        "#     hallucination analysis. The questions will be paired to the documents\n",
        "#     when generating output.\n",
        "#   - Second, the story will be preemptively truncated to about 1024 tokens\n",
        "#     so that all models will have the same inputs to compare in the study.\n",
        "#     This also will make it easier to read the exact content that is being\n",
        "#     fed to each model.\n",
        "#   - Third, a random sample of 10 will be selected at the end. When paired\n",
        "#     with 2 questions each, there will be a total of 20 samples for the study.\n",
        "def load_data(file_path):\n",
        "  with open(file_path) as f:\n",
        "      lines = f.read().split(\"\\n\")[:-1]\n",
        "\n",
        "  documents = []\n",
        "  for line in lines:\n",
        "      data = json.loads(line)\n",
        "      # Remove extra white space. Since the tokenizer is subword\n",
        "      # and not sentence, then the newlines will not likely affect\n",
        "      # the word embedding underlying meaning.\n",
        "      document = \" \".join(data[\"document\"].split())\n",
        "\n",
        "      # Truncate to approximately 450 tokens.\n",
        "      # Max input for models is 1024 including the\n",
        "      # document text, the question text, and the answer\n",
        "      # text. In our case, we'll just truncate to about\n",
        "      # 450 words to simulate the test set evaluation.\n",
        "      # We will instead inspect the output from the model\n",
        "      # manually and not compute metrics against ground\n",
        "      # truth labels.\n",
        "      document = truncate_tokens(document, 450)\n",
        "      documents.append(document)\n",
        "\n",
        "  return random.sample(documents, 5)\n",
        "\n",
        "\n",
        "# Get the data. Print the first sample.\n",
        "test_documents = load_data(test_file)\n",
        "print(\"\\n\")\n",
        "print(\"Test documents:\", len(test_documents), \"\\n\")\n",
        "document_length = len(test_documents[0].split())\n",
        "document_text = fill(test_documents[0], width=80)\n",
        "print(f\"\\033[1mDocument ({document_length} words):\\033[0m {document_text}\", \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Samples\n",
        "\n",
        "Will create 5 total document/question samples from the SQuALITY data set. The question will be generated by MixQG."
      ],
      "metadata": {
        "id": "oVkGjTinXEi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create this data structure to process each document's\n",
        "# sentences in one pass (one loop). See step 1 below.\n",
        "class ScoredSentence(object):\n",
        "    def __init__(self, index, score, sentence):\n",
        "        self.index = index\n",
        "        self.score = score\n",
        "        self.sentence = sentence\n",
        "    def __lt__(self, other):\n",
        "        return self.score < other.score\n",
        "    def __repr__(self):\n",
        "        s = self.sentence\n",
        "        if len(s) > 5:\n",
        "            s = s[:5] + \"...\"\n",
        "        return f\"ScoredSentence(index={self.index}, score={self.score}, sentence='{s}')\"\n",
        "\n",
        "class TopScoredSentences(object):\n",
        "    def __init__(self, m):\n",
        "        self.m = m\n",
        "\n",
        "        # Sorted in reverse order.\n",
        "        sort_scored_sentences = lambda scored_sentence: -1 * scored_sentence.score\n",
        "        self.scored_sentences = SortedList([], key=sort_scored_sentences)\n",
        "    def __repr__(self):\n",
        "        lst = \"\"\n",
        "        for scored_sentence in self.scored_sentences:\n",
        "          lst += f\"{scored_sentence}, \"\n",
        "        lst.rstrip(\",\")\n",
        "        return f\"TopScoredSentences([{lst}])\"\n",
        "    def __len__(self):\n",
        "        return len(self.scored_sentences)\n",
        "    def get_indexes(self):\n",
        "      indexes = []\n",
        "      for scored_sentence in self.scored_sentences:\n",
        "        indexes.append(scored_sentence.index)\n",
        "      indexes.sort()\n",
        "      return indexes\n",
        "    def maintain_top_m(self, ScoredSentence):\n",
        "      # m is the number of sentences you want to keep in\n",
        "      # scored_sentences. The top m scores will be kept.\n",
        "      # If scored_sentences length is >= m, then if ScoredSentence\n",
        "      # is > the last item in the list, then pop off the last\n",
        "      # item and add the new ScoredSentence. Otherwise, if\n",
        "      # scored_sentences length < m, then there's room so just\n",
        "      # add the new ScoredSentence.\n",
        "      if len(self.scored_sentences) >= self.m:\n",
        "        if self.scored_sentences[-1] < ScoredSentence:\n",
        "          self.scored_sentences.pop()\n",
        "          self.scored_sentences.add(ScoredSentence)\n",
        "      else:\n",
        "        self.scored_sentences.add(ScoredSentence)\n",
        "    def get_pseudo_summary(self, truncate=False, truncate_length=256):\n",
        "      pseudo_summary = \"\"\n",
        "      for scored_sentence in self.scored_sentences:\n",
        "        pseudo_summary += scored_sentence.sentence + \" \"\n",
        "      pseudo_summary = pseudo_summary.rstrip()\n",
        "\n",
        "      # Needed for the Pagnoni model which has limited input tokens.\n",
        "      if truncate:\n",
        "        tokens = word_tokenize(pseudo_summary)\n",
        "        num_tokens = len(tokens)\n",
        "        truncated = tokens[:truncate_length] if num_tokens > truncate_length else tokens\n",
        "        reconstructed_text = \" \".join(truncated)\n",
        "        pseudo_summary = reconstructed_text\n",
        "      return pseudo_summary\n",
        "\n",
        "# Get salient sentences.\n",
        "def get_sentences(text):\n",
        "  return nltk.sent_tokenize(text)\n",
        "\n",
        "rouge = evaluate.load('rouge')\n",
        "def compute_rouge1(sentence, text):\n",
        "    rouge_scores = rouge.compute(predictions=[sentence], references=[text])\n",
        "    return rouge_scores[\"rouge1\"]\n",
        "\n",
        "def select_salient_sentences(sentences, text, metric, gsr=.45):\n",
        "  # Compute how many sentences to select.\n",
        "  if type(gsr) == int:\n",
        "    pseudo_summary_sentence_count = gsr\n",
        "  else:\n",
        "    pseudo_summary_sentence_count = int(len(sentences) * gsr)\n",
        "\n",
        "  # Next, for each sentence, assign a metric\n",
        "  # score for the sentence against the rest of\n",
        "  # the text. This score will be used to maintain\n",
        "  # the top m scoring sentences from the document.\n",
        "  top_scored_sentences = TopScoredSentences(pseudo_summary_sentence_count)\n",
        "  i = 0\n",
        "  for sentence in sentences:\n",
        "    score = metric(sentence, text)\n",
        "    top_scored_sentences.maintain_top_m(ScoredSentence(i, score, sentence))\n",
        "    i += 1\n",
        "\n",
        "  return top_scored_sentences\n",
        "\n",
        "mixQG = pipeline(\"text2text-generation\", model='Salesforce/mixqg-base', tokenizer='Salesforce/mixqg-base')\n",
        "\n",
        "def generate_document_question_sample(document_text, top_scored_sentences):\n",
        "  context = top_scored_sentences.get_pseudo_summary()\n",
        "  # Grab the most salient sentence to be the answer for the MixQG model. This is the first\n",
        "  # item in the sorted list since it is sorted by score in descending order.\n",
        "  answer = top_scored_sentences.scored_sentences[0].sentence\n",
        "  question = mixQG(f\"{answer} \\\\n {context}\")[0][\"generated_text\"]\n",
        "  question = question.capitalize()\n",
        "  return (document_text, question)\n",
        "\n",
        "def create_samples(documents):\n",
        "  samples = []\n",
        "  pbar = tqdm(documents)\n",
        "  for document_text in pbar:\n",
        "    pbar.set_description_str(\"Get sentences\")\n",
        "    document_sentences = get_sentences(document_text)\n",
        "    dsl = len(document_sentences)\n",
        "    dtl = len(document_text)\n",
        "    pbar.set_description_str(f\"Select by salience score (s={dsl}, t={dtl})\")\n",
        "    top_scored_sentences = select_salient_sentences(document_sentences, document_text, compute_rouge1, .33)\n",
        "    pbar.set_description_str(f\"Add samples\")\n",
        "\n",
        "    # # The first sample is always the same.\n",
        "    # samples.append((document_text, \"What is the plot of the story?\"))\n",
        "\n",
        "    # The second sample is generated with MixQG.\n",
        "    pbar.set_description_str(f\"Generate question\")\n",
        "    sample = generate_document_question_sample(document_text, top_scored_sentences)\n",
        "    samples.append(sample)\n",
        "\n",
        "  return samples\n",
        "\n",
        "\n",
        "# Now, create the 20 samples.\n",
        "test_samples = create_samples(test_documents)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Test samples:\", len(test_samples), \"\\n\")\n",
        "\n",
        "first_sample = test_samples[0]\n",
        "first_sample_document_length = len(first_sample[0].split())\n",
        "first_sample_document_text = fill(first_sample[0], width=80)\n",
        "first_sample_question_length = len(first_sample[1].split())\n",
        "first_sample_question_text = first_sample[1]\n",
        "print(f\"\\033[1mFirst document ({first_sample_document_length} words):\\033[0m {first_sample_document_text}\", \"\\n\")\n",
        "print(f\"\\033[1mFirst question ({first_sample_question_length} words):\\033[0m {first_sample_question_text}\", \"\\n\")\n",
        "\n",
        "second_sample = test_samples[1]\n",
        "second_sample_document_length = len(second_sample[0].split())\n",
        "second_sample_document_text = fill(second_sample[0], width=80)\n",
        "second_sample_question_length = len(second_sample[1].split())\n",
        "second_sample_question_text = second_sample[1]\n",
        "print(f\"\\033[1mSecond document ({second_sample_document_length} words):\\033[0m {second_sample_document_text}\", \"\\n\")\n",
        "print(f\"\\033[1mSecond question ({second_sample_question_length} words):\\033[0m {second_sample_question_text}\", \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2ce6394dfb494af6bb67227eac1b57f1",
            "7cad2571c1b04c47ae22d403604a0eb6",
            "c7f571a0fe94436dbbcd5ffaf1610bd9",
            "cbc246d4a208402daccb9f81c969aca3",
            "e20f6c8cdf7f45e18a26672f09b914b1",
            "1b596865d3424a72996e9870a06f10d8",
            "54487931ba844549a9db51117c970dd8",
            "2e2500db3e724258923242d5f5021c7b",
            "29de4446d57f4f0883c2fa1b7e3303ab",
            "b473ae2e28474a0d82ffba9110c48c85",
            "cc5c5662bba54bf1ab4334988eea7e45",
            "07cfbb8f07c14922b42630f837bc5a90",
            "c4b7d4c575b8455fb82c1df74cb0dd42",
            "d8293e7bfe214a29b16bdd2550fd5e99",
            "dcdd4d26245a4e189128ff217732a329",
            "98dd8f129b6a477eb16899cb8008fc86",
            "449d24f23de04f5fbf21af11a484ba0f",
            "3944951c3e57436f90f22d9ff78eaf11",
            "7295189fb92b4692804824e2d8dd6faa",
            "c4a323d081ae43589ee23d83cb93e2f1",
            "4dc220733fc64c66bf5f842090b5c905",
            "35e24042853d4588b7e496a7de162cc8",
            "281806e8be674b8bb9fc5caf8b76608a",
            "c3e3a9e8de6a4b4691b819aaa238c1a1",
            "7be95344cfc64a558bab5e2b9f6eb25c",
            "1f13c5486b11474daa2fd5ab3ccb876a",
            "40f908b9b5364004b7a944bb53419e14",
            "644a263c3be04646aae4f020e175d5f6",
            "e1b44528afd94d2aa42eeda12b544f07",
            "6d130b62e8b5495eb8eb4a17bed60a5c",
            "8c0df82c00844422a4a4c075b52dd959",
            "ab07b4e1eb7b406fad2292e676466047",
            "9c73c93eb62f470ca9b6a41c4292b063",
            "09ea3ffbe4534b2291ac16b283ea4036",
            "be6e87f0124349ca9581783b2e1f8604",
            "0e27ab0974d146a8b5f96829d70cd819",
            "3794330141fd4f8ca7fa3bec55f6d67c",
            "375f4f62612948bfbee30337be4404e2",
            "69abe0305a0e41498640cfd1be7a19ae",
            "1ebc8c03018740d2a644f62ae41297ea",
            "c5f92951fdf94c10b6fdda9904a7d292",
            "4aea32c1fa7b42b99b6a84ad156125ff",
            "365b662f9436438c9eb486d6e87085b9",
            "ef5aa1af6d6c45eb87d11c8fab6502be",
            "6375633557e84642bda9637f90cdde60",
            "d9353b03eacc4992a10d2d3ec9459e64",
            "cafcbe7deb9f42f7b9d052d1c6540b61",
            "67e7ee082d9445649f06afa4f026ba5d",
            "e68fd1d5a3a54a1ea2371b07df457631",
            "1879632e0d074ad7aaeae223a01b0448",
            "d7a37b5a29a04637aa9e76fc7874f3a1",
            "5b7bed1469d644fb9838a190ab0418ef",
            "e4ae4225fe55469f94446564c2dfa2b0",
            "221417f9fb7f43978625e15f1c977de4",
            "a8f5032be98742b29432b290b5f83ffa",
            "7def44d86d504d04a8a6734eda4b9eaa",
            "cff4cee394eb4ab897ba01e641bb8d4e",
            "e6956a4b3f6d4bc490d19aa735b88b3c",
            "21d6ce37a54f417f919e7a0ef497119e",
            "f730d92722f647bab1cd475015c6c159",
            "7c3feffa4bbf41979fa593f3bc562a69",
            "3e39b92c82004c7aa9aadd2e86122649",
            "a110d9d1e1414fcfaf66ba0054652644",
            "3b21c67743db4ad5890b072973b11751",
            "bab49a3effdf4a7b9530dd4bb8518138",
            "f3567bfedb334bb08b38ddf1b4b4b10a",
            "f9ee2b3703744ffbbf37bf5300f5a196",
            "681491f30ce24716966847af287fcefc",
            "bdda9e82b8394e2a98bba17d9c7b67cd",
            "a2cc6d5083ef4fe284a98c753eb9a798",
            "73d5fd01a9cd4c50b2fe99337f3718b9",
            "79a6626e9885488d96aa7091ac2380e0",
            "cfd0809b3dcd4d2ab491c039352c496a",
            "92a4ef7ba2644e9fac0085bc7c107d06",
            "fba004201b0e406da4d2ee4ed3d470ce",
            "310e21b1ad09484f88291be0b26a0521",
            "e40a1421b0084dbe941f13bfd2622600",
            "9c06571a76d14ef3b17809218c174101",
            "74039236ec8444f383dcdc1b775ab4b7",
            "de8ba69f206c41ce98f852087ad4a386",
            "4361b951feb84507bdc7e40d610b9ac4",
            "bc86d3f5a03745618cbfe67ba0d46baf",
            "df14252e77ef4937944ed23dd58f33f9",
            "c399990b14e548f3abe6625d3a93221b",
            "237901b768d442269abf830dfce2ccbd",
            "71a42237002c497c98132fe1e4b12b13",
            "c76942ae5f224747aee45a555c09bf48",
            "739a52c2dbe544b3992faf2241191582",
            "0d15fce21b7d424d95bc3fae9f72ebfb",
            "017fe10f10e14c0b8bf156c42479d3c1",
            "b73ca2cde9634a52be9b727179761434",
            "ccc9a6cc0c01481cba5be3f716c99e7a",
            "8ed989c63b2f4548a595c9c1763de00a",
            "c3ee06130d3249a09dd2230ddbca4702",
            "519cc22502e04716b2af462115f32140",
            "22fdf323042e4608aa81cc69e79cbb1f",
            "0952b65cddd34aaeb1fb603e8f235870",
            "6b4a5d3368504f3cad9ae419241c0813",
            "8eed58ef97564998be9bf023cbc667f1"
          ]
        },
        "id": "IthvCgJlXDRm",
        "outputId": "74145a9a-8913-445e-c6ff-4542e02d8cf8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2ce6394dfb494af6bb67227eac1b57f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "07cfbb8f07c14922b42630f837bc5a90"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "281806e8be674b8bb9fc5caf8b76608a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09ea3ffbe4534b2291ac16b283ea4036"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6375633557e84642bda9637f90cdde60"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7def44d86d504d04a8a6734eda4b9eaa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f9ee2b3703744ffbbf37bf5300f5a196"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c06571a76d14ef3b17809218c174101"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d15fce21b7d424d95bc3fae9f72ebfb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Test samples: 5 \n",
            "\n",
            "\u001b[1mFirst document (401 words):\u001b[0m HOME IS WHERE YOU LEFT IT By ADAM CHASE [Transcriber Note: This etext was\n",
            "produced from Amazing Stories February 1957. Extensive research did not uncover\n",
            "any evidence that the U.S. copyright on this publication was renewed.] The\n",
            "chance of mass slaughter was their eternal nightmare. How black is the blackest\n",
            "treachery? Is the most callous traitor entitled to mercy? Steve pondered these\n",
            "questions. His decision? That at times the villain should possibly be spoken of\n",
            "as a hero. Only the shells of deserted mud-brick houses greeted Steve Cantwell\n",
            "when he reached the village. He poked around in them for a while. The desert\n",
            "heat was searing, parching, and the Sirian sun gleamed balefully off the blades\n",
            "of Steve's unicopter, which had brought him from Oasis City, almost five hundred\n",
            "miles away. He had remembered heat from his childhood here on Sirius' second\n",
            "planet with the Earth colony, but not heat like this. It was like a magnet\n",
            "drawing all the moisture out of his body. He walked among the buildings,\n",
            "surprise and perhaps sadness etched on his gaunt, weather-beaten face. Childhood\n",
            "memories flooded back: the single well from which all the families drew their\n",
            "water, the mud-brick house, hardly different from the others and just four walls\n",
            "and a roof now, in which he'd lived with his aunt after his parents had been\n",
            "killed in a Kumaji raid, the community center where he'd spent his happiest time\n",
            "as a boy. He went to the well and hoisted up a pailful of water. The winch\n",
            "creaked as he remembered. He ladled out the water, suddenly very thirsty, and\n",
            "brought the ladle to his lips. He hurled the ladle away. The water was bitter.\n",
            "Not brackish. Poisoned. He spat with fury, then kneeled and stuffed his mouth\n",
            "with sand, almost gagging. After a while he spat out the sand too and opened his\n",
            "canteen and rinsed his mouth. His lips and mouth were paralyzed by contact with\n",
            "the poison. He walked quickly across the well-square to his aunt's house.\n",
            "Inside, it was dim but hardly cooler. Steve was sweating, the saline sweat\n",
            "making him blink. He scowled, not understanding. The table was set in his aunt's\n",
            "house. A coffeepot was on the stove and last night's partially-consumed dinner\n",
            "still on the table. The well had been poisoned, the town had been deserted on\n",
            "the spur of the moment, and Steve had returned t \n",
            "\n",
            "\u001b[1mFirst question (5 words):\u001b[0m What did steve cantwell remember? \n",
            "\n",
            "\u001b[1mSecond document (370 words):\u001b[0m Wanderers of the Wolf Moon By NELSON S. BOND They were marooned on Titan, their\n",
            "ship wrecked, the radio smashed. Yet they had to exist, had to build a new life\n",
            "on a hostile world. And the man who assumed command was Gregory Malcolm, the\n",
            "bespectacled secretary—whose only adventures had come through the pages of a\n",
            "book. [Transcriber's Note: This etext was produced from Planet Stories Spring\n",
            "1944. Extensive research did not uncover any evidence that the U.S. copyright on\n",
            "this publication was renewed.] Sparks snapped off the switches and followed him\n",
            "to the door of the radio turret. Sparks was a stunted, usually-grinning, little\n",
            "redhead named Hannigan. But he wasn't grinning now. He laid an anxious hand on\n",
            "Greg's arm. \"If I was you,\" he said, \"if I was you, Malcolm, I don't think I'd\n",
            "say nothing to the boss about this. Not just yet, anyhow.\" Greg said, \"Why not?\"\n",
            "Sparks spluttered and fussed and made heavy weather of answering. \"Well, for one\n",
            "thing, it ain't important. It would only worry him. And then there's the\n",
            "womenfolks, they scare easy. Which of course they ain't no cause to.\n",
            "Atmospherics don't mean nothing. I've rode out worse storms than this—plenty of\n",
            "times. And in worse crates than the Carefree .\" Greg studied him carefully from\n",
            "behind trim plasta-rimmed spectacles. He drew a deep breath. He said levelly,\n",
            "\"So it's that bad, eh, Sparks?\" \"What bad? I just told you—\" \"I know. Sparks,\n",
            "I'm not a professional spaceman. But I've studied astrogation as few\n",
            "Earthlubbers have. It's been my hobby for years. And I think I know what we're\n",
            "up against. \"We hit a warp-eddy last night. We've been trapped in a vortex for\n",
            "more than eight hours. Lord only knows how many hundreds of thousands of miles\n",
            "we've been borne off our course. And now we've blasted into a super-ionized belt\n",
            "of atmospherics. Your radio signals are blanketed. You can't get signals in or\n",
            "out. We're a deaf-mute speck of metal being whirled headlong through space.\n",
            "Isn't that it?\" \"I don't know what—\" began Sparks hotly. Then he stopped,\n",
            "studied his companion thoughtfully, nodded. \"O.Q.,\" he confessed, \"that's it.\n",
            "But we ain't licked yet. We got three good men on \n",
            "\n",
            "\u001b[1mSecond question (4 words):\u001b[0m What did malcolm say? \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Random sample.\n",
        "\n",
        "even_indexed_elements = [test_samples[i] for i in range(len(test_samples)) if i % 2 != 0]\n",
        "random_sample = random.choice(even_indexed_elements)\n",
        "random_sample_document_length = len(random_sample[0].split())\n",
        "random_sample_document_text = fill(random_sample[0], width=80)\n",
        "random_sample_question_length = len(random_sample[1].split())\n",
        "random_sample_question_text = random_sample[1]\n",
        "print(f\"\\033[1mRandom document ({random_sample_document_length} words):\\033[0m {random_sample_document_text}\", \"\\n\")\n",
        "print(f\"\\033[1mRandom question ({random_sample_question_length} words):\\033[0m {random_sample_question_text}\", \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNiNDZPQndIN",
        "outputId": "90e70466-36e4-444e-87e1-2d0dd9fa6750"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mRandom document (399 words):\u001b[0m DEATH STAR By TOM PACE Trapped by the most feared of space pirates Devil\n",
            "Garrett, Starrett Blade was fighting for his life. Weaponless, his ship gone, he\n",
            "was pinning his hopes on a girl—who wanted him dead. [Transcriber's Note: This\n",
            "etext was produced from Planet Stories Spring 1945. Extensive research did not\n",
            "uncover any evidence that the U.S. copyright on this publication was renewed.]\n",
            "Starrett Blade crouched in the rocks by the tiny Centaurian lake. It was only\n",
            "about two or three hundred feet across, but probably thousands of feet deep.\n",
            "This lake, and hundreds of others like it, were the only things to break the\n",
            "monotony of the flat, rocky surface of Alpha Centauri III—called the most barren\n",
            "planet in space. Ten minutes ago, Star Blade's ship had spun into the stagnant\n",
            "waters before him. An emergency release had flung the air-lock doors open, and\n",
            "the air pressure had flung Star out. And now he was waiting for Devil Garrett to\n",
            "come down to the water's edge to search for him. For eight years, Devil Garrett\n",
            "had been the top space pirate in the void. For a year, Star himself had\n",
            "personally been hunting him. And on a tour over Alpha III, a Barden energy-beam\n",
            "had stabbed up at Blade's ship, and Star Blade had crashed into the lake. That\n",
            "Barden Beam had Star worried and puzzled. It took a million volts of power for a\n",
            "split-second flash of the beam. Garrett didn't have an atomics plant on Alpha\n",
            "III—if he had, escaping rays would point it out, no matter how well it was\n",
            "camouflaged. There was no water power, for there was no running water. There\n",
            "were only the lakes ... and tidal power was out, for Alpha III had no moon.\n",
            "However, that could wait. Star slid the electron knife from his water-proof\n",
            "sheath, gripped it firmly. He could hear quick footsteps as a man came down the\n",
            "trail that led directly past his hiding place. It wasn't Garrett, which was\n",
            "disappointing. But it was one of his men, and he was heavily armed. That didn't\n",
            "worry Star. His fighting had earned Starrett Blade the nickname of \"Death Star.\"\n",
            "The man walked to the water's edge, and peered out over the pool. He saw the\n",
            "bubbles that were coming up from the sinking ship, and he nodded, grunted in\n",
            "satisfaction, and started to turn back. S \n",
            "\n",
            "\u001b[1mRandom question (8 words):\u001b[0m What is the most barren planet in space? \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Data"
      ],
      "metadata": {
        "id": "sWSSwemyXGNL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gTaL5HWjK0A",
        "outputId": "d136bcd4-a0a9-41f1-e80a-c48ff7031328"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Save to reimport later if needed.\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "!pwd\n",
        "!mkdir -p ./analysis/data\n",
        "\n",
        "# Get the current time in the US Pacific time zone.\n",
        "timezone_obj = ZoneInfo(\"America/Los_Angeles\")\n",
        "current_time = datetime.now(timezone_obj)\n",
        "current_time = current_time.strftime(\"%Y-%m-%d_%H%M%S\")\n",
        "\n",
        "test_samples_name = \"hallucination_study_test_samples-\" + str(current_time) + \".pkl\"\n",
        "with open(f\"./analysis/data/{test_samples_name}\", \"wb\") as f:\n",
        "  pickle.dump(test_samples, f)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir -p \"/content/drive/MyDrive/DS266/project/analysis/data\"\n",
        "!cp ./analysis/data/{test_samples_name} \"/content/drive/MyDrive/DS266/project/analysis/data/{test_samples_name}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYapbV3v4ZzK",
        "outputId": "8d8403e5-7466-45f2-a702-19b0f4dfcbbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "\n",
            "Test samples: 5 \n",
            "\n",
            "Sample 1: 32 sentences.\n",
            "\n",
            "Sample 2: 42 sentences.\n",
            "\n",
            "Sample 3: 40 sentences.\n",
            "\n",
            "Sample 4: 28 sentences.\n",
            "\n",
            "Sample 5: 19 sentences.\n",
            "\n",
            "\u001b[1mDocument (401 words):\u001b[0m HOME IS WHERE YOU LEFT IT By ADAM CHASE [Transcriber Note: This etext was\n",
            "produced from Amazing Stories February 1957. Extensive research did not uncover\n",
            "any evidence that the U.S. copyright on this publication was renewed.] The\n",
            "chance of mass slaughter was their eternal nightmare. How black is the blackest\n",
            "treachery? Is the most callous traitor entitled to mercy? Steve pondered these\n",
            "questions. His decision? That at times the villain should possibly be spoken of\n",
            "as a hero. Only the shells of deserted mud-brick houses greeted Steve Cantwell\n",
            "when he reached the village. He poked around in them for a while. The desert\n",
            "heat was searing, parching, and the Sirian sun gleamed balefully off the blades\n",
            "of Steve's unicopter, which had brought him from Oasis City, almost five hundred\n",
            "miles away. He had remembered heat from his childhood here on Sirius' second\n",
            "planet with the Earth colony, but not heat like this. It was like a magnet\n",
            "drawing all the moisture out of his body. He walked among the buildings,\n",
            "surprise and perhaps sadness etched on his gaunt, weather-beaten face. Childhood\n",
            "memories flooded back: the single well from which all the families drew their\n",
            "water, the mud-brick house, hardly different from the others and just four walls\n",
            "and a roof now, in which he'd lived with his aunt after his parents had been\n",
            "killed in a Kumaji raid, the community center where he'd spent his happiest time\n",
            "as a boy. He went to the well and hoisted up a pailful of water. The winch\n",
            "creaked as he remembered. He ladled out the water, suddenly very thirsty, and\n",
            "brought the ladle to his lips. He hurled the ladle away. The water was bitter.\n",
            "Not brackish. Poisoned. He spat with fury, then kneeled and stuffed his mouth\n",
            "with sand, almost gagging. After a while he spat out the sand too and opened his\n",
            "canteen and rinsed his mouth. His lips and mouth were paralyzed by contact with\n",
            "the poison. He walked quickly across the well-square to his aunt's house.\n",
            "Inside, it was dim but hardly cooler. Steve was sweating, the saline sweat\n",
            "making him blink. He scowled, not understanding. The table was set in his aunt's\n",
            "house. A coffeepot was on the stove and last night's partially-consumed dinner\n",
            "still on the table. The well had been poisoned, the town had been deserted on\n",
            "the spur of the moment, and Steve had returned t \n",
            "\n",
            "\u001b[1mQuestion (5 words):\u001b[0m What did steve cantwell remember? \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "retrieve_test_samples = []\n",
        "\n",
        "try:\n",
        "  if not retrieve_test_samples:\n",
        "    file_name = \"hallucination_study_test_samples-2025-08-02_171427.pkl\"\n",
        "    with open(f\"/content/drive/MyDrive/DS266/project/analysis/data/{file_name}\", \"rb\") as file:  # \"rb\" for read binary\n",
        "      retrieve_test_samples = pickle.load(file)\n",
        "except Exception as e:\n",
        "  print(f\"Error: {e}\")\n",
        "\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Test samples:\", len(retrieve_test_samples), \"\\n\")\n",
        "for i, sample in enumerate(retrieve_test_samples, start=1):\n",
        "  sentence_count = len(get_sentences(sample[0]))\n",
        "  print(f\"Sample {i}: {sentence_count} sentences.\\n\")\n",
        "\n",
        "retrieve_test_document = retrieve_test_samples[0]\n",
        "retrieve_document_length = len(retrieve_test_document[0].split())\n",
        "retrieve_document_text = fill(retrieve_test_document[0], width=80)\n",
        "retrieve_question_length = len(retrieve_test_document[1].split())\n",
        "retrieve_question_text = retrieve_test_document[1]\n",
        "print(f\"\\033[1mDocument ({retrieve_document_length} words):\\033[0m {retrieve_document_text}\", \"\\n\")\n",
        "print(f\"\\033[1mQuestion ({retrieve_question_length} words):\\033[0m {retrieve_question_text}\", \"\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
