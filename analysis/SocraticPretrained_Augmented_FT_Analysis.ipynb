{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "sGahQhf6_-jY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "# For Preprocessing\n",
        "!pip install -q -U datasets\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import json\n",
        "import random\n",
        "from datasets import Dataset, load_dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "!pip install -q -U torch torchvision torchaudio fastai\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U transformers\n",
        "!pip install -q -U peft\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U tokenizers\n",
        "!pip install -q -U datasets\n",
        "!pip install -q -U evaluate\n",
        "!pip install -q -U rouge_score\n",
        "!pip install -q -U bert_score\n",
        "!pip install -q -U loralib einops xformers\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "import os\n",
        "import bitsandbytes as bnb\n",
        "import pandas as pd\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    PeftConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        "    PeftModel,\n",
        ")\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "import evaluate\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from textwrap import fill\n",
        "from transformers import pipeline\n",
        "from sortedcontainers import SortedList\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "pAHIvOeYeE9v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc9a1b1b-feaf-4e83-beb2-c6cd7a3e7dd6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m127.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m116.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m108.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.3/235.3 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m127.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m115.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.9/503.9 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.1/117.1 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Prepared Documents\n",
        "\n",
        "These are 10 randomly chosen short stories from the SQuALITY test set, with 2 questions each, for 20 total samples."
      ],
      "metadata": {
        "id": "MI52o9aNnFpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "retrieve_test_samples = []\n",
        "\n",
        "try:\n",
        "  if not retrieve_test_samples:\n",
        "    file_name = \"hallucination_study_test_samples-2025-08-02_171427.pkl\"\n",
        "    with open(f\"/content/drive/MyDrive/DS266/project/analysis/data/{file_name}\", \"rb\") as file:  # \"rb\" for read binary\n",
        "      retrieve_test_samples = pickle.load(file)\n",
        "except Exception as e:\n",
        "  print(f\"Error: {e}\")\n",
        "\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Test samples:\", len(retrieve_test_samples), \"\\n\")\n",
        "retrieve_test_document = retrieve_test_samples[0]\n",
        "retrieve_document_length = len(retrieve_test_document[0].split())\n",
        "retrieve_document_text = fill(retrieve_test_document[0], width=80)\n",
        "retrieve_question_length = len(retrieve_test_document[1].split())\n",
        "retrieve_question_text = retrieve_test_document[1]\n",
        "print(f\"\\033[1mDocument ({retrieve_document_length} words):\\033[0m {retrieve_document_text}\", \"\\n\")\n",
        "print(f\"\\033[1mQuestion ({retrieve_question_length} words):\\033[0m {retrieve_question_text}\", \"\\n\")\n"
      ],
      "metadata": {
        "id": "U6B1mGK4Io6S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1172192c-64e8-400d-e88b-087cd676f162"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "\n",
            "Test samples: 5 \n",
            "\n",
            "\u001b[1mDocument (401 words):\u001b[0m HOME IS WHERE YOU LEFT IT By ADAM CHASE [Transcriber Note: This etext was\n",
            "produced from Amazing Stories February 1957. Extensive research did not uncover\n",
            "any evidence that the U.S. copyright on this publication was renewed.] The\n",
            "chance of mass slaughter was their eternal nightmare. How black is the blackest\n",
            "treachery? Is the most callous traitor entitled to mercy? Steve pondered these\n",
            "questions. His decision? That at times the villain should possibly be spoken of\n",
            "as a hero. Only the shells of deserted mud-brick houses greeted Steve Cantwell\n",
            "when he reached the village. He poked around in them for a while. The desert\n",
            "heat was searing, parching, and the Sirian sun gleamed balefully off the blades\n",
            "of Steve's unicopter, which had brought him from Oasis City, almost five hundred\n",
            "miles away. He had remembered heat from his childhood here on Sirius' second\n",
            "planet with the Earth colony, but not heat like this. It was like a magnet\n",
            "drawing all the moisture out of his body. He walked among the buildings,\n",
            "surprise and perhaps sadness etched on his gaunt, weather-beaten face. Childhood\n",
            "memories flooded back: the single well from which all the families drew their\n",
            "water, the mud-brick house, hardly different from the others and just four walls\n",
            "and a roof now, in which he'd lived with his aunt after his parents had been\n",
            "killed in a Kumaji raid, the community center where he'd spent his happiest time\n",
            "as a boy. He went to the well and hoisted up a pailful of water. The winch\n",
            "creaked as he remembered. He ladled out the water, suddenly very thirsty, and\n",
            "brought the ladle to his lips. He hurled the ladle away. The water was bitter.\n",
            "Not brackish. Poisoned. He spat with fury, then kneeled and stuffed his mouth\n",
            "with sand, almost gagging. After a while he spat out the sand too and opened his\n",
            "canteen and rinsed his mouth. His lips and mouth were paralyzed by contact with\n",
            "the poison. He walked quickly across the well-square to his aunt's house.\n",
            "Inside, it was dim but hardly cooler. Steve was sweating, the saline sweat\n",
            "making him blink. He scowled, not understanding. The table was set in his aunt's\n",
            "house. A coffeepot was on the stove and last night's partially-consumed dinner\n",
            "still on the table. The well had been poisoned, the town had been deserted on\n",
            "the spur of the moment, and Steve had returned t \n",
            "\n",
            "\u001b[1mQuestion (5 words):\u001b[0m What did steve cantwell remember? \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    load_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "socratic_checkpoint_name = \"Salesforce/squality-socratic-books-30M\"\n",
        "socratic_model_quantized = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    socratic_checkpoint_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0})\n",
        "socratic_tokenizer_quantized = AutoTokenizer.from_pretrained(socratic_checkpoint_name)\n",
        "socratic_model_config_quantized = AutoConfig.from_pretrained(socratic_checkpoint_name)\n",
        "\n",
        "# Add special tokens for Socratic FT.\n",
        "special_tokens = [\"[Ask&Answer]\", \"[Mask]\", \"[QSep]\"]\n",
        "num_added_tokens = socratic_tokenizer_quantized.add_special_tokens({'additional_special_tokens': special_tokens})\n",
        "print(f\"Added {num_added_tokens} tokens to the tokenizer.\")\n",
        "\n",
        "# Resize the model accordingly.\n",
        "print(\"Resized model vocab:\", socratic_model_config_quantized.vocab_size, len(socratic_tokenizer_quantized))\n",
        "socratic_model_quantized.resize_token_embeddings(max(\n",
        "    len(socratic_tokenizer_quantized),\n",
        "    socratic_model_config_quantized.vocab_size))\n",
        "\n",
        "# Load fine tuned model.\n",
        "model_name = \"socraticpretraining_augmented-2025-07-28_095952\"\n",
        "model_quantized = PeftModel.from_pretrained(\n",
        "    socratic_model_quantized,\n",
        "    f\"/content/drive/MyDrive/DS266/project/models/{model_name}\"\n",
        "    )\n",
        "\n",
        "# In this case the tokenizer was changed.\n",
        "tokenizer_quantized = socratic_tokenizer_quantized\n",
        "\n",
        "print(model_quantized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3cbd98d4af3249fe8bec7856899365d4",
            "2fa7179789f14a7396431e616d77a0df",
            "2d41eab8e97f468aa9120ca5e404a41d",
            "6964985d1c8e441ea6349634f232398a",
            "9d0b43788233415783addfff899f8162",
            "2a217570df8643c6a50ed676ff3ccfb4",
            "3c7f3b7ef96044b99ebfcc558bc29ab6",
            "5e6b05161f1b41f38bad84dd677054cb",
            "c91dfe6119464039b759c8e9dc6e483c",
            "ef03851f2e57467bbe7a5ee5d926c814",
            "54e783fff7c94fb7a92f5fcb2cbea5e5",
            "44db1c2ba66e4bc8a02902ce7f4bad95",
            "d21537b85e8b429d84feea1ba1ad2e30",
            "40f45ac2b3174b9da2a6d8b7f7b53737",
            "5f19ec2acfcf427294ed807429dd833c",
            "f0faffa249964cdb86267173d36c4f3c",
            "7baf507b2d404600ac9975afaa8ccb7a",
            "40dce12abb304803b7f59cae71fc0912",
            "45a05990d87b474a84c538182ecfa733",
            "9ddff0174edd45d5ac8fa0ad4785324a",
            "e620cfd28b2f4dea852886fe401deb7d",
            "960ab53a911840b8a24fab226da5c3e3",
            "6f10f660f848490ab1cc0ff653ee3af1",
            "f94d1c44fc7149a4b1d29172fd595871",
            "045257151eb04d4ba9e51c90c73f52ae",
            "5cd244fc2b5544c8bc62dab22e50cd94",
            "ed9aaf9d8b66489ca0492389f301ab58",
            "410de76f23e34b69bd36918ed2235a66",
            "a962abfcb9514abea11f05c22671693c",
            "0dc5672290a747ab99ff4e6738f83c20",
            "5e702f46f67640e580f89d38c3762089",
            "ef128be7e93440bdb502e8e4d0fe1d39",
            "2665e267c2804b4691ed4aaeb330daa0",
            "9dfeb3ac4179444abe2b532767edc7af",
            "40dccf761fe142f1a7d661b0599b7f7b",
            "dcbc4e0ff81c4fa7a050dca51ab30ebf",
            "f7e2b24014134814a7d5ef10f8998b1b",
            "eba7c349eee0434d92efa1c82dd9ee2e",
            "fa1a6c7e802c4f22bd75b737eedced9f",
            "e41743fad226486ba0a79e7c86d88db3",
            "199fd40a32ad458283bbed88a454a5f5",
            "e0aa69647ccd4961b284b7abe2028db3",
            "a2046877864f432e93fb05bea71166bf",
            "64066bbfef0b40f68ccd2706df8b20e8",
            "9d0aec3cb0414935b177f9aba5422889",
            "91ecc2cd162d4565a43db676bbb4c4cb",
            "7f4e4723c9c84e259a10a9c981a058de",
            "97efd6bcc97f46a8a3b3c78ac813428b",
            "0f3726f558444099b78e72bfd017faea",
            "51e120b7ebab41ea9c7e3950126ce379",
            "08c453a61fb240aab435dc7f25a580a3",
            "19684543e7c44f598714da643b68fbc0",
            "abbe492de1bd48e0bf18018b05769be2",
            "c198c73f726a43089b595d68e74cf7c3",
            "565ce71a6a8e458e939674214b8827df",
            "3203c62a7e7c446b9fc91636716fe892",
            "c5d242abeea441e7b82fcc5cfdb878ad",
            "5228fc255238445c9173c188d281b19d",
            "63e84c3497fa491aa2e2157e98de860e",
            "57233dcbedb44c20a29f6407dd90ee78",
            "9af2eccadfbb4dccaab340fc7d97e036",
            "03f0d99bfdb541b2b328e671ec1b4d33",
            "664d2d7dfb5748288c1a27578cda8777",
            "670ae66599b44ad3be8b1902fb2e7018",
            "91a897dfc1424257bfeac8bdd1c1ccce",
            "35214a8ff0294dfa845af70bbd0a935e",
            "1d237c5c96774dbe834ebb20772812ad",
            "da7a3e5b167a42a28f9901607e8207f3",
            "dbdc63ea0691493d9b2239ae7dd1e063",
            "ba9d6f7e18e8414491489a6d54bd565f",
            "d6760e92607f4bbd9012ae4f3dfdbe61",
            "7fd46e82bbf048d6b1a83efc8d035ca6",
            "cd1621ade28a4c0085c6e2663cb642d7",
            "08381c771c1a4ba4aa529b29ef7ae162",
            "3163fe30d60846c2a9bcfd704c889df7",
            "41b1b5570933475a9e2368fd218ab0e1",
            "132abb43ba2d460e98125c123312ea84",
            "b5e182c564794099922abc26213fd77d",
            "21e84a40e479486bb1c58844fc87dbd1",
            "38e2b280628347f2a3bb6760934366c0",
            "346d85ed7a8b4f15b59bb3b00e0169f1",
            "66e0133d952a4978bbed68300450f158",
            "8769f1c844bf4a2c93193c2a5da8fba3",
            "c65ba0b8d268456ebfc7ddec2ffb9be9",
            "a5d2bc4a2835498baa10017e59b5a4f3",
            "34f00aa2ca5d4f8182856ec2b26c4229",
            "685a5a560e11475092bb44945b2707ef",
            "6c7c27217bca40ffb74c1afc4f22c649",
            "7121c741788d4f08a5abe6c5f3af6267",
            "d6ae1058c5aa43858fd31999f7ad540f",
            "73a35107a96b40c2afa34013d93a47c3",
            "4e6640da20eb43b383a7281f5ac0c18d",
            "6bcfc5045a694d6890bdd98cd9ea0054",
            "e511b39531214aa7b11a6150e1ace937",
            "b32c0ce4b3d646ecbb8707e5d9732893",
            "2f1cda468b0149459058c52c81b47a9b",
            "d582b59f1526484aa2a1aa3c30e8b67d",
            "9ccd309c7c6c40aebe8e8a39ba7bfab4",
            "162fed14e1334b3794f6f7a06be86689"
          ]
        },
        "id": "M3McbIl93ATA",
        "outputId": "4f836355-c1e0-4566-e12a-e8c64da88b6a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3cbd98d4af3249fe8bec7856899365d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44db1c2ba66e4bc8a02902ce7f4bad95"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f10f660f848490ab1cc0ff653ee3af1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/384 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9dfeb3ac4179444abe2b532767edc7af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d0aec3cb0414935b177f9aba5422889"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3203c62a7e7c446b9fc91636716fe892"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d237c5c96774dbe834ebb20772812ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5e182c564794099922abc26213fd77d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7121c741788d4f08a5abe6c5f3af6267"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 3 tokens to the tokenizer.\n",
            "Resized model vocab: 50274 50277\n",
            "PeftModelForSeq2SeqLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): BartForConditionalGeneration(\n",
            "      (model): BartModel(\n",
            "        (shared): BartScaledWordEmbedding(50277, 1024, padding_idx=1)\n",
            "        (encoder): BartEncoder(\n",
            "          (embed_tokens): TrainableTokensWrapper(\n",
            "            (original_module): None\n",
            "            (token_adapter): TrainableTokensLayer(\n",
            "              (base_layer): BartScaledWordEmbedding(50277, 1024, padding_idx=1)\n",
            "              (trainable_tokens_delta): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 3x1024 (cuda:0)])\n",
            "              (trainable_tokens_original): BufferDict(  (default): Buffer containing: [torch.cuda.FloatTensor of size 3x1024 (GPU 0)])\n",
            "            )\n",
            "          )\n",
            "          (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
            "          (layers): ModuleList(\n",
            "            (0-11): 12 x BartEncoderLayer(\n",
            "              (self_attn): BartAttention(\n",
            "                (k_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (v_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (q_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (activation_fn): GELUActivation()\n",
            "              (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
            "              (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
            "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "          (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (decoder): BartDecoder(\n",
            "          (embed_tokens): TrainableTokensWrapper(\n",
            "            (original_module): None\n",
            "            (token_adapter): TrainableTokensLayer(\n",
            "              (base_layer): BartScaledWordEmbedding(50277, 1024, padding_idx=1)\n",
            "              (trainable_tokens_delta): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 3x1024 (cuda:0)])\n",
            "              (trainable_tokens_original): BufferDict(  (default): Buffer containing: [torch.cuda.FloatTensor of size 3x1024 (GPU 0)])\n",
            "            )\n",
            "          )\n",
            "          (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
            "          (layers): ModuleList(\n",
            "            (0-11): 12 x BartDecoderLayer(\n",
            "              (self_attn): BartAttention(\n",
            "                (k_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (v_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (q_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (activation_fn): GELUActivation()\n",
            "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (encoder_attn): BartAttention(\n",
            "                (k_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (v_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (q_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
            "              )\n",
            "              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
            "              (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
            "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "          (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (lm_head): ModulesToSaveWrapper(\n",
            "        (original_module): Linear(in_features=1024, out_features=50277, bias=False)\n",
            "        (modules_to_save): ModuleDict(\n",
            "          (default): Linear(in_features=1024, out_features=50277, bias=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions to generate output.\n",
        "from textwrap import fill\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 1024\n",
        "\n",
        "# Hyperparameters\n",
        "generate_kwargs = {\n",
        "    \"num_beams\": 3,\n",
        "    \"do_sample\": True,\n",
        "    \"no_repeat_ngram_size\": 4,\n",
        "    \"max_length\": 512\n",
        "}\n",
        "\n",
        "def get_sentences(text):\n",
        "  return nltk.sent_tokenize(text)\n",
        "\n",
        "def make_question_document_pairs(dataset):\n",
        "    question_document_pairs = []\n",
        "    for document, question in zip(dataset[\"document\"], dataset[\"question\"]):\n",
        "        question_document_pairs.append(f\"<ask&answer> {question} <qsep> {document}\")\n",
        "\n",
        "    return question_document_pairs\n",
        "\n",
        "def generate_output_quantized(model, tokenizer, dataset, batch_size):\n",
        "    input_sentences = make_question_document_pairs(dataset)\n",
        "\n",
        "    all_outputs = []\n",
        "    #pbar = tqdm(range(int(len(input_sentences) / batch_size) + 1))\n",
        "    for i in range(int(len(input_sentences) / batch_size) + 1):\n",
        "        start_i, end_i = i * batch_size, (i + 1) * batch_size\n",
        "        if start_i >= len(input_sentences):\n",
        "            break\n",
        "\n",
        "        #pbar.set_description_str(f\"Samples {start_i} to {end_i}\")\n",
        "\n",
        "        inputs_encoded = tokenizer(\n",
        "            input_sentences[start_i:end_i],\n",
        "            max_length=MAX_SEQUENCE_LENGTH,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors='pt')\n",
        "\n",
        "        output_ids = model.cuda().generate(\n",
        "            input_ids = inputs_encoded['input_ids'].cuda(),\n",
        "            **generate_kwargs)\n",
        "\n",
        "        generated_sentences = tokenizer.batch_decode(output_ids,\n",
        "                                                     skip_special_tokens=True,\n",
        "                                                     clean_up_tokenization_spaces=False)\n",
        "        all_outputs.extend(generated_sentences)\n",
        "\n",
        "    return all_outputs\n",
        "\n",
        "def generate_and_print_output(samples):\n",
        "  for i, (document, question) in enumerate(samples, start=1):\n",
        "    document = fill(document, width=80)\n",
        "    summary = generate_output_quantized(\n",
        "      model_quantized,\n",
        "      tokenizer_quantized,\n",
        "      { \"document\": [document], \"question\": [question] },\n",
        "      16)\n",
        "    response = fill(summary[0], width=80)\n",
        "    d_sentence_count = len(get_sentences(document))\n",
        "    r_sentence_count = len(get_sentences(response))\n",
        "    print(\"\\n\")\n",
        "    print(\"----------------------------------------------------------------\\n\")\n",
        "    print(f\"\\033[1mDocument {i} ({d_sentence_count} sentences):\\033[0m {document}\", \"\\n\")\n",
        "    print(f\"\\033[1mQuestion {i}:\\033[0m {question}\", \"\\n\")\n",
        "    print(f\"\\033[1mResponse {i} ({r_sentence_count} sentences):\\033[0m {response}\", \"\\n\")\n"
      ],
      "metadata": {
        "id": "FggrV0XA5wkO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_and_print_output(retrieve_test_samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5qy5cS6Wlxo",
        "outputId": "6876a2e5-c705-4f1c-a28a-d75411fef21b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "\n",
            "\u001b[1mDocument 1 (32 sentences):\u001b[0m HOME IS WHERE YOU LEFT IT By ADAM CHASE [Transcriber Note: This etext was\n",
            "produced from Amazing Stories February 1957. Extensive research did not uncover\n",
            "any evidence that the U.S. copyright on this publication was renewed.] The\n",
            "chance of mass slaughter was their eternal nightmare. How black is the blackest\n",
            "treachery? Is the most callous traitor entitled to mercy? Steve pondered these\n",
            "questions. His decision? That at times the villain should possibly be spoken of\n",
            "as a hero. Only the shells of deserted mud-brick houses greeted Steve Cantwell\n",
            "when he reached the village. He poked around in them for a while. The desert\n",
            "heat was searing, parching, and the Sirian sun gleamed balefully off the blades\n",
            "of Steve's unicopter, which had brought him from Oasis City, almost five hundred\n",
            "miles away. He had remembered heat from his childhood here on Sirius' second\n",
            "planet with the Earth colony, but not heat like this. It was like a magnet\n",
            "drawing all the moisture out of his body. He walked among the buildings,\n",
            "surprise and perhaps sadness etched on his gaunt, weather-beaten face. Childhood\n",
            "memories flooded back: the single well from which all the families drew their\n",
            "water, the mud-brick house, hardly different from the others and just four walls\n",
            "and a roof now, in which he'd lived with his aunt after his parents had been\n",
            "killed in a Kumaji raid, the community center where he'd spent his happiest time\n",
            "as a boy. He went to the well and hoisted up a pailful of water. The winch\n",
            "creaked as he remembered. He ladled out the water, suddenly very thirsty, and\n",
            "brought the ladle to his lips. He hurled the ladle away. The water was bitter.\n",
            "Not brackish. Poisoned. He spat with fury, then kneeled and stuffed his mouth\n",
            "with sand, almost gagging. After a while he spat out the sand too and opened his\n",
            "canteen and rinsed his mouth. His lips and mouth were paralyzed by contact with\n",
            "the poison. He walked quickly across the well-square to his aunt's house.\n",
            "Inside, it was dim but hardly cooler. Steve was sweating, the saline sweat\n",
            "making him blink. He scowled, not understanding. The table was set in his aunt's\n",
            "house. A coffeepot was on the stove and last night's partially-consumed dinner\n",
            "still on the table. The well had been poisoned, the town had been deserted on\n",
            "the spur of the moment, and Steve had returned t \n",
            "\n",
            "\u001b[1mQuestion 1:\u001b[0m What did steve cantwell remember? \n",
            "\n",
            "\u001b[1mResponse 1 (23 sentences):\u001b[0m ’s home is where you left it by Adam Chase was produced from Amazing Stories\n",
            "February 1957. When Steve Cantwell left Oasis City, his childhood memories\n",
            "flooded back to his childhood on the Sirius' second planet with the Earth colony\n",
            "on Sirius’ second planet, where he had lived with his aunt after his parents had\n",
            "been killed in a Kumaji raid, the community center where he'd spent his happiest\n",
            "time as a boy. His family had been killed by the Kumaji raid. When he was a boy,\n",
            "he lived in a mud-brick house with four walls and just four walls and a roof\n",
            "now. His family’s house had been poisoned by poison, and his aunt had cooked\n",
            "dinner last night’s partially-consumed dinner. He had eaten dinner and a\n",
            "coffeepot was on the stove. He went to the well and drank the water from the\n",
            "well, but the water was bitter and brackish. He drank the water and spat out the\n",
            "sand, then stuffed his mouth with sand and gagged, almost gagging. When he got\n",
            "thirsty, he hurled the ladle away and threw the ladle. The water was not\n",
            "brackish, but salty. He tried to drink the water again, then rinsed his mouth\n",
            "and spat out sand too. When he tried to wash his mouth, his lips and mouth were\n",
            "paralyzed by contact with the poison. The well had been poisoned, the town had\n",
            "been deserted on the spur of the moment, and the poison had been poisoned. He\n",
            "returned to his aunt's house. He walked quickly across the well-square to his\n",
            "house, and the house was dark and dim but hardly cooler. He walked among the\n",
            "buildings, feeling sadness and surprise and sadness. He went among the buildings\n",
            "and walked among the mud-bick houses. His aunt’s coffeepots and stove were still\n",
            "on the stove, and there was a partially-cooked dinner still in the stove. The\n",
            "village was deserted, and there were no Kumaji raiding. When he returned to the\n",
            "village, he saw the shell of deserted houses, the town was deserted. He\n",
            "remembered the hot desert heat and the Sirian sun gleamed balefully off the\n",
            "blades of his unicopter. He remembered heat from his childhood, but not like\n",
            "this. When he reached the village, the heat was searing, parching, and the sun\n",
            "gleamed off the blades off the unicopter, which brought him from Oasis City\n",
            "almost five hundred miles away \n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "\n",
            "\u001b[1mDocument 2 (42 sentences):\u001b[0m Wanderers of the Wolf Moon By NELSON S. BOND They were marooned on Titan, their\n",
            "ship wrecked, the radio smashed. Yet they had to exist, had to build a new life\n",
            "on a hostile world. And the man who assumed command was Gregory Malcolm, the\n",
            "bespectacled secretary—whose only adventures had come through the pages of a\n",
            "book. [Transcriber's Note: This etext was produced from Planet Stories Spring\n",
            "1944. Extensive research did not uncover any evidence that the U.S. copyright on\n",
            "this publication was renewed.] Sparks snapped off the switches and followed him\n",
            "to the door of the radio turret. Sparks was a stunted, usually-grinning, little\n",
            "redhead named Hannigan. But he wasn't grinning now. He laid an anxious hand on\n",
            "Greg's arm. \"If I was you,\" he said, \"if I was you, Malcolm, I don't think I'd\n",
            "say nothing to the boss about this. Not just yet, anyhow.\" Greg said, \"Why not?\"\n",
            "Sparks spluttered and fussed and made heavy weather of answering. \"Well, for one\n",
            "thing, it ain't important. It would only worry him. And then there's the\n",
            "womenfolks, they scare easy. Which of course they ain't no cause to.\n",
            "Atmospherics don't mean nothing. I've rode out worse storms than this—plenty of\n",
            "times. And in worse crates than the Carefree .\" Greg studied him carefully from\n",
            "behind trim plasta-rimmed spectacles. He drew a deep breath. He said levelly,\n",
            "\"So it's that bad, eh, Sparks?\" \"What bad? I just told you—\" \"I know. Sparks,\n",
            "I'm not a professional spaceman. But I've studied astrogation as few\n",
            "Earthlubbers have. It's been my hobby for years. And I think I know what we're\n",
            "up against. \"We hit a warp-eddy last night. We've been trapped in a vortex for\n",
            "more than eight hours. Lord only knows how many hundreds of thousands of miles\n",
            "we've been borne off our course. And now we've blasted into a super-ionized belt\n",
            "of atmospherics. Your radio signals are blanketed. You can't get signals in or\n",
            "out. We're a deaf-mute speck of metal being whirled headlong through space.\n",
            "Isn't that it?\" \"I don't know what—\" began Sparks hotly. Then he stopped,\n",
            "studied his companion thoughtfully, nodded. \"O.Q.,\" he confessed, \"that's it.\n",
            "But we ain't licked yet. We got three good men on \n",
            "\n",
            "\u001b[1mQuestion 2:\u001b[0m What did malcolm say? \n",
            "\n",
            "\u001b[1mResponse 2 (26 sentences):\u001b[0m Gregory Malcolm, the secretary of the crew of the Wanderers of the Wolf Moon,\n",
            "assumed command after the radio was smashed and the ship wrecked on Titan. His\n",
            "only adventures had come through the pages of books. He had studied astrogation\n",
            "as few Earthlubbers have. He knew what they were up against because he had been\n",
            "studying astrogation for years. He told Sparks and Hannigan that they had been\n",
            "trapped in a vortex for more than eight hours. They were trapped in a super-\n",
            "ionized belt of atmospherics. They couldn't get signals in or out because their\n",
            "radio signals were blanketed. They had three good men on board, but they\n",
            "couldn't get their radio signals in and out. They had to build a new life on a\n",
            "hostile world. They needed to survive on Titan because they had to live on the\n",
            "hostile world. And the man who assumed command was Gregory Malcolm, the bearded\n",
            "secretary. He had only adventures come through the books of books. Gregory was a\n",
            "secretary. He was a bespectacled secretary, who had only adventures in the pages\n",
            "of a book. He had learned astrogation from books. He was not a professional\n",
            "spaceman, but he knew what they had been up against. He told his friend Sparks\n",
            "that he didn't think he should tell the boss about the situation because it\n",
            "would worry the boss. He explained astrogation to Sparks and then to Hannigan.\n",
            "He told them about astrogation. He explained about the storms that he had rode\n",
            "out plenty of times. He explained that the womenfolkfolk scare easy because they\n",
            "scare easy. He also told them about the womenfolks. He said that the men on the\n",
            "ship were good men, and that they didn't have cause to scare the womenfolk. He\n",
            "told the men that they had to exist, and they had to be there. He explained to\n",
            "the men that there was a hostile world on Titan, and he told them that they\n",
            "would have to build a life there. He asked them to tell their boss about this. \n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "\n",
            "\u001b[1mDocument 3 (40 sentences):\u001b[0m THE EXPENDABLES BY JIM HARMON It was just a little black box, useful for getting\n",
            "rid of things. Trouble was, it worked too well! [Transcriber's Note: This etext\n",
            "was produced from Worlds of If Science Fiction, May 1962. Extensive research did\n",
            "not uncover any evidence that the U.S. copyright on this publication was\n",
            "renewed.] \"You see my problem, Professor?\" Tony Carmen held his pinkly\n",
            "manicured, flashily ringed hands wide. I saw his problem and it was warmly\n",
            "embarrassing. \"Really, Mr. Carmen,\" I said, \"this isn't the sort of thing you\n",
            "discuss with a total stranger. I'm not a doctor—not of medicine, anyway—or a\n",
            "lawyer.\" \"They can't help me. I need an operator in your line.\" \"I work for the\n",
            "United States government. I can't become involved in anything illegal.\" Carmen\n",
            "smoothed down the front of his too-tight midnight blue suit and touched the\n",
            "diamond sticking in his silver tie. \"You can't, Professor Venetti? Ever hear of\n",
            "the Mafia?\" \"I've heard of it,\" I said uneasily. \"An old fraternal organization\n",
            "something like the Moose or Rosicrucians, founded in Sicily. It allegedly\n",
            "controls organized crime in the U.S. But that is a responsibility-eluding myth\n",
            "that honest Italian-Americans are stamping out. We don't even like to see the\n",
            "word in print.\" \"I can understand honest Italian-Americans feeling that way. But\n",
            "guys like me know the Mafia is still with it. We can put the squeeze on marks\n",
            "like you pretty easy.\" You don't have to tell even a third generation American\n",
            "about the Mafia. Maybe that was the trouble. I had heard too much and for too\n",
            "long. All the stories I had ever heard about the Mafia, true or false, built up\n",
            "an unendurable threat. \"All right, I'll try to help you, Carmen. But ... that\n",
            "is, you didn't kill any of these people?\" He snorted. \"I haven't killed anybody\n",
            "since early 1943.\" \"Please,\" I said weakly. \"You needn't incriminate yourself\n",
            "with me.\" \"I was in the Marines,\" Carmen said hotly. \"Listen, Professor, these\n",
            "aren't no Prohibition times. Not many people get made for a hit these days.\n",
            "Mother, most of these bodies they keep ditching at my club haven't been murdered\n",
            "by anybody. They're accident victims. Rumbums with \n",
            "\n",
            "\u001b[1mQuestion 3:\u001b[0m What did carmen do? \n",
            "\n",
            "\u001b[1mResponse 3 (33 sentences):\u001b[0m Tony Carmen is a club owner who keeps ditching bodies of accident victims at his\n",
            "club. Most of the bodies he keeps ditching at his club haven't been murdered by\n",
            "anybody. They're accident victims. They're accidents with rumbums with accident\n",
            "victims. He hasn't killed anybody since early 1943 since early 1943. But most of\n",
            "the bodies they keep ditching at the club are accident victims. Most of these\n",
            "bodies they keep dumping at his club aren't murder victims. They haven't been\n",
            "made for a hit since early 1943.\" Carmen was in the Marines. Not many people get\n",
            "made for hit these days. Carmen says he hasn't killed any people since early\n",
            "1943, but that is not true. He snorts and says he hasn’t killed anybody since\n",
            "1943. Professor Venetti says he can’t become involved in anything illegal. He\n",
            "says he works for the United States government. Carmen says that honest Italian-\n",
            "Americans don’t like to see the word Mafia in print because honest Italian-\n",
            "American-Americans are stamping out the word in print. He says that honest\n",
            "honest Italian- Americans don't like the word “Mafia” in print. But that is a\n",
            "responsibility-eluding myth that honest Italian Americans are stampeding out.\n",
            "The Mafia is still with it. But honest honest American-Americans feel that way.\n",
            "The word “is still with it” is a responsibility eluding myth. The black box was\n",
            "just a little black box, useful for getting rid of things. It worked too well.\n",
            "Trouble was, it worked too well too well. Tony Carmen had heard too much about\n",
            "the Mafia. All the stories about the Mafia, true or false, built up an\n",
            "unendurable threat. He needed an operator in his line. Venetti tried to help Mr.\n",
            "Carmen, but he couldn’t help him because he didn’t kill any of the people he\n",
            "keeps dumping bodies at his club since 1943. Carmen’s mother says that most of\n",
            "these bodies haven’t been murdered by accident victims. Carmen said that the\n",
            "bodies aren’t accident victims either. Carmen says the bodies they have been\n",
            "ditching since early 1943 because they’re accident victims. Venetti says that\n",
            "he’s been in the Marines since early 1943 and he’ve been in the Marine since\n",
            "early 1943; he was in the Marine during Prohibition times. Carmen is in the\n",
            "Marines now. He’s in the U.S. government. \n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "\n",
            "\u001b[1mDocument 4 (28 sentences):\u001b[0m DEATH STAR By TOM PACE Trapped by the most feared of space pirates Devil\n",
            "Garrett, Starrett Blade was fighting for his life. Weaponless, his ship gone, he\n",
            "was pinning his hopes on a girl—who wanted him dead. [Transcriber's Note: This\n",
            "etext was produced from Planet Stories Spring 1945. Extensive research did not\n",
            "uncover any evidence that the U.S. copyright on this publication was renewed.]\n",
            "Starrett Blade crouched in the rocks by the tiny Centaurian lake. It was only\n",
            "about two or three hundred feet across, but probably thousands of feet deep.\n",
            "This lake, and hundreds of others like it, were the only things to break the\n",
            "monotony of the flat, rocky surface of Alpha Centauri III—called the most barren\n",
            "planet in space. Ten minutes ago, Star Blade's ship had spun into the stagnant\n",
            "waters before him. An emergency release had flung the air-lock doors open, and\n",
            "the air pressure had flung Star out. And now he was waiting for Devil Garrett to\n",
            "come down to the water's edge to search for him. For eight years, Devil Garrett\n",
            "had been the top space pirate in the void. For a year, Star himself had\n",
            "personally been hunting him. And on a tour over Alpha III, a Barden energy-beam\n",
            "had stabbed up at Blade's ship, and Star Blade had crashed into the lake. That\n",
            "Barden Beam had Star worried and puzzled. It took a million volts of power for a\n",
            "split-second flash of the beam. Garrett didn't have an atomics plant on Alpha\n",
            "III—if he had, escaping rays would point it out, no matter how well it was\n",
            "camouflaged. There was no water power, for there was no running water. There\n",
            "were only the lakes ... and tidal power was out, for Alpha III had no moon.\n",
            "However, that could wait. Star slid the electron knife from his water-proof\n",
            "sheath, gripped it firmly. He could hear quick footsteps as a man came down the\n",
            "trail that led directly past his hiding place. It wasn't Garrett, which was\n",
            "disappointing. But it was one of his men, and he was heavily armed. That didn't\n",
            "worry Star. His fighting had earned Starrett Blade the nickname of \"Death Star.\"\n",
            "The man walked to the water's edge, and peered out over the pool. He saw the\n",
            "bubbles that were coming up from the sinking ship, and he nodded, grunted in\n",
            "satisfaction, and started to turn back. S \n",
            "\n",
            "\u001b[1mQuestion 4:\u001b[0m What is the most barren planet in space? \n",
            "\n",
            "\u001b[1mResponse 4 (18 sentences):\u001b[0m The surface of Alpha Centauri III is flat, rocky. There are no running water, no\n",
            "running water power, and tidal power is out. There is no moon, for Alpha\n",
            "Centauri III does not have a moon. There is a Centaurian lake, and hundreds of\n",
            "others like it. This lake and hundreds like it are the only things to break the\n",
            "monotony of the flat, rocky surface of the Alpha Centauri III surface. There are\n",
            "only the lakes and tidal power, for there is no moon on Alpha Centauri III.\n",
            "There is also no running water. There is only the Centaurian lakes and hundreds\n",
            "of other lakes. The surface is flat and rocky, with no running water and no\n",
            "moon. The surface has no air power, no air-lock doors, and no air pressure. The\n",
            "air pressure has flung Star out of his ship, and his ship spun into the stagnant\n",
            "water before him. Ten minutes ago, Star Blade’s ship had spun into the water\n",
            "before him, and the air pressure flung Star out. The ship’s air-locks open, and\n",
            "air pressure flung the ship into the stagnant waters before him. The water is\n",
            "only two or three hundred feet across, but probably thousands of feet deep. This\n",
            "lake is only two hundred feet across but probably hundreds of feet deep, and\n",
            "hundreds more like it like it. The Centaurians lake is the only thing to break\n",
            "the barren, rocky surface. This lake, like hundreds like it, is the only things\n",
            "that break the monotonony of the surface of the planet. It is called the most\n",
            "barren planet in space. \n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "\n",
            "\u001b[1mDocument 5 (19 sentences):\u001b[0m THE FIRST MAN INTO SPACE Cadet Marshall Farnsworth woke from a nightmare of\n",
            "exploding novae and fouling rockets. After recovering from his fright, he\n",
            "laughed contemptuously at himself. “Here I was picked as the most stable of a\n",
            "group of two hundred cadets,” he thought, “and chosen to make man’s first trip\n",
            "into space, yet I’m shaking like a leaf.” He got out of bed and went over to the\n",
            "window. From his father’s temporary apartment, he could see distant Skyharbor,\n",
            "the scene of the plunge into space tomorrow night. He had been awarded the\n",
            "frightening honor of making that trip. 10 As he watched teardrop cars whip along\n",
            "Phoenix, Arizona’s, double-decked streets, elevated over one another to avoid\n",
            "dangerous intersections and delaying stop lights, he thought back over the\n",
            "years; to the 1950’s, when mice and monkeys were sent up in Vikings to launch\n",
            "mankind’s first probing of the mysterious space beyond Earth, and the first\n",
            "satellites were launched; to the 1960’s, when huger, multiple-stage rockets\n",
            "finally conquered the problem of escape velocity; to 1975—today—when man was\n",
            "finally ready to send one of his own kind into the uninhabited deeps. Marsh\n",
            "climbed back into bed, but sleep would not come. In the adjoining room, he could\n",
            "hear the footsteps of mother and father. By their sound he knew they were the\n",
            "footsteps of worried people. This hurt Marsh more than his own uneasiness. The\n",
            "anxiety had begun for them, he knew, when he had first signed up for space-cadet\n",
            "training. They had known there was an extremely high percentage of washouts, and\n",
            "after each test he passed, they had pretended to be glad. But Marsh knew that\n",
            "inwardly they had hoped he would fail, for they were aware of the ultimate goal\n",
            "that the space scientists were working for—the goal that had just now been\n",
            "reached. Marsh finally fell into a troubled sleep that lasted until morning. He\n",
            "woke early, before the alarm rang. He got up, showered, pulled on his blue-\n",
            "corded cadet uniform, and tugged on the polished gray boots. He took one final\n",
            "look around his room as though in farewell, then went out to the kitchen. 11 His\n",
            "folks were up ahead of time too, trying to act as though it were just another\n",
            "day. Dad was pretending to enjoy his morning paper, nodding only casually to\n",
            "Marsh as he came in. \n",
            "\n",
            "\u001b[1mQuestion 5:\u001b[0m When did marsh think back to? \n",
            "\n",
            "\u001b[1mResponse 5 (7 sentences):\u001b[0m Marsh thinks back over the years when he wakes from his nightmare of exploding\n",
            "novae and fouling rockets. He thinks back to the 1950’s and 1960’s when mice and\n",
            "monkeys were sent up in Vikings to launch mice and monkeys to explore the\n",
            "mysterious space beyond Earth. Later, monkeys and mice were sent up into space\n",
            "in Vikings to probe beyond Earth, and the first spacecraftsatellites were\n",
            "launched; to the 1960’S, when huger, multiple-stage rockets finally conquered\n",
            "the problem of escape velocity; to 1975 when man was finally ready to send one\n",
            "of his own kind into the uninhabited deeps. 10 As he watched teardrop cars whip\n",
            "along Phoenix, Arizona’s, double-decked Phoenix, Arizona's, double decked\n",
            "streets, he thinks back over years, as he watches Phoenix’s double-decked\n",
            "streets, teardrop car whip along Phoenix double decked Phoenix, double-d-decked\n",
            "streets, and delaying stop lights, he watches the scene of the plunge into space\n",
            "from his father’s temporary apartment. 11 He thinks back over all the years that\n",
            "man’s first trip into space has been accomplished. He thinks about the 1954’s of\n",
            "mice and monkeys being sent up into the unknown space beyond Earth, to probe the\n",
            "mysteries beyond Earth, the 1960s of multiple stages rockets conquering escape\n",
            "velocity, and the 1975’s. Marsh thinks about the 1960s when huger rockets\n",
            "finally conquered escape velocity and multiple stages were finally conquered the\n",
            "escape velocity problem. \n",
            "\n"
          ]
        }
      ]
    }
  ]
}
