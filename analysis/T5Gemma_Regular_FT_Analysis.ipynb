{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "sGahQhf6_-jY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "# For Preprocessing\n",
        "!pip install -q -U datasets\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import json\n",
        "import random\n",
        "from datasets import Dataset, load_dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "!pip install -q -U torch torchvision torchaudio fastai\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U transformers\n",
        "!pip install -q -U peft\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U tokenizers\n",
        "!pip install -q -U datasets\n",
        "!pip install -q -U evaluate\n",
        "!pip install -q -U rouge_score\n",
        "!pip install -q -U bert_score\n",
        "!pip install -q -U loralib einops xformers\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "import os\n",
        "import bitsandbytes as bnb\n",
        "import pandas as pd\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    PeftConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        ")\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "import evaluate\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from textwrap import fill\n",
        "from transformers import pipeline\n",
        "from sortedcontainers import SortedList\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "pAHIvOeYeE9v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d54e6bc3-6042-40f4-e369-05d3c5197d17"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m117.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m129.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m113.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.3/235.3 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m126.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.9/503.9 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.1/117.1 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Prepared Documents\n",
        "\n",
        "These are 10 randomly chosen short stories from the SQuALITY test set, with 2 questions each, for 20 total samples."
      ],
      "metadata": {
        "id": "MI52o9aNnFpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "retrieve_test_samples = []\n",
        "\n",
        "try:\n",
        "  if not retrieve_test_samples:\n",
        "    file_name = \"hallucination_study_test_samples-2025-08-02_171427.pkl\"\n",
        "    with open(f\"/content/drive/MyDrive/DS266/project/analysis/data/{file_name}\", \"rb\") as file:  # \"rb\" for read binary\n",
        "      retrieve_test_samples = pickle.load(file)\n",
        "except Exception as e:\n",
        "  print(f\"Error: {e}\")\n",
        "\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Test samples:\", len(retrieve_test_samples), \"\\n\")\n",
        "retrieve_test_document = retrieve_test_samples[0]\n",
        "retrieve_document_length = len(retrieve_test_document[0].split())\n",
        "retrieve_document_text = fill(retrieve_test_document[0], width=80)\n",
        "retrieve_question_length = len(retrieve_test_document[1].split())\n",
        "retrieve_question_text = retrieve_test_document[1]\n",
        "print(f\"\\033[1mDocument ({retrieve_document_length} words):\\033[0m {retrieve_document_text}\", \"\\n\")\n",
        "print(f\"\\033[1mQuestion ({retrieve_question_length} words):\\033[0m {retrieve_question_text}\", \"\\n\")\n"
      ],
      "metadata": {
        "id": "U6B1mGK4Io6S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b85902e5-7b8e-42d5-9b10-c0939660c340"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "\n",
            "Test samples: 5 \n",
            "\n",
            "\u001b[1mDocument (401 words):\u001b[0m HOME IS WHERE YOU LEFT IT By ADAM CHASE [Transcriber Note: This etext was\n",
            "produced from Amazing Stories February 1957. Extensive research did not uncover\n",
            "any evidence that the U.S. copyright on this publication was renewed.] The\n",
            "chance of mass slaughter was their eternal nightmare. How black is the blackest\n",
            "treachery? Is the most callous traitor entitled to mercy? Steve pondered these\n",
            "questions. His decision? That at times the villain should possibly be spoken of\n",
            "as a hero. Only the shells of deserted mud-brick houses greeted Steve Cantwell\n",
            "when he reached the village. He poked around in them for a while. The desert\n",
            "heat was searing, parching, and the Sirian sun gleamed balefully off the blades\n",
            "of Steve's unicopter, which had brought him from Oasis City, almost five hundred\n",
            "miles away. He had remembered heat from his childhood here on Sirius' second\n",
            "planet with the Earth colony, but not heat like this. It was like a magnet\n",
            "drawing all the moisture out of his body. He walked among the buildings,\n",
            "surprise and perhaps sadness etched on his gaunt, weather-beaten face. Childhood\n",
            "memories flooded back: the single well from which all the families drew their\n",
            "water, the mud-brick house, hardly different from the others and just four walls\n",
            "and a roof now, in which he'd lived with his aunt after his parents had been\n",
            "killed in a Kumaji raid, the community center where he'd spent his happiest time\n",
            "as a boy. He went to the well and hoisted up a pailful of water. The winch\n",
            "creaked as he remembered. He ladled out the water, suddenly very thirsty, and\n",
            "brought the ladle to his lips. He hurled the ladle away. The water was bitter.\n",
            "Not brackish. Poisoned. He spat with fury, then kneeled and stuffed his mouth\n",
            "with sand, almost gagging. After a while he spat out the sand too and opened his\n",
            "canteen and rinsed his mouth. His lips and mouth were paralyzed by contact with\n",
            "the poison. He walked quickly across the well-square to his aunt's house.\n",
            "Inside, it was dim but hardly cooler. Steve was sweating, the saline sweat\n",
            "making him blink. He scowled, not understanding. The table was set in his aunt's\n",
            "house. A coffeepot was on the stove and last night's partially-consumed dinner\n",
            "still on the table. The well had been poisoned, the town had been deserted on\n",
            "the spur of the moment, and Steve had returned t \n",
            "\n",
            "\u001b[1mQuestion (5 words):\u001b[0m What did steve cantwell remember? \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    load_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model_name = \"t5gemma_baseline-2025-07-29_170029\"\n",
        "\n",
        "model_quantized = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    f\"/content/drive/MyDrive/DS266/project/models/{model_name}\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0})\n",
        "\n",
        "# Tokenizer was unchanged.\n",
        "tokenizer_quantized = AutoTokenizer.from_pretrained(\"google/t5gemma-2b-2b-ul2-it\")\n",
        "model_quantized = prepare_model_for_kbit_training(model_quantized)\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"k_proj\", \"v_proj\", \"q_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_2_SEQ_LM\",\n",
        "    modules_to_save=[\"lm_head\"]\n",
        ")\n",
        "\n",
        "model_quantized = get_peft_model(model_quantized, config)\n",
        "print(model_quantized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a3be2509d33144948db4ce394645c9a3",
            "1d69284a71ab48cdb184e3afd955d38d",
            "366454f1509e4d3cbe0c9d9f8994d8b3",
            "c2454f575ca445da993e196803ff244f",
            "70bd84b025384b848d0048e7d2298762",
            "e72cf388b08246ecb24b6088c3012de0",
            "2853495d0cc64dbba51c9b3c5f5dce51",
            "d36800757c854b9385300d7bab5f0f65",
            "848a3c3442cf483fa6363b3953728579",
            "585e8fac10e94ec9a87aaec2278891c7",
            "a6ca5527846b48149dcfeb9d8e4fb5d8",
            "9c94121e03d3408fa8a93f1aec0827d3",
            "ba4c1ad026de4775a2b02d5d2fc0f063",
            "c38f0533c78f4dea93f5fd1ea68c676f",
            "eca77ef12db245688f6cf0c5d5384bb6",
            "7af232d4efa54d08826bcc1354d11ef0",
            "7da66b2556e24de191f4c937deee2385",
            "fba839106fc347c7ba1dc9416a3b5015",
            "774954529e444016a951b3dc12f52eea",
            "e2ea90f060414021a825ffb63a9a00bb",
            "8ce4cfb8fd1b4b12b66edc31c4beadbd",
            "c2f05120aff847959bfa4a99e5a39ae3",
            "fad82ae75c864e8aa4420d04c7ccd2f0",
            "a3b58ce3a0a3403cb9a446d6e4c28ea8",
            "39d8e5e037fc425fbf6e3ee5d8bf9d64",
            "fd6f4c88c2b44143bf427c17a5489ed8",
            "75ddef2e42bd4f638d0b00119e3345e7",
            "7f3f59708b424af8a06f3711cc5aa3c8",
            "b505533fb4334783a81e5a685aa52f96",
            "76cbd725626c423bbafa0e3e0cfad50b",
            "22373407b57e4180bf6159a0ccfaecbc",
            "96dbb9e91ae748c1abda4483e6b3e1d7",
            "2824f2e9709d48b0bcf4c3d106a846a3",
            "de7d613ce0f142fab5baf77367cca0bf",
            "c8439f935d5f4d73b958b425d0b9a238",
            "30b2ff5d1e1a4f3baf5e6e199f5cfc59",
            "c6318acd36304cb0ad5fea46324bca91",
            "8268c3fccc78491f8b27b7b90972182f",
            "739d6db04ac645e8828ca773fa6a7ba5",
            "1aaa9fe338184ae0970b774fa918a5de",
            "f1cd48a596424f67a3f8ce6e0de1ec56",
            "2d4dfa18419b4d95ba0f471812cee12f",
            "25aecf8ab0124b01aa4a589988359098",
            "14f978b74d454c09aec198b214f2342c",
            "1670e9ced2074bea9003bc0e20099a7a",
            "e12beb45d9e043a8a62bd47406aedbf9",
            "2afca615c5164239b5f0656928e8d561",
            "d1ddfffe6ef54e3687319a224c56eb8d",
            "4ac6a7f9287a48008bc103cc41941824",
            "3dbdd634884e4f119ae618d599393b9f",
            "13ca56defaf64a038344477447965c51",
            "403183298dff4819889e390005fdd7e8",
            "9a430574441249169a9d2a2a63fcbaed",
            "a8d30dbce5484475ad9bbe2ff59d7d8d",
            "7b6d8fb9d7b04bc79af3d6ee7e3dc0fc",
            "0f07635eaf924077879a5ae7425d421c",
            "9540c57a768b4fd2a452b73754efc738",
            "49f47603aff54c0a92fc4b447258e235",
            "a6db0cfaa6ab4c59848b845c738ca612",
            "6958e3971d85435bb2d62bab1bf8a03f",
            "f5cb4af12e304f7397b1836129a9bae3",
            "a586bb4778ec43be884a157bf07ad482",
            "ec11bd6f77144be392be59c9ec114867",
            "1701b839f9d34abbb022bbef4edda490",
            "705f640670274973be27160d182759fa",
            "f3ddaaa89593426ab60ce9217bfc4973",
            "8a1c98c12f944bf7be1f1a3531532565",
            "865006bd97ea48c3ba9da6043f871497",
            "2cda9e756f4e4e5d9daa5feaab9cdfd6",
            "b5461ab7d44b4deda4e0a5b13a436d7f",
            "6395f296b4af4de0a9948664c3fbe9b8",
            "9d6aa5c765874912b3124432663e2b9d",
            "51ac0641196342eaa4661cd0fa3819b4",
            "b45f42b0ff904ca3bff44a00d16aa495",
            "0fd80eb4625242f7b02913b1a32affe9",
            "6bb60510203f4f6397637daf85576dcd",
            "bae1bfe6003f41b7b2941e0c99f77781",
            "e02da43f7fa747a6a3a0500d3722da13",
            "c14ad285e0324993bf8d4aa20a49d4fe",
            "f3abf7b5187f4e2b95d9c82359701723",
            "006e57f780fa4091bfbfa685e3ca1098",
            "cbc83f224c1e44fe94271c7a4dabaf36",
            "69e6f1d23cac465594037bf95b24d6ea",
            "ea522b793a624280ae529790ba5e2031",
            "32f415aeda93460bb642525d789073c8",
            "0cf13aac292c4da7b8c5b103cb0f5c99",
            "21dc175e95c94fb1b3c600f081de48d6",
            "3b3e766d35a44442afd97a6e32d0450b",
            "6c0ec5e2091f4bcea84d1803b7c30f96",
            "521adbe3b85740969d08661cba3628a6",
            "6a70a63cfbe84b048c8cb085937a423e",
            "9bcca749b0cd418d8ac427c1298610d8",
            "e69136c396844d3696588037e39474e2",
            "8412a593850d4ebdb1fbd072d13dd531",
            "a104745db4044975aa5608fff4054442",
            "93c49308abd4430aa3a44a73f78d88ce",
            "46742b0ffbf24db58b6e91c59eef9ce2",
            "bfeb183ae1da481eacf945c536ab64b3",
            "11c5af6197ec4833bcbb425899e7702d",
            "2fbaa5b194a443309dfd60a822482d7a",
            "40481bfd70e74f3a959d32525d5f54c6",
            "d2f9025ddce94643bbe0d43b148f1b37",
            "5a6ff5a2f76e4ae2968a221dc0fe331d",
            "c5cdaf1dde5c4a9eaf71ad6e36b08c39",
            "9d8f1f57d4434540a9f69b9ce7a45f3f",
            "efe08f762db142ff9546d92ef660d15a",
            "6925dfb1008c40028641fe0e0ed832b3",
            "45b8b243cdb74ae1a9d4a0560d5ca9b8",
            "e384dc4359ba47c8a6a4ed9a34095294",
            "300d4ebbc4e24ad2b950cf09582de847",
            "5e19a6fdd2014297be234f03bad9fb98",
            "346c28316e574f88935061de1a5caea8",
            "77c0e11e6786413f9a8d74ba9b5f97fb",
            "7a3eb830cc604f1bb22a2baf7753ca05",
            "d5ac901f326b4b84b6661ff5a29a8e2a",
            "4091ca50384a4b86a6e84db0d50db556",
            "dba8763d2033451aa3804226cef0527e",
            "ef47ee6e4fa8451cbafb83d08970a1d3",
            "94066cee017244eb829f23d25f741494",
            "7d4222a596964a07bdc9f3fb134dc468",
            "0070e1b8a8924ebab7a30ebe0f28fdfc",
            "3741e3b4d0f840e4a5e53a6eb9c8dc78",
            "9c5e26b0074841d3b15919b1de22cb01",
            "4e8c0f1de0e94868930dd0acad830601",
            "2331599de7d9485eba3c4f184939d0b3",
            "3cc7bc9e14694c17a50afe27e04c0b3e",
            "459147347d23472daaa781eb478086ec",
            "dfdcc87dc11b4c0a901b73decf208b3e",
            "93b686b0bec44659915fd61a2ee0a5f4",
            "b5de5d4670424dfd9c59291be36d4245",
            "aada7ab5ea414f3ea58a9d678e55db59",
            "5d0f03c7cf9140389664fdc70e136275",
            "96ef0d4a66c4495e9ca9cd310194753e",
            "2a438a3fbc72425982c6d5dcbb774b14",
            "18887dea7d314ac9b212b8c22796201e",
            "d67559f0de9e45e4b8f06754338756e9",
            "a72e7bacc2f44ee59ecbe77cf2cb33b0",
            "954d69a3bca8418ead9007a71f31619c",
            "c29be342496c4797bfbe49c4cebfdede",
            "f375ae579cf646b58f426e303ee0280d",
            "1c5f0a00eb134d2eac88d5048a30a20b",
            "168feee50a9b45de8c592986c728c007",
            "56e9741862254c9ead38c0eab0946938"
          ]
        },
        "id": "M3McbIl93ATA",
        "outputId": "c59fafaf-5245-41dc-bf90-515fa4b16d40"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/3.26k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a3be2509d33144948db4ce394645c9a3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/68.1k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c94121e03d3408fa8a93f1aec0827d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fad82ae75c864e8aa4420d04c7ccd2f0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de7d613ce0f142fab5baf77367cca0bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1670e9ced2074bea9003bc0e20099a7a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f07635eaf924077879a5ae7425d421c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a1c98c12f944bf7be1f1a3531532565"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e02da43f7fa747a6a3a0500d3722da13"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/46.4k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c0ec5e2091f4bcea84d1803b7c30f96"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2fbaa5b194a443309dfd60a822482d7a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e19a6fdd2014297be234f03bad9fb98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3741e3b4d0f840e4a5e53a6eb9c8dc78"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "chat_template.jinja:   0%|          | 0.00/577 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "96ef0d4a66c4495e9ca9cd310194753e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PeftModelForSeq2SeqLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): T5GemmaForConditionalGeneration(\n",
            "      (model): T5GemmaModel(\n",
            "        (encoder): T5GemmaEncoder(\n",
            "          (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
            "          (norm): T5GemmaRMSNorm((2304,), eps=1e-06)\n",
            "          (rotary_emb): T5GemmaRotaryEmbedding()\n",
            "          (layers): ModuleList(\n",
            "            (0-25): 26 x T5GemmaEncoderLayer(\n",
            "              (self_attn): T5GemmaSelfAttention(\n",
            "                (q_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=2304, out_features=2048, bias=False)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=2304, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=2048, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (k_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=2304, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (v_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=2304, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (o_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=2048, out_features=2304, bias=False)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "              )\n",
            "              (pre_self_attn_layernorm): T5GemmaRMSNorm((2304,), eps=1e-06)\n",
            "              (post_self_attn_layernorm): T5GemmaRMSNorm((2304,), eps=1e-06)\n",
            "              (mlp): T5GemmaMLP(\n",
            "                (gate_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
            "                (up_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
            "                (down_proj): Linear4bit(in_features=9216, out_features=2304, bias=False)\n",
            "                (act_fn): PytorchGELUTanh()\n",
            "                (dropout): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "              (pre_feedforward_layernorm): T5GemmaRMSNorm((2304,), eps=1e-06)\n",
            "              (post_feedforward_layernorm): T5GemmaRMSNorm((2304,), eps=1e-06)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (decoder): T5GemmaDecoder(\n",
            "          (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
            "          (norm): T5GemmaRMSNorm((2304,), eps=1e-06)\n",
            "          (rotary_emb): T5GemmaRotaryEmbedding()\n",
            "          (layers): ModuleList(\n",
            "            (0-25): 26 x T5GemmaDecoderLayer(\n",
            "              (self_attn): T5GemmaSelfAttention(\n",
            "                (q_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=2304, out_features=2048, bias=False)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=2304, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=2048, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (k_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=2304, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (v_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=2304, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (o_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=2048, out_features=2304, bias=False)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "              )\n",
            "              (pre_self_attn_layernorm): T5GemmaRMSNorm((2304,), eps=1e-06)\n",
            "              (post_self_attn_layernorm): T5GemmaRMSNorm((2304,), eps=1e-06)\n",
            "              (mlp): T5GemmaMLP(\n",
            "                (gate_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
            "                (up_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
            "                (down_proj): Linear4bit(in_features=9216, out_features=2304, bias=False)\n",
            "                (act_fn): PytorchGELUTanh()\n",
            "                (dropout): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "              (pre_feedforward_layernorm): T5GemmaRMSNorm((2304,), eps=1e-06)\n",
            "              (post_feedforward_layernorm): T5GemmaRMSNorm((2304,), eps=1e-06)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (cross_attn): T5GemmaCrossAttention(\n",
            "                (q_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=2304, out_features=2048, bias=False)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=2304, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=2048, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (k_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=2304, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (v_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=2304, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (o_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=2048, out_features=2304, bias=False)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "              )\n",
            "              (pre_cross_attn_layernorm): T5GemmaRMSNorm((2304,), eps=1e-06)\n",
            "              (post_cross_attn_layernorm): T5GemmaRMSNorm((2304,), eps=1e-06)\n",
            "            )\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (lm_head): ModulesToSaveWrapper(\n",
            "        (original_module): T5GemmaLMHead(\n",
            "          (out_proj): Linear(in_features=2304, out_features=256000, bias=False)\n",
            "        )\n",
            "        (modules_to_save): ModuleDict(\n",
            "          (default): T5GemmaLMHead(\n",
            "            (out_proj): Linear(in_features=2304, out_features=256000, bias=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions to generate output.\n",
        "from textwrap import fill\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 1024\n",
        "\n",
        "# Hyperparameters\n",
        "generate_kwargs = {\n",
        "    \"num_beams\": 3,\n",
        "    \"do_sample\": True,\n",
        "    \"no_repeat_ngram_size\": 4,\n",
        "    \"max_length\": 512\n",
        "}\n",
        "\n",
        "def get_sentences(text):\n",
        "  return nltk.sent_tokenize(text)\n",
        "\n",
        "def generate_output_quantized(model, tokenizer, dataset, batch_size, max_length=MAX_SEQUENCE_LENGTH):\n",
        "  formatted_inputs = []\n",
        "  for document, question in zip(dataset[\"document\"], dataset[\"question\"]):\n",
        "    formatted_inputs.append([{\"role\": \"user\", \"content\": f\"Question: {question} Context: {document}\"}])\n",
        "\n",
        "  outputs = []\n",
        "  #pbar = tqdm(range(int(len(formatted_inputs) / batch_size) + 1))\n",
        "  for i in range(int(len(formatted_inputs) / batch_size) + 1):\n",
        "    start_i, end_i = i * batch_size, (i + 1) * batch_size\n",
        "    if start_i >= len(formatted_inputs):\n",
        "        break\n",
        "\n",
        "    #pbar.set_description_str(f\"Samples {start_i} to {end_i}\")\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        formatted_inputs[start_i:end_i],\n",
        "        return_tensors=\"pt\",\n",
        "        return_dict=True,\n",
        "        add_generation_prompt=True,\n",
        "        max_length=max_length,\n",
        "        padding=True,\n",
        "        truncation=True,).to(\"cuda\")\n",
        "    encoded_output = model.generate(**input_ids, **generate_kwargs)\n",
        "    generated_sentences = tokenizer.batch_decode(encoded_output,\n",
        "                                                     skip_special_tokens=True,\n",
        "                                                     clean_up_tokenization_spaces=False)\n",
        "    outputs.extend(generated_sentences)\n",
        "\n",
        "  return outputs\n",
        "\n",
        "def generate_and_print_output(samples):\n",
        "  for i, (document, question) in enumerate(samples, start=1):\n",
        "    document = fill(document, width=80)\n",
        "    summary = generate_output_quantized(\n",
        "      model_quantized,\n",
        "      tokenizer_quantized,\n",
        "      { \"document\": [document], \"question\": [question] },\n",
        "      16)\n",
        "    response = fill(summary[0], width=80)\n",
        "    d_sentence_count = len(get_sentences(document))\n",
        "    r_sentence_count = len(get_sentences(response))\n",
        "    print(\"\\n\")\n",
        "    print(\"----------------------------------------------------------------\\n\")\n",
        "    print(f\"\\033[1mDocument {i} ({d_sentence_count} sentences):\\033[0m {document}\", \"\\n\")\n",
        "    print(f\"\\033[1mQuestion {i}:\\033[0m {question}\", \"\\n\")\n",
        "    print(f\"\\033[1mResponse {i} ({r_sentence_count} sentences):\\033[0m {response}\", \"\\n\")\n"
      ],
      "metadata": {
        "id": "FggrV0XA5wkO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_and_print_output(retrieve_test_samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5qy5cS6Wlxo",
        "outputId": "34d32797-a79e-43a4-d8c5-7a9bd2cf8ef1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "\n",
            "\u001b[1mDocument 1 (32 sentences):\u001b[0m HOME IS WHERE YOU LEFT IT By ADAM CHASE [Transcriber Note: This etext was\n",
            "produced from Amazing Stories February 1957. Extensive research did not uncover\n",
            "any evidence that the U.S. copyright on this publication was renewed.] The\n",
            "chance of mass slaughter was their eternal nightmare. How black is the blackest\n",
            "treachery? Is the most callous traitor entitled to mercy? Steve pondered these\n",
            "questions. His decision? That at times the villain should possibly be spoken of\n",
            "as a hero. Only the shells of deserted mud-brick houses greeted Steve Cantwell\n",
            "when he reached the village. He poked around in them for a while. The desert\n",
            "heat was searing, parching, and the Sirian sun gleamed balefully off the blades\n",
            "of Steve's unicopter, which had brought him from Oasis City, almost five hundred\n",
            "miles away. He had remembered heat from his childhood here on Sirius' second\n",
            "planet with the Earth colony, but not heat like this. It was like a magnet\n",
            "drawing all the moisture out of his body. He walked among the buildings,\n",
            "surprise and perhaps sadness etched on his gaunt, weather-beaten face. Childhood\n",
            "memories flooded back: the single well from which all the families drew their\n",
            "water, the mud-brick house, hardly different from the others and just four walls\n",
            "and a roof now, in which he'd lived with his aunt after his parents had been\n",
            "killed in a Kumaji raid, the community center where he'd spent his happiest time\n",
            "as a boy. He went to the well and hoisted up a pailful of water. The winch\n",
            "creaked as he remembered. He ladled out the water, suddenly very thirsty, and\n",
            "brought the ladle to his lips. He hurled the ladle away. The water was bitter.\n",
            "Not brackish. Poisoned. He spat with fury, then kneeled and stuffed his mouth\n",
            "with sand, almost gagging. After a while he spat out the sand too and opened his\n",
            "canteen and rinsed his mouth. His lips and mouth were paralyzed by contact with\n",
            "the poison. He walked quickly across the well-square to his aunt's house.\n",
            "Inside, it was dim but hardly cooler. Steve was sweating, the saline sweat\n",
            "making him blink. He scowled, not understanding. The table was set in his aunt's\n",
            "house. A coffeepot was on the stove and last night's partially-consumed dinner\n",
            "still on the table. The well had been poisoned, the town had been deserted on\n",
            "the spur of the moment, and Steve had returned t \n",
            "\n",
            "\u001b[1mQuestion 1:\u001b[0m What did steve cantwell remember? \n",
            "\n",
            "\u001b[1mResponse 1 (10 sentences):\u001b[0m Steve Cantwell remembered:  * **The searing heat of his childhood on Sirius's\n",
            "second planet.** It wasn't just hot, it was like a magnet, drawing out all the\n",
            "moisture from his body. * **The single well from which all the families drew\n",
            "their water.** It was the source of life for the entire village. He remembered\n",
            "the creaking of the winch as he hoisted up a pailful of water. * The **mud-brick\n",
            "house he lived in with his aunt after his parents were killed in a Kumaji\n",
            "raid.** It was just a shell now, but he could still picture it. He remembered\n",
            "spending his happiest time at the community center, which was now just a pile of\n",
            "rubble.  The well was **poisoned**, and the town had been **deserted on the spur\n",
            "of the moment**. Steve had returned to find his childhood home in ruins, and he\n",
            "didn't understand what had happened. \n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "\n",
            "\u001b[1mDocument 2 (42 sentences):\u001b[0m Wanderers of the Wolf Moon By NELSON S. BOND They were marooned on Titan, their\n",
            "ship wrecked, the radio smashed. Yet they had to exist, had to build a new life\n",
            "on a hostile world. And the man who assumed command was Gregory Malcolm, the\n",
            "bespectacled secretary—whose only adventures had come through the pages of a\n",
            "book. [Transcriber's Note: This etext was produced from Planet Stories Spring\n",
            "1944. Extensive research did not uncover any evidence that the U.S. copyright on\n",
            "this publication was renewed.] Sparks snapped off the switches and followed him\n",
            "to the door of the radio turret. Sparks was a stunted, usually-grinning, little\n",
            "redhead named Hannigan. But he wasn't grinning now. He laid an anxious hand on\n",
            "Greg's arm. \"If I was you,\" he said, \"if I was you, Malcolm, I don't think I'd\n",
            "say nothing to the boss about this. Not just yet, anyhow.\" Greg said, \"Why not?\"\n",
            "Sparks spluttered and fussed and made heavy weather of answering. \"Well, for one\n",
            "thing, it ain't important. It would only worry him. And then there's the\n",
            "womenfolks, they scare easy. Which of course they ain't no cause to.\n",
            "Atmospherics don't mean nothing. I've rode out worse storms than this—plenty of\n",
            "times. And in worse crates than the Carefree .\" Greg studied him carefully from\n",
            "behind trim plasta-rimmed spectacles. He drew a deep breath. He said levelly,\n",
            "\"So it's that bad, eh, Sparks?\" \"What bad? I just told you—\" \"I know. Sparks,\n",
            "I'm not a professional spaceman. But I've studied astrogation as few\n",
            "Earthlubbers have. It's been my hobby for years. And I think I know what we're\n",
            "up against. \"We hit a warp-eddy last night. We've been trapped in a vortex for\n",
            "more than eight hours. Lord only knows how many hundreds of thousands of miles\n",
            "we've been borne off our course. And now we've blasted into a super-ionized belt\n",
            "of atmospherics. Your radio signals are blanketed. You can't get signals in or\n",
            "out. We're a deaf-mute speck of metal being whirled headlong through space.\n",
            "Isn't that it?\" \"I don't know what—\" began Sparks hotly. Then he stopped,\n",
            "studied his companion thoughtfully, nodded. \"O.Q.,\" he confessed, \"that's it.\n",
            "But we ain't licked yet. We got three good men on \n",
            "\n",
            "\u001b[1mQuestion 2:\u001b[0m What did malcolm say? \n",
            "\n",
            "\u001b[1mResponse 2 (13 sentences):\u001b[0m Malcolm said, \"So it's that bad, eh, Sparks?\" and \"I know, Sparks, I'm not a\n",
            "professional spaceman. But I've studied astrogation as few Earthlubbers have.\n",
            "It's been my hobby for years. And I think I know what we're up against. We hit a\n",
            "warp-eddy last night. We've been trapped in a vortex for more than eight hours.\n",
            "Lord only knows how many hundreds of thousands of miles we've been borne off our\n",
            "course. And now we've blasted into a super-ionized belt of atmospherics. Your\n",
            "radio signals are blanketed. You can't get signals in or out. We're a deaf-mute\n",
            "speck of metal being whirled headlong through space. Isn't that it?\" \n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "\n",
            "\u001b[1mDocument 3 (40 sentences):\u001b[0m THE EXPENDABLES BY JIM HARMON It was just a little black box, useful for getting\n",
            "rid of things. Trouble was, it worked too well! [Transcriber's Note: This etext\n",
            "was produced from Worlds of If Science Fiction, May 1962. Extensive research did\n",
            "not uncover any evidence that the U.S. copyright on this publication was\n",
            "renewed.] \"You see my problem, Professor?\" Tony Carmen held his pinkly\n",
            "manicured, flashily ringed hands wide. I saw his problem and it was warmly\n",
            "embarrassing. \"Really, Mr. Carmen,\" I said, \"this isn't the sort of thing you\n",
            "discuss with a total stranger. I'm not a doctor—not of medicine, anyway—or a\n",
            "lawyer.\" \"They can't help me. I need an operator in your line.\" \"I work for the\n",
            "United States government. I can't become involved in anything illegal.\" Carmen\n",
            "smoothed down the front of his too-tight midnight blue suit and touched the\n",
            "diamond sticking in his silver tie. \"You can't, Professor Venetti? Ever hear of\n",
            "the Mafia?\" \"I've heard of it,\" I said uneasily. \"An old fraternal organization\n",
            "something like the Moose or Rosicrucians, founded in Sicily. It allegedly\n",
            "controls organized crime in the U.S. But that is a responsibility-eluding myth\n",
            "that honest Italian-Americans are stamping out. We don't even like to see the\n",
            "word in print.\" \"I can understand honest Italian-Americans feeling that way. But\n",
            "guys like me know the Mafia is still with it. We can put the squeeze on marks\n",
            "like you pretty easy.\" You don't have to tell even a third generation American\n",
            "about the Mafia. Maybe that was the trouble. I had heard too much and for too\n",
            "long. All the stories I had ever heard about the Mafia, true or false, built up\n",
            "an unendurable threat. \"All right, I'll try to help you, Carmen. But ... that\n",
            "is, you didn't kill any of these people?\" He snorted. \"I haven't killed anybody\n",
            "since early 1943.\" \"Please,\" I said weakly. \"You needn't incriminate yourself\n",
            "with me.\" \"I was in the Marines,\" Carmen said hotly. \"Listen, Professor, these\n",
            "aren't no Prohibition times. Not many people get made for a hit these days.\n",
            "Mother, most of these bodies they keep ditching at my club haven't been murdered\n",
            "by anybody. They're accident victims. Rumbums with \n",
            "\n",
            "\u001b[1mQuestion 3:\u001b[0m What did carmen do? \n",
            "\n",
            "\u001b[1mResponse 3 (3 sentences):\u001b[0m The provided text doesn't explicitly state what Carmen did. It only mentions\n",
            "that he has a problem with people ditching bodies at his club and that he claims\n",
            "he hasn't killed anyone since 1943.   To understand what Carmen did, we need\n",
            "more information from the story \"The Expendables\" by Jim Harmon. \n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "\n",
            "\u001b[1mDocument 4 (28 sentences):\u001b[0m DEATH STAR By TOM PACE Trapped by the most feared of space pirates Devil\n",
            "Garrett, Starrett Blade was fighting for his life. Weaponless, his ship gone, he\n",
            "was pinning his hopes on a girl—who wanted him dead. [Transcriber's Note: This\n",
            "etext was produced from Planet Stories Spring 1945. Extensive research did not\n",
            "uncover any evidence that the U.S. copyright on this publication was renewed.]\n",
            "Starrett Blade crouched in the rocks by the tiny Centaurian lake. It was only\n",
            "about two or three hundred feet across, but probably thousands of feet deep.\n",
            "This lake, and hundreds of others like it, were the only things to break the\n",
            "monotony of the flat, rocky surface of Alpha Centauri III—called the most barren\n",
            "planet in space. Ten minutes ago, Star Blade's ship had spun into the stagnant\n",
            "waters before him. An emergency release had flung the air-lock doors open, and\n",
            "the air pressure had flung Star out. And now he was waiting for Devil Garrett to\n",
            "come down to the water's edge to search for him. For eight years, Devil Garrett\n",
            "had been the top space pirate in the void. For a year, Star himself had\n",
            "personally been hunting him. And on a tour over Alpha III, a Barden energy-beam\n",
            "had stabbed up at Blade's ship, and Star Blade had crashed into the lake. That\n",
            "Barden Beam had Star worried and puzzled. It took a million volts of power for a\n",
            "split-second flash of the beam. Garrett didn't have an atomics plant on Alpha\n",
            "III—if he had, escaping rays would point it out, no matter how well it was\n",
            "camouflaged. There was no water power, for there was no running water. There\n",
            "were only the lakes ... and tidal power was out, for Alpha III had no moon.\n",
            "However, that could wait. Star slid the electron knife from his water-proof\n",
            "sheath, gripped it firmly. He could hear quick footsteps as a man came down the\n",
            "trail that led directly past his hiding place. It wasn't Garrett, which was\n",
            "disappointing. But it was one of his men, and he was heavily armed. That didn't\n",
            "worry Star. His fighting had earned Starrett Blade the nickname of \"Death Star.\"\n",
            "The man walked to the water's edge, and peered out over the pool. He saw the\n",
            "bubbles that were coming up from the sinking ship, and he nodded, grunted in\n",
            "satisfaction, and started to turn back. S \n",
            "\n",
            "\u001b[1mQuestion 4:\u001b[0m What is the most barren planet in space? \n",
            "\n",
            "\u001b[1mResponse 4 (1 sentences):\u001b[0m The passage states that **Alpha Centauri III** is called the most barren planet\n",
            "in space. \n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "\n",
            "\u001b[1mDocument 5 (19 sentences):\u001b[0m THE FIRST MAN INTO SPACE Cadet Marshall Farnsworth woke from a nightmare of\n",
            "exploding novae and fouling rockets. After recovering from his fright, he\n",
            "laughed contemptuously at himself. “Here I was picked as the most stable of a\n",
            "group of two hundred cadets,” he thought, “and chosen to make man’s first trip\n",
            "into space, yet I’m shaking like a leaf.” He got out of bed and went over to the\n",
            "window. From his father’s temporary apartment, he could see distant Skyharbor,\n",
            "the scene of the plunge into space tomorrow night. He had been awarded the\n",
            "frightening honor of making that trip. 10 As he watched teardrop cars whip along\n",
            "Phoenix, Arizona’s, double-decked streets, elevated over one another to avoid\n",
            "dangerous intersections and delaying stop lights, he thought back over the\n",
            "years; to the 1950’s, when mice and monkeys were sent up in Vikings to launch\n",
            "mankind’s first probing of the mysterious space beyond Earth, and the first\n",
            "satellites were launched; to the 1960’s, when huger, multiple-stage rockets\n",
            "finally conquered the problem of escape velocity; to 1975—today—when man was\n",
            "finally ready to send one of his own kind into the uninhabited deeps. Marsh\n",
            "climbed back into bed, but sleep would not come. In the adjoining room, he could\n",
            "hear the footsteps of mother and father. By their sound he knew they were the\n",
            "footsteps of worried people. This hurt Marsh more than his own uneasiness. The\n",
            "anxiety had begun for them, he knew, when he had first signed up for space-cadet\n",
            "training. They had known there was an extremely high percentage of washouts, and\n",
            "after each test he passed, they had pretended to be glad. But Marsh knew that\n",
            "inwardly they had hoped he would fail, for they were aware of the ultimate goal\n",
            "that the space scientists were working for—the goal that had just now been\n",
            "reached. Marsh finally fell into a troubled sleep that lasted until morning. He\n",
            "woke early, before the alarm rang. He got up, showered, pulled on his blue-\n",
            "corded cadet uniform, and tugged on the polished gray boots. He took one final\n",
            "look around his room as though in farewell, then went out to the kitchen. 11 His\n",
            "folks were up ahead of time too, trying to act as though it were just another\n",
            "day. Dad was pretending to enjoy his morning paper, nodding only casually to\n",
            "Marsh as he came in. \n",
            "\n",
            "\u001b[1mQuestion 5:\u001b[0m When did marsh think back to? \n",
            "\n",
            "\u001b[1mResponse 5 (6 sentences):\u001b[0m Marsh thought back to:  1. **The 1950's:** when mice and monkeys were sent up in\n",
            "Vikings to launch mankind's first probing of space, and the first satellites\n",
            "were launched. 2. **The1960's** when larger, multiple-stage rockets finally\n",
            "conquered the problem of escape velocity. 3. **1975—today—** when man was\n",
            "finally ready to send one of his own kind into the uninhabited deeps. \n",
            "\n"
          ]
        }
      ]
    }
  ]
}
