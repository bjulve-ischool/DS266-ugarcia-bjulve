{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U transformers\n",
        "!pip install -q -U datasets\n",
        "!pip install -q -U evaluate\n",
        "!pip install -q -U tokenizers"
      ],
      "metadata": {
        "id": "XJRqtzGeKqVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "import evaluate\n",
        "import json\n",
        "from datasets import Dataset, load_dataset\n",
        "\n",
        "# For pre-trained T5 model\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration  # this won't import twice, just noting here what's for each model\n",
        "\n",
        "# For all T5 models\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "# For BLEURT (to load a trained model for evaluation)\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "# For style classifier model (also for evaluating the seq2seq model output)\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer"
      ],
      "metadata": {
        "id": "Muf_c2FLMgIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "D-2IrbYnKefW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fd6584f-5286-4606-8926-199328e34744"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DS266-ugarcia-bjulve'...\n",
            "remote: Enumerating objects: 24, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 24 (delta 3), reused 4 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (24/24), 3.23 MiB | 6.01 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n",
            "'Baseline Model Evaluation.ipynb'   EDA.ipynb\t  QMSum_data_process.py\n",
            " data\t\t\t\t    outputs\t  requirements.txt\n",
            " dataset.py\t\t\t    __pycache__   squality_rouge.py\n",
            " DS266-ugarcia-bjulve\t\t    QMSum\t  train.py\n"
          ]
        }
      ],
      "source": [
        "# Mount Google drive.\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "\n",
        "# Go to working directory for the final project.\n",
        "# %cd /content/gdrive/My Drive/DS266/project\n",
        "\n",
        "!cd /content/\n",
        "!rm -rf DS266-ugarcia-bjulve\n",
        "!git clone https://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@github.com/bjulve-ischool/DS266-ugarcia-bjulve.git\n",
        "!cd DS266-ugarcia-bjulve\n",
        "!ls .\n",
        "\n",
        "train_file = 'data/v1-3/train.jsonl'\n",
        "dev_file = 'data/v1-3/dev.jsonl'\n",
        "test_file = 'data/v1-3/test.jsonl'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_path):\n",
        "  with open(test_file) as f:\n",
        "      lines = f.read().split(\"\\n\")[:-1]\n",
        "\n",
        "  document_question_response = []\n",
        "  for line in lines:\n",
        "      data = json.loads(line)\n",
        "      document = data[\"document\"]\n",
        "      questions = data[\"questions\"]\n",
        "      for question in questions:\n",
        "          question_text = question[\"question_text\"]\n",
        "          responses = question[\"responses\"]\n",
        "          for response in responses:\n",
        "              response_text = response[\"response_text\"]\n",
        "              document_question_response.append((document, question_text, response_text))\n",
        "\n",
        "  return document_question_response"
      ],
      "metadata": {
        "id": "HD7XqenrP0fE"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_triplets = load_data(test_file)"
      ],
      "metadata": {
        "id": "LkvB3bD56jtz"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at some examples\n",
        "for _ in range(3):\n",
        "    sample = random.choice(test_triplets)\n",
        "    print(f\"Document: {sample[0][:50]}\", \"\\n\")\n",
        "    print(f\"Question: {sample[1]}\", \"\\n\")\n",
        "    print(f\"Response: {sample[2]}\", \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "earXG9xLQuXQ",
        "outputId": "161f1660-0972-4e8c-ed33-20ba65d5c700"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document: He was something out of a nightmare but his music  \n",
            "\n",
            "Question: Why is John Smith interested in holes? \n",
            "\n",
            "Response: He is interested in the holes because he needs to find the correct one in order to go home. He mentions that the foundation of their cultures is based on the history of all the time dimensions. The different dimensions are interwoven and the holes can provide insights into the different dimensions. Similar to time or space travel. Back in his time, he was proving a point to the University, but some researcher forgot to set a force-field for the hole, thus, he fell through the hole into Jimmie’s time. He needs to go back immediately. If he doesn’t, the University will think that he cannot prove his theory and ran away. But because everything are made up of holes – even his body has holes – John becomes worried of not able to find the correct one.  \n",
            "\n",
            "Document: ACID BATH\n",
            " \n",
            "\n",
            " By VASELEOS GARSON\n",
            " \n",
            " \n",
            "  The starway \n",
            "\n",
            "Question: What are some of the equipment used in the story? \n",
            "\n",
            "Response: Both Jon and the Steel-blues use equipment throughout the story. First, Jon uses space boots and an oxygen dial when running from the steel-blues. The space boots allow him to control his gravitational pull, and the oxygen dial allows him to control the amount of oxygen that he is inhaling. Jon also uses a stubray pistol throughout the story to fight off the steel-blues and to escape from them. The steel-blues use black boxes to control different things. They also have smaller robots, which is what gives Jon the acid every day. \n",
            "\n",
            "Document: TIME IN THE ROUND\n",
            " \n",
            " \n",
            " By FRITZ LEIBER\n",
            " \n",
            " Illustra \n",
            "\n",
            "Question: What is the plot of the story? \n",
            "\n",
            "Response: Butch, Hal, and Joggy, are three kids of varying ages: Joggy is five, Butch is under five, and Hal is older. Butch exhibits a lot of frustration toward their non-violent and heavily age-regimented society. He says he’s going to be World Director, and seems to want to be a dictator like those from the time before humanity conditioned out violence. \n",
            "\n",
            "Butch goes with the other boys to Time in the Round, a place where they can see events from the past and have them explained. Because it is carefully curated for specific ages, Hal tells Butch he won’t be able to enter. Butch tries anyway, but an invisible blockade they call an “usher” won’t let him through. \n",
            "\n",
            "The other boys watch a sorcerer and some warriors inside the Time Bubble. Before too long, Butch appears, telling them he lied his way in with a sympathetic adult. Hal is upset that he did this, and also by Butch’s behavior once he’s in there. Though they have been told that it would be impossible for the Time Bubble to be used for time travel, Butch yells at the sorcerer to “sock it to ‘em” and he listens; somehow, between Butch and the sorcerer’s willpower, a few of the warriors end up outside the bubble and in the auditorium, along with their wolves. The interpreter and audience start to panic. \n",
            "\n",
            "Butch takes control, order his and his friends’ uninjs to attack the wolves, who are larger but not invincible like the uninjs. He orders a warrior to put down a lady he has slung over his shoulder and his uninj, Brute, bites the warrior in the ankle, causing him to drop her. Butch tells them to go back where they came from and Brute chases them back into the bubble. Butch calls Brute, and as soon as he jumps back out the bubble dims and goes back to normal. \n",
            "\n",
            "Everyone is relieved, and the adults are more talkative and less “mature” than usual. People discuss “revised theories” and both the formerly captive woman and Brute embrace and kiss Butch, but he is too dazed and happy to notice. He pets Brute and says “we came, we saw, we conquered, didn’t we, Brute?” \n",
            " \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(test_triplets)"
      ],
      "metadata": {
        "id": "gromeaGw-BaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dataset(triplets):\n",
        "    documents, questions, responses = zip(*triplets)\n",
        "    documents = list(documents)\n",
        "    questions = list(questions)\n",
        "    responses = list(responses)\n",
        "\n",
        "    dataset = Dataset.from_dict({\"documents\": documents, \"questions\": questions, \"responses\": responses})\n",
        "    return dataset.shuffle()\n",
        "\n",
        "# Make the test data set.\n",
        "test_dataset = make_dataset(test_triplets)\n"
      ],
      "metadata": {
        "id": "uTVAdwIg9X6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"First Document: \", test_dataset[\"documents\"][0][:50], \"\\n\")\n",
        "print(\"First Question: \", test_dataset[\"questions\"][0], \"\\n\")\n",
        "print(\"First Response: \", test_dataset[\"responses\"][0], \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPIKlfEN-tkL",
        "outputId": "455447f3-6e7a-4ba9-d02d-a0d71d4bbdc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Document:  \n",
            "\n",
            "\n",
            "\n",
            "Produced by Greg Weeks, Mary Meehan and the On \n",
            "\n",
            "First Question:  Who is Magnan, and what is his role in and relevance to the story? \n",
            "\n",
            "First Response:  Magnan is the Terrestrial Ambassador to the Fustians. He is the figurehead of their influence on the Fustian planet, and works closely with Retief, the Terrestrial diplomat who uncovers a plot against the Terrestrials through the course of the story. He is the man who tries to convince Retief to sponsor the Youth Group SCARS in the beginning of the story, and we encounter him at the banquet near the end of the story. As the figurehead, he is responsible for announcing the role of the Terrestrials in funding the Youth Group, which creates an opportunity for Retief to announce the Grocian plot to everyone. Ambassador Magnan eventually joins Retief and Whonk as they leave the event to stop the criminals, but he is thrown into an alley by Whonk and doesn't have an opportunity to help directly. After the issue is dealt with by Whonk and Retief, Magnan resumes normal duty, and as the story ends he is looking at other groups that his government could potentially fund.\n",
            " \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer, BartForConditionalGeneration\n",
        "\n",
        "# Load the baseline SQuALITY model checkpoint from Pagnoni et. al., 2021.\n",
        "socratic_checkpoint_name = \"Salesforce/squality-socratic-books-30M\"\n",
        "socratic_tokenizer = AutoTokenizer.from_pretrained(socratic_checkpoint_name)\n",
        "socratic_model = BartForConditionalGeneration.from_pretrained(socratic_checkpoint_name)\n",
        "#pipeline(\"summarization\", model=socratic_checkpoint_name, tokenizer=socratic_checkpoint_name)\n"
      ],
      "metadata": {
        "id": "sBNZJJ-uUKXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQUENCE_LENGTH = 1024\n",
        "\n",
        "def make_question_document_pairs(batch_triplets):\n",
        "    question_document_pairs = []\n",
        "    for document, question in zip(batch_triplets[\"documents\"], batch_triplets[\"questions\"]):\n",
        "        question_document_pairs.append(f\"<ask&answer> {question} <qsep> {document}\")\n",
        "\n",
        "    return question_document_pairs\n",
        "\n",
        "def preprocess_socratic_batch(batch_triplets, tokenizer):\n",
        "    question_document_pairs = make_question_document_pairs(batch_triplets)\n",
        "\n",
        "    input_encoded = tokenizer.batch_encode_plus(\n",
        "        question_document_pairs,\n",
        "        max_length=MAX_SEQUENCE_LENGTH,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    labels_encoded = tokenizer.batch_encode_plus(\n",
        "        batch_triplets[\"responses\"],\n",
        "        max_length=MAX_SEQUENCE_LENGTH,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    return {'input_ids': input_encoded['input_ids'],\n",
        "            'label_ids': labels_encoded['input_ids']}"
      ],
      "metadata": {
        "id": "iuaurDe1B--F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_encoded = test_dataset.map(\n",
        "    preprocess_socratic_batch,\n",
        "    batched=True,\n",
        "    fn_kwargs={\n",
        "      'tokenizer': socratic_tokenizer\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "694b05e2c12842dfa8ab7333d081a66f",
            "23280dd5bbf549e1b2d01c0357efbf56",
            "3027ab3c78594f90a3ad5b90a855eed6",
            "fe2013c547164cdaaaab9d8179170a3a",
            "ad7a18dfb7984f379077ad96df962700",
            "383955dc12884d078780e7b3bb5faa3c",
            "5716e1b2d8834cd9b593d82d60e394ab",
            "dfa11fa127344703a3d9c2060d6e7eaf",
            "072fcd129abc42afbd996ef5543adeb5",
            "5707eb313340488cab7dd189524f6b3c",
            "fc1251abbeff4b819556a7f2aabef261"
          ]
        },
        "id": "rokgWYKpFvUs",
        "outputId": "ec6370fb-5de7-4303-80c2-894291e02863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1040 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "694b05e2c12842dfa8ab7333d081a66f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import logging\n",
        "logging.set_verbosity(logging.INFO)\n",
        "\n",
        "def generate_output(model, tokenizer, batch_triplets, batch_size, **kwargs):\n",
        "    input_sentences = make_question_document_pairs(batch_triplets)\n",
        "\n",
        "    all_outputs = []\n",
        "\n",
        "    for i in range(int(len(input_sentences) / batch_size) + 1):\n",
        "        start_i, end_i = i * batch_size, (i + 1) * batch_size\n",
        "        if start_i >= len(input_sentences):\n",
        "            break\n",
        "\n",
        "        inputs_encoded = tokenizer(\n",
        "            input_sentences[start_i:end_i],\n",
        "            max_length=MAX_SEQUENCE_LENGTH,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors='pt')\n",
        "\n",
        "        output_ids = model.cuda().generate(\n",
        "            inputs_encoded['input_ids'].cuda(),\n",
        "            **kwargs)\n",
        "\n",
        "        generated_sentences = tokenizer.batch_decode(output_ids,\n",
        "                                                     skip_special_tokens=True,\n",
        "                                                     clean_up_tokenization_spaces=False)\n",
        "        all_outputs.extend(generated_sentences)\n",
        "\n",
        "    return all_outputs"
      ],
      "metadata": {
        "id": "EuyqB6BbG1Zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the outputs from the model.\n",
        "generate_kwargs = {\n",
        "    \"num_beams\": 3,\n",
        "    \"do_sample\": True,\n",
        "    \"no_repeat_ngram_size\": 3,\n",
        "    \"max_length\": 512\n",
        "}\n",
        "samples = generate_output(socratic_model, socratic_tokenizer, test_dataset, 16, **generate_kwargs)"
      ],
      "metadata": {
        "id": "8vGYpjjoJUPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(random.choice(samples))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haoFF6ZfjOj9",
        "outputId": "5dd22285-31e8-4ac7-a11c-1d47d5e5fbaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The members of the space ship Explorer hunt animals to test them for contagion. The people of the ship are in airtight spacesuits and doctors are in green spacesuits. They hunt animals that look like humans to test for the animals’ diseases. They wait while their doctors, in air\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Samples Last Run: 1h 37m 39s\n",
        "# Save to reimport later.\n",
        "\n",
        "import pickle\n",
        "\n",
        "samples_file_path = \"outputs/sample_output-test.jsonl-socratic-squality.pkl\"\n",
        "\n",
        "with open(samples_file_path, \"wb\") as file:  # \"wb\" for write binary\n",
        "    pickle.dump(samples, file)"
      ],
      "metadata": {
        "id": "_K_hmyj1YNGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload if samples is empty.\n",
        "\n",
        "try:\n",
        "    samples\n",
        "except NameError:\n",
        "    with open(samples_file_path, \"rb\") as file:  # \"rb\" for read binary\n",
        "      samples = pickle.load(file)\n",
        "\n",
        "print(len(samples))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnpvkxupZD8t",
        "outputId": "abf67c68-6309-4ba3-a7d3-9a4264fbd8d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1040\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate ROUGE scores\n",
        "\n",
        "!pip install -q rouge_score\n",
        "\n",
        "rouge = evaluate.load('rouge')\n",
        "predictions = samples\n",
        "references = test_dataset[\"responses\"]\n",
        "rouge_results = rouge.compute(predictions=predictions,\n",
        "                        references=references)\n",
        "\n",
        "print(\"ROUGE: \", rouge_results)"
      ],
      "metadata": {
        "id": "tAEFNl-PKLSI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "c2a200d0e84746768676c957ec5c00aa",
            "a7f63f1090e8462e89f9fe1ed43e8b23",
            "f52c05a1bd5b4888b33105b4facce0b4",
            "f1675b9dddbf4d3b8891b9397c3b07fa",
            "39c61a9713844dff84ea5c83cb6123f7",
            "74a5fe78c5ea452d8c1ae7d2347081de",
            "ea0c1952461d440fbda114653907b826",
            "860d1dbda1844ce0ba2b8773209014a4",
            "45ded2ddeef7455aa806ca53916f586e",
            "a01434a33b4d4148bc035357e044c719",
            "4e5c3cdc29e64dbca8684fa929675b0a"
          ]
        },
        "outputId": "5cb35585-02ed-4ab1-c7c0-6ec64e139007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2a200d0e84746768676c957ec5c00aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE:  {'rouge1': np.float64(0.3109285772757844), 'rouge2': np.float64(0.0625451922802101), 'rougeL': np.float64(0.15850135492874273), 'rougeLsum': np.float64(0.17814044477983487)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate BLEU scores\n",
        "\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "bleu_results = bleu.compute(predictions=predictions, references=references)\n",
        "print(\"BLEU: \", bleu_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150,
          "referenced_widgets": [
            "6b4bca5a3ea444b9b3a0322d60592fb8",
            "97dada4aba984c1bbd95b29801995e55",
            "776c3231bcf043af913978688a80af07",
            "e1586bd899a744709d62c1662d7187fa",
            "3d96d745f0624fa08bee71fd12163c6c",
            "7050ee07e4554eb8be244d84ad8b6dc6",
            "4acd184a160d4ddd8af66acdcbac7604",
            "37dc50bb61dd4cf0b365cfe50d8ad60a",
            "b7406735ec5b40f5b77b43f757c0df11",
            "f046e9b841294a4e8a893f2925c296cc",
            "9960d180fcd54bb8bb9ba42e280344ef",
            "7bd2e308b05547a6a0d88c28eae84295",
            "4b2d26cfe0044bf683fbdec33e4f28bf",
            "349b282e6c8642eb96250f85ee9e6e04",
            "a1585cc973634029b9b9870ca4e8f2d0",
            "802f037c80744592b122b9b5721ea97a",
            "468a530ad4814a5fa81af1a3021af929",
            "6a241912b923412d9e55dc51be816fb4",
            "e0980389fe834600aacc566211f1ce0b",
            "ea39dc344c1e4a1ea727e79d91364abc",
            "a60c26c9d4424c8a96287e88219abf26",
            "e1b46b725f964e849ad42b4f7495913f",
            "99e38ae5c06044c587231373ced77451",
            "693becbcd8ec471a9086822a80403765",
            "f846b6dd808b451b94a2f08948b882e8",
            "a0da2881692747cc84c4aa604fc584be",
            "66726900bc6a448bb2fdb8f2fdc35f68",
            "1f72d97365604f459866070f4ecbc7ab",
            "5584338f21694bc49e46eba94c64186f",
            "2ca93cef668d494a8bff86eb4873dd84",
            "f3f0b798a0bf47c087f44228657b76df",
            "946152452ff440dfa4340c19e6080dab",
            "77f23d371d26456188d8c8d670e5c0e3"
          ]
        },
        "id": "axsOR5HyW-mj",
        "outputId": "4c09c478-e8a8-4a4b-b265-9c4a9bbfa5ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b4bca5a3ea444b9b3a0322d60592fb8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7bd2e308b05547a6a0d88c28eae84295"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading extra modules: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99e38ae5c06044c587231373ced77451"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU:  {'bleu': 0.036026649487998386, 'precisions': [0.3197619834278181, 0.07399894305980903, 0.01583924272017338, 0.0044947830771417145], 'brevity_penalty': 1.0, 'length_ratio': 1.1670388907341023, 'translation_length': 305693, 'reference_length': 261939}\n"
          ]
        }
      ]
    }
  ]
}
