{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "Xh69sEOcl97c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For Preprocessing\n",
        "!pip install -q -U datasets\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import json\n",
        "import random\n",
        "import os\n",
        "from datasets import Dataset, load_dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "waPq3v58qS84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Training\n",
        "\n",
        "!pip install -q -U torch torchvision torchaudio fastai\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U transformers\n",
        "!pip install -q -U peft\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U tokenizers\n",
        "!pip install -q -U evaluate\n",
        "!pip install -q -U rouge_score\n",
        "!pip install -q -U loralib einops xformers\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "import bitsandbytes\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    PeftConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        ")\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    BitsAndBytesConfig,\n",
        ")"
      ],
      "metadata": {
        "id": "pAHIvOeYeE9v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb1c8879-9546-4547-f800-0a0c51f0ba49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m126.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m114.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m130.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.3/235.3 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m127.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m129.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m558.8/558.8 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.1/117.1 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility.\n",
        "RANDOM_SEED = 33\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "#tf.random.set_seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "torch.cuda.manual_seed(RANDOM_SEED)\n",
        "torch.cuda.manual_seed_all(RANDOM_SEED)"
      ],
      "metadata": {
        "id": "wifjrl9dKWBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "zwKWovoamBuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prep for download.\n",
        "%cd /content/\n",
        "!rm -rf DS266-ugarcia-bjulve\n",
        "!git clone https://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@github.com/bjulve-ischool/DS266-ugarcia-bjulve.git\n",
        "%cd DS266-ugarcia-bjulve\n",
        "!ls .\n",
        "\n",
        "train_file = 'data/v1-3/train.jsonl'\n",
        "dev_file = 'data/v1-3/dev.jsonl'\n",
        "test_file = 'data/v1-3/test.jsonl'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMbBByFGl8oq",
        "outputId": "6a171553-a2fb-4229-fe78-4f6b83a4eb5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'DS266-ugarcia-bjulve'...\n",
            "remote: Enumerating objects: 32, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 32 (delta 8), reused 4 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (32/32), 3.34 MiB | 4.44 MiB/s, done.\n",
            "Resolving deltas: 100% (8/8), done.\n",
            "/content/DS266-ugarcia-bjulve\n",
            "Baseline_Model_Evaluation.ipynb  QFS_Datasets.ipynb\n",
            "data\t\t\t\t README.md\n",
            "EDA2.ipynb\t\t\t Socratic_FT_Data_Augmentation.ipynb\n",
            "EDA.ipynb\t\t\t Socratic_Pretrained_Sampler.ipynb\n",
            "outputs\t\t\t\t T5Gemma_Sampler.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper to load the data into memory.\n",
        "def load_data(file_path):\n",
        "  with open(file_path) as f:\n",
        "      lines = f.read().split(\"\\n\")[:-1]\n",
        "\n",
        "  document_question_response = []\n",
        "  for line in lines:\n",
        "      data = json.loads(line)\n",
        "      # Remove extra white space. Since the tokenizer is subword\n",
        "      # and not sentence, then the newlines will not likely affect\n",
        "      # the word embedding underlying meaning.\n",
        "      document = \" \".join(data[\"document\"].split())\n",
        "      questions = data[\"questions\"]\n",
        "      for question in questions:\n",
        "          question_text = \" \".join(question[\"question_text\"].split())\n",
        "          responses = question[\"responses\"]\n",
        "          for response in responses:\n",
        "              response_text = \" \".join(response[\"response_text\"].split())\n",
        "              document_question_response.append((document, question_text, response_text))\n",
        "\n",
        "  return document_question_response\n",
        "\n",
        "\n",
        "# Get the data. Preserve the original splits.\n",
        "train_triplets = load_data(train_file)\n",
        "dev_triplets = load_data(dev_file)\n",
        "test_triplets =  load_data(test_file)\n",
        "print(\"Train:\", len(train_triplets))\n",
        "print(\"Dev:\", len(dev_triplets))\n",
        "print(\"Test:\", len(test_triplets))\n",
        "\n",
        "# Create a HF dataset. Shuffle the order\n",
        "# before returning it.\n",
        "def make_dataset(triplets):\n",
        "    documents, questions, responses = zip(*triplets)\n",
        "    documents = list(documents)\n",
        "    questions = list(questions)\n",
        "    responses = list(responses)\n",
        "\n",
        "    dataset = Dataset.from_dict({\"document\": documents, \"question\": questions, \"response\": responses})\n",
        "    return dataset.shuffle(seed=RANDOM_SEED)\n",
        "\n",
        "train_dataset = make_dataset(train_triplets)\n",
        "dev_dataset = make_dataset(dev_triplets)\n",
        "test_dataset = make_dataset(test_triplets)\n",
        "\n",
        "# Print a sample.\n",
        "random_sample = random.choice(train_dataset)\n",
        "random_document, random_question, random_response = random_sample[\"document\"], random_sample[\"question\"], random_sample[\"response\"]\n",
        "print(\"\\nRANDOM SAMPLE:\\n\")\n",
        "print(f\"\\033[1mDocument:\\033[0m {random_document[:50]}\", \"\\n\")\n",
        "print(f\"\\033[1mQuestion:\\033[0m {random_question}\", \"\\n\")\n",
        "print(f\"\\033[1mResponse:\\033[0m {random_response}\", \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5puHhhOmRGK",
        "outputId": "92ca83ed-0deb-4941-ca1f-e18633035faf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 1000\n",
            "Dev: 500\n",
            "Test: 1040\n",
            "\n",
            "RANDOM SAMPLE:\n",
            "\n",
            "\u001b[1mDocument:\u001b[0m THE MAN OUTSIDE By EVELYN E. SMITH Illustrated by  \n",
            "\n",
            "\u001b[1mQuestion:\u001b[0m What is the relationship between Martin and Ives? \n",
            "\n",
            "\u001b[1mResponse:\u001b[0m Cousin Ives enters Martin’s life when he is a little older, and is the third descendant to accompany him as his guardian. Out of all his descendants to assume guardianship, Martin forms the closest relationship with Ives. Rather than seeing Martin as a responsibility and duty, Ives sees Martin as an individual and seeks ways to connect and encourage his passions. For one, Ives buys a yacht named The Interregnum to which the pair take upon themselves to explore the current world in. They traveled across the waters and inland to see both the civilized and uncivilized world, with Martin taking it all in. When it was just the two of them, their relationship progressed further. Ives began to open up about the future world that he and his descendants come from and explain the nuances of the social order that rules. Ives is the first to explicitly and honestly describe the feudal and privileged social class that Martin’s descendants take part in, only due to their fortunate ancestry. Additionally, Ives is the only cousin to admit the potential truth in Conrad’s intentions, noting the dilemma between achieving moral good and selfishing maintaining their own good life. Martin even comments his confidence in Ives being able to see the obvious flaw in the cousins’ plans. However, during one winter, Ives fell ill to a severe chill and passed away before his own birth. After Ives’ death, Martin relently voyages across oceans and soon as they and the cousins blur, he begins to live detachedly. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the pretrained model and prepare it for QLoRA.\n",
        "# We'll use the quantized version of the model for\n",
        "# PEFT.\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    load_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "socratic_checkpoint_name = \"Salesforce/squality-socratic-books-30M\"\n",
        "socratic_model_quantized = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    socratic_checkpoint_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0})\n",
        "socratic_tokenizer_quantized = AutoTokenizer.from_pretrained(socratic_checkpoint_name)\n",
        "socratic_model_quantized = prepare_model_for_kbit_training(socratic_model_quantized)\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"k_proj\", \"v_proj\", \"q_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_2_SEQ_LM\",\n",
        "    modules_to_save=[\"lm_head\"]\n",
        ")\n",
        "\n",
        "socratic_model_quantized = get_peft_model(socratic_model_quantized, config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305,
          "referenced_widgets": [
            "842e68b4b91448f4b1aedd42cf2f0a2b",
            "bafdba61b8524b8ea64196759603a869",
            "76c5e208bb0e4560b7677f38965c029c",
            "cf5d6b9991e24473960b2e586a77ece1",
            "ad0b7996b4c946e1bfaf7d366bc192cd",
            "a6c44f385d174371b48d8b88cd579485",
            "0f2cb87541a84a8f944473b3de895131",
            "ed28ed02c1404d3cb5eb05a58601def8",
            "43bdfa3d16184cbdaa363240da8f26af",
            "1bdf0242648d4fc082634a300761bb10",
            "a30f07c427674dbfb4b93c5a50e7d12d",
            "b77c36aac62847b8a0a4c537fe775fa7",
            "301bfea48b924c138d5be2e33e09dc6b",
            "5a53253af82c46e38fd7278afb6d3d51",
            "4ea190eb864d48e79390ec3a9905f309",
            "2c7207aa3f0b491db589e165f11c2fea",
            "aea2e7f3fd0f46a9a9704c563f0452ad",
            "f47a3758219a4b618870bc9c4b6b44a3",
            "f62d857aae424e4881d8d717bbb389c4",
            "2a7563b8ce004a83ada21bb013671d85",
            "7e05c9dbe0ff45de9c8aa79ad3f0bab1",
            "0fb2d18945eb426c9068e606bc997d71",
            "2fe344e2758a4fada7ba82c54c70b5ab",
            "8728d55552444603b36104dd2d6464ff",
            "96e67666d5c14ad8a613e6ebd53db166",
            "38c47bbac6e047ddb3cf1d899008d25e",
            "e452353646bd4ef8a833586b03efdefb",
            "cbccba219e574569b209b7b80ef8adb7",
            "655ada71a6894ab790acdcc617f8bb31",
            "9cbda4f514844cc0b7f085db5bc20067",
            "03f5c2c46c7e43068d1323b5088f7c9d",
            "9f2979ab99214ca5a67f59ddec983988",
            "e603c38ad4be4921bd631b030b0c581d",
            "aef4c341f56c41e4b5b5296ffa29d600",
            "888f5889b4f94894afaf869d0ed9ffaa",
            "50d056d7541c42d08127ccf692a4ae8d",
            "327825ab8ec04e88809693dbff3bb163",
            "b5b57103c1e64bd18cf3530ffbac5512",
            "cf0db0ad40ef41c0a9bfa77d21dce44c",
            "8821019da9d646a18ed20bbc02548958",
            "b18b08e2fccd42d3a43cfdb72346e7c8",
            "7eafa4ea66e4492280a27fd37d8767bb",
            "c9777e5a4dc84fd6ae31de44dea973b8",
            "bcb16a202f99466093e901f999ea26b7",
            "6aa2607590854834bd767e151f96f1d7",
            "19588adbf7e94215b114f0b610268246",
            "7fc279623461449fba83ac7fcbf40eda",
            "c5ea1230002a460f87e35fc3d1c9af5d",
            "e5499749d4e04d1b888c9159de38e9f6",
            "f5962138e4dc4407bf9538948b2c096b",
            "328003cdbc9544739da0d5d2b2f8b01b",
            "5793f29d3da04f09a550378a054d0427",
            "4144d43725524c8fbb29f16277f20b24",
            "f9f922e6e61f43f7b2ad92d5baa97ee1",
            "81978207a42943a0ae9e3f1d478d361e",
            "a37349374bf549cd92c110891ede004c",
            "1755c49fed8d43989a366261941e2419",
            "4dca8123979c4cf69572d89d8c888cc6",
            "de4b327e63dc456d99d19fc3e7b9d5bf",
            "7f449668f0c34ac791e11d9e0cc7ab74",
            "25a6b0c540eb4e3c8fac0245b435963a",
            "ae695b73da2c40f99f27bdb1802755ed",
            "2363e04ccea64ae9add52e65037cfe80",
            "0aa86023d6234dd79672c85c6461f061",
            "7f62547819de4bd8af8a4228a722f165",
            "b82b0577f06a42e5bd0361fd409f401f",
            "a3ad62b0cee642d286ce0d714badd425",
            "4567dd18a23d4886a9c63bbf236b5312",
            "eab4491193214e4da12d3c6bcfcfa106",
            "34e3dcc2fda64a2fafeefbbdb80a0125",
            "1f31c1dd32094f57b9776850f717d568",
            "10b7fac1c6554c9b914c01d6d3b9323e",
            "3af41423da134ac9877fbaf912bf9d96",
            "ce30ad3f8467486486c56742a3d21d1c",
            "320f4720121b47c0aa8c0905567fd646",
            "fd53fadb1e084deaa1a6fe6da2dfb607",
            "ae694332e2854838af64dcab9db15209",
            "a771ad7debda474c88ed4cf6c666fa4d",
            "787609e9baf048f8bd8e9e68ed00872d",
            "6b2a4cc37a7f41049138c5647d37bcd2",
            "53b7999fcf464008b6ca6aba5961235b",
            "145697feb4d945c0adf07282533589f7",
            "079a52496528467ab5bcb0c2f93f770b",
            "181839102fef4ba6bc6dcdb2f1d6ac60",
            "a14e1332fbaf491dac532d1dd5c70c0b",
            "164dd2c5fb954b6e8f88863ec05eec7a",
            "54e8111762974a36b143f2941e6884ab",
            "65685779c82d4d0280c77ff6e9c95a01",
            "a45565b3bcb547e491cc6b90773801f8",
            "84475dd17e684c2cadff7ef7a8352266",
            "f50bec32d78c451899e4087c865596be",
            "47c6c10f2cd349a78987bf3e70c27cd1",
            "bf3385a790bf4922afc86a575f935c7c",
            "9eb0f0985dec4dc9849388dd20f94f97",
            "78bf3c68eb954e24ae2900522c20419b",
            "38c5d0558db54799b0a1517f0871fe75",
            "4587c0b7ae18493b92c4a5b0dbb2cf2d",
            "39d699ff67ae470997d1b08099331e18",
            "04aaeb820387486fbc4e85bb980602bb"
          ]
        },
        "id": "WIBil0nBz4ZK",
        "outputId": "4bd6c60a-8ee3-4cad-93f5-d3011d006388"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "842e68b4b91448f4b1aedd42cf2f0a2b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b77c36aac62847b8a0a4c537fe775fa7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2fe344e2758a4fada7ba82c54c70b5ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/384 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aef4c341f56c41e4b5b5296ffa29d600"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6aa2607590854834bd767e151f96f1d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a37349374bf549cd92c110891ede004c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a3ad62b0cee642d286ce0d714badd425"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a771ad7debda474c88ed4cf6c666fa4d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a45565b3bcb547e491cc6b90773801f8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the training and eval datasets and prep them for fine tuning.\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = socratic_tokenizer_quantized.model_max_length\n",
        "print(f\"Max sequence length: {MAX_SEQUENCE_LENGTH}\", \"\\n\")\n",
        "\n",
        "def make_question_document_pairs(dataset):\n",
        "    question_document_pairs = []\n",
        "    for document, question in zip(dataset[\"document\"], dataset[\"question\"]):\n",
        "        question_document_pairs.append(f\"<ask&answer> {question} <qsep> {document}\")\n",
        "\n",
        "    return question_document_pairs\n",
        "\n",
        "def preprocess_socratic_batch(dataset, tokenizer):\n",
        "    question_document_pairs = make_question_document_pairs(dataset)\n",
        "\n",
        "    input_encoded = tokenizer.batch_encode_plus(\n",
        "        question_document_pairs,\n",
        "        max_length=MAX_SEQUENCE_LENGTH,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    labels_encoded = tokenizer.batch_encode_plus(\n",
        "        dataset[\"response\"],\n",
        "        max_length=MAX_SEQUENCE_LENGTH,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    return {'input_ids': input_encoded['input_ids'],\n",
        "            'labels': labels_encoded['input_ids']}\n",
        "\n",
        "train_encoded = train_dataset.map(\n",
        "    preprocess_socratic_batch,\n",
        "    batched=True,\n",
        "    fn_kwargs={\n",
        "      'tokenizer': socratic_tokenizer_quantized\n",
        "})\n",
        "\n",
        "val_encoded = dev_dataset.map(\n",
        "    preprocess_socratic_batch,\n",
        "    batched=True,\n",
        "    fn_kwargs={\n",
        "      'tokenizer': socratic_tokenizer_quantized\n",
        "})\n",
        "\n",
        "print()\n",
        "print(\"Train encoded:\", train_encoded, \"\\n\")\n",
        "print(\"Val encoded:\", val_encoded, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307,
          "referenced_widgets": [
            "876a64bcc333453d92815bd174e35aed",
            "a39de5c3faea48da89abbe90a237e158",
            "452540759cff443b8de14fccd617b4c2",
            "43b91c9cdfde4ee4ac71ba662e18a231",
            "7b93390defa54a60b7d391f56d3ec30d",
            "9ef43ce67571478688ab24b54897da6d",
            "efe26483832d4352ba804f613c621aa2",
            "8d28d74460c24804bbb9c96d0d2a4165",
            "5fe2e112403543e694d3e527213f95d9",
            "587e89aa09c74845a34f08453c1c3a0a",
            "73b97b8f679d4042800f03aa9716a3f5",
            "1634038deb6143fe92602dfb0f14525e",
            "59dad0a7302842f1b215d63dd994b4c0",
            "e92e66510a28428c91a3b25e2446c12b",
            "a1c268e201074861879a05e110710bd4",
            "35837857fb02405781d570b278291388",
            "d1578f0a99cf481b92d71d81254e3c3b",
            "8449a9b5ffac47f7b0d20c2db877c5ef",
            "5dcb932fc9624253884c395e738290b5",
            "176b10e905054827a06b275807b4ff96",
            "e3b6364345c34ffdaa447858a7219890",
            "3857f8e938d24c3886dcf4b83fa6a301"
          ]
        },
        "id": "0tZ-fFLIsuOz",
        "outputId": "892f901f-4ed5-43ad-ea03-70780b98b26b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max sequence length: 1024 \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "876a64bcc333453d92815bd174e35aed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1634038deb6143fe92602dfb0f14525e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train encoded: Dataset({\n",
            "    features: ['document', 'question', 'response', 'input_ids', 'labels'],\n",
            "    num_rows: 1000\n",
            "}) \n",
            "\n",
            "Val encoded: Dataset({\n",
            "    features: ['document', 'question', 'response', 'input_ids', 'labels'],\n",
            "    num_rows: 500\n",
            "}) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_encoded[0][\"input_ids\"])\n",
        "print(train_encoded[0][\"labels\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qa-35YLiNGO7",
        "outputId": "eaf573f8-264d-49f5-95cc-2698ad0438aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 50269, 3394, 16, 8001, 1825, 219, 116, 653, 2594, 7, 123, 1328, 5, 527, 116, 1437, 50266, 47507, 154, 13391, 870, 226, 1723, 7831, 16286, 4979, 975, 252, 3559, 75, 1050, 4, 252, 58, 402, 55, 578, 463, 402, 540, 578, 10010, 58, 6, 11, 765, 6, 9187, 18, 1991, 13, 7967, 328, 646, 19163, 438, 44260, 18, 6068, 35, 152, 364, 29015, 21, 2622, 31, 36580, 9, 318, 4662, 35320, 6, 772, 20990, 4, 19188, 17355, 557, 222, 45, 20489, 143, 1283, 14, 5, 121, 4, 104, 4, 4857, 15, 42, 5362, 21, 7867, 21838, 20, 44792, 25504, 8633, 8435, 8, 11491, 22597, 25, 69, 6684, 18569, 5668, 25169, 352, 11, 5, 475, 19873, 4084, 9, 5, 11355, 232, 751, 4, 264, 2551, 7, 28, 36844, 7, 253, 69, 17275, 18, 22379, 463, 259, 6, 80, 6317, 1109, 107, 31, 5, 2445, 40570, 15, 3875, 4, 5997, 8173, 4204, 12957, 11901, 17770, 8, 13314, 149, 69, 5179, 5924, 4, 8977, 272, 20143, 37893, 8, 6387, 81, 6, 3970, 13, 39, 10317, 4, 91, 21, 10, 380, 6, 6087, 12573, 196, 313, 6, 6254, 24503, 131, 53, 2724, 107, 9, 2640, 56, 11224, 159, 39, 10762, 8, 342, 1046, 12, 7068, 35780, 26395, 29, 223, 39, 39275, 4490, 2473, 4, 20, 999, 462, 11372, 227, 3875, 8, 69, 801, 28863, 58, 6744, 15, 5, 604, 54, 8468, 106, 122, 4, 91, 30573, 1329, 1706, 5, 797, 929, 6, 4435, 16250, 23, 5, 2016, 17244, 4, 19503, 7343, 10855, 1415, 62, 6, 41979, 10, 12953, 471, 23, 123, 25, 37, 1410, 1706, 5, 655, 12, 2739, 2838, 4728, 9, 30324, 3895, 4, 22, 23736, 6, 3045, 4, 370, 240, 10, 30749, 72, 22, 14783, 72, 91, 32305, 5, 2131, 3895, 396, 18469, 24, 6, 172, 2075, 10, 865, 420, 5, 2933, 35672, 5225, 15, 39, 23223, 4, 85, 115, 2067, 4, 22, 32663, 92, 148, 5, 363, 1917, 22, 21674, 10, 4039, 3089, 14247, 547, 402, 101, 10, 8825, 10, 410, 1319, 1926, 9, 201, 4, 252, 2263, 62, 59, 41, 1946, 536, 8, 22246, 8435, 160, 88, 5, 10722, 72, 20, 3089, 14247, 58, 10, 47812, 118, 21528, 9, 42, 5518, 59, 61, 5907, 1467, 932, 4, 252, 1415, 101, 81, 20629, 668, 21849, 6, 53, 2551, 7, 33, 41, 818, 45517, 20610, 59, 932, 1375, 15, 5, 1255, 4, 22, 2409, 84, 80, 12260, 2580, 18013, 8435, 66, 456, 4, 21998, 1432, 106, 6, 53, 685, 106, 11, 5, 22802, 330, 4, 38, 348, 1682, 10, 6029, 164, 7, 4704, 106, 124, 72, 272, 20143, 34576, 37498, 7, 1003, 4, 3875, 1705, 75, 1004, 66, 615, 1690, 4526, 225, 11, 5, 1304, 6, 98, 6177, 1159, 58, 145, 12502, 66, 13, 1058, 25, 12260, 2580, 15, 49, 11901, 39663, 4115, 4, 20, 80, 37, 1017, 4777, 6, 32610, 8, 11542, 10054, 6, 2551, 7, 28, 4940, 32095, 9, 143, 1472, 9, 8038, 4, 1525, 768, 89, 21, 117, 4678, 240, 13, 8038, 259, 4, 20, 3089, 14247, 5844, 75, 2551, 2702, 6, 8, 5, 400, 3122, 58, 4100, 70, 23826, 1879, 24477, 8, 27243, 4, 252, 58, 11355, 615, 6, 546, 101, 20289, 11, 14117, 9, 49, 3425, 39528, 6, 19, 4558, 31, 237, 7, 11971, 5856, 349, 15, 49, 2835, 196, 3738, 4, 9291, 8337, 101, 2702, 39498, 4, 125, 402, 56, 1102, 7, 5, 6942, 537, 23843, 107, 124, 6, 8, 7, 5, 55, 485, 3627, 223, 8001, 1825, 219, 14, 21, 1051, 7, 1649, 62, 4, 91, 1224, 7, 5, 4103, 7, 27655, 66, 23, 5, 5518, 4, 20, 4856, 12, 12528, 3778, 531, 28, 2227, 6, 187, 89, 21, 10, 14548, 1109, 4, 125, 5, 7992, 10722, 14, 8144, 5, 1445, 232, 25871, 6199, 63, 28496, 88, 10, 34008, 4, 286, 10, 464, 6, 24, 938, 75, 31832, 6, 600, 5, 1255, 21, 2913, 30, 7992, 33361, 29, 9, 11048, 4, 96, 5, 4472, 6, 5, 13657, 9, 15383, 19424, 14, 156, 10, 24168, 6693, 5921, 9725, 5718, 12, 12354, 4, 6434, 2485, 198, 106, 2528, 10, 19400, 9, 10943, 3122, 4, 10574, 58, 4703, 7, 192, 149, 5, 34008, 4, 1648, 5, 1844, 39966, 147, 51, 1017, 303, 8001, 1825, 219, 18, 7015, 8203, 3627, 21, 2198, 7397, 30, 5, 11048, 4, 345, 58, 130, 9, 5, 3089, 14247, 7950, 59, 81, 5, 26881, 3122, 122, 6, 25, 51, 747, 2551, 7, 109, 4, 272, 20143, 32392, 23, 106, 13, 10, 2289, 6, 667, 7, 1166, 1472, 88, 5, 383, 4, 318, 37, 56, 86, 7, 892, 106, 17220, 125, 89, 21, 117, 86, 4, 3875, 56, 2740, 123, 7, 6769, 2126, 259, 6, 71, 1618, 39, 7511, 9, 1844, 12, 43146, 10696, 44845, 15, 13485, 623, 6121, 6, 7, 1649, 15, 143, 1203, 9, 8001, 1825, 219, 4, 91, 1017, 57, 259, 10, 186, 1181, 87, 37, 197, 33, 4711, 416, 4, 318, 89, 21, 117, 1203, 11, 277, 183, 50, 98, 9, 99, 56, 1102, 7, 5, 604, 54, 1017, 31712, 49, 3627, 8, 63, 2104, 6, 37, 1017, 33, 7, 266, 124, 4, 91, 74, 33, 314, 137, 6, 114, 10, 485, 27320, 1588, 5844, 75, 4924, 615, 9, 5, 8203, 3627, 13, 39, 4204, 18327, 3629, 7, 1514, 31, 5, 935, 30, 6620, 4, 85, 56, 3334, 57, 7397, 1844, 615, 7, 25114, 5, 25458, 3249, 4, 22, 25158, 2901, 7343, 10855, 18, 2236, 847, 149, 39, 13362, 2961, 4, 22, 25158, 6, 89, 32, 5, 1159, 2901, 3224, 37, 115, 7021, 7, 1407, 69, 7547, 8411, 6, 2079, 2037, 39, 2295, 4, 20, 3089, 14247, 56, 314, 5, 19400, 4, 978, 5, 130, 58, 22246, 7520, 23, 5500, 2078, 7, 10, 1514, 583, 5, 3627, 6, 7, 30731, 2283, 352, 1065, 402, 14, 1410, 89, 4, 91, 794, 5, 80, 12260, 2580, 172, 6, 3393, 124, 7, 5, 2445, 3627, 6, 95, 1684, 5, 2079, 37, 1017, 450, 149, 5, 12241, 4, 11999, 21, 442, 5, 11048, 33361, 531, 33, 1348, 723, 1255, 4, 13676, 880, 7, 37, 4097, 19603, 4, 85, 21, 350, 444, 7, 192, 2563, 6, 53, 272, 20143, 7249, 5, 18896, 6, 16600, 88, 5, 3188, 1706, 2]\n",
            "[0, 39975, 1825, 219, 16, 65, 9, 5, 25043, 9, 10, 3627, 14, 21, 1051, 7, 1649, 15, 41, 6942, 537, 14, 9939, 4, 832, 308, 3627, 34536, 350, 6, 61, 16, 596, 272, 20143, 17, 27, 29, 3627, 16, 1051, 7, 1649, 62, 15, 5, 32359, 5518, 4, 2223, 8001, 1825, 219, 16, 3225, 802, 7, 33, 9939, 6, 39, 3627, 16, 303, 11, 10, 1844, 39966, 7397, 30, 11048, 4, 272, 20143, 8, 39, 3419, 1930, 5, 1647, 9, 49, 86, 667, 7, 465, 143, 22314, 9, 8001, 1825, 219, 50, 39, 3419, 4, 6811, 6, 24, 16, 1487, 14, 8001, 1825, 219, 56, 888, 15517, 8, 555, 402, 21468, 7, 65, 9, 5, 15916, 14, 51, 6376, 4, 91, 34, 9885, 141, 7, 1994, 2340, 2370, 25, 10, 898, 9, 519, 7, 464, 5, 2777, 198, 7, 146, 5, 4428, 2564, 4, 520, 272, 20143, 3457, 123, 6, 8001, 1825, 219, 2215, 14, 89, 32, 130, 741, 4526, 5526, 29, 23, 5, 12819, 6867, 7641, 131, 39, 7763, 979, 17, 27, 29, 2335, 34, 707, 181, 4489, 6, 8, 4146, 9, 106, 32, 6219, 4, 8001, 1825, 219, 172, 3026, 39, 527, 6, 8, 272, 20143, 3315, 123, 66, 7, 147, 10, 2180, 9, 27056, 16, 2445, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning"
      ],
      "metadata": {
        "id": "DpbWte_zCyj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training args and other parameters.\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"outputs\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    optim=\"paged_adamw_8bit\", #used with QLoRA\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=5,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to='none',\n",
        "    label_names=[\"labels\"]\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=socratic_tokenizer_quantized,\n",
        "    model=socratic_model_quantized)\n",
        "\n",
        "metric = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "\n",
        "    # decode preds and labels\n",
        "    labels = np.where(labels != -100, labels, socratic_tokenizer_quantized.pad_token_id)\n",
        "    decoded_preds = socratic_tokenizer_quantized.batch_decode(preds, skip_special_tokens=True)\n",
        "    decoded_labels = socratic_tokenizer_quantized.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # rougeLSum expects newline after each sentence\n",
        "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    return result\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=socratic_model_quantized,\n",
        "    args=training_args,\n",
        "    train_dataset=train_encoded,\n",
        "    eval_dataset=val_encoded,\n",
        "    processing_class=socratic_tokenizer_quantized,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jb6UTBGnCsRy",
        "outputId": "bbe2caa4-5195-4055-928a-2e1fbbac551d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model.\n",
        "os.environ['WANDB_MODE'] = 'disabled'\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "1AtBVdZEFnHz",
        "outputId": "67cc9936-18ac-46a2-bf98-0f67a5aa63fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [625/625 34:03, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Rougelsum</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>14.337400</td>\n",
              "      <td>16.056553</td>\n",
              "      <td>0.068407</td>\n",
              "      <td>0.020320</td>\n",
              "      <td>0.058269</td>\n",
              "      <td>0.065271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>12.310800</td>\n",
              "      <td>14.552622</td>\n",
              "      <td>0.068170</td>\n",
              "      <td>0.020540</td>\n",
              "      <td>0.058046</td>\n",
              "      <td>0.064998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>11.739500</td>\n",
              "      <td>13.481462</td>\n",
              "      <td>0.068712</td>\n",
              "      <td>0.021098</td>\n",
              "      <td>0.058550</td>\n",
              "      <td>0.065721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>10.898300</td>\n",
              "      <td>12.833299</td>\n",
              "      <td>0.068514</td>\n",
              "      <td>0.020666</td>\n",
              "      <td>0.058558</td>\n",
              "      <td>0.065449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>10.582700</td>\n",
              "      <td>12.605450</td>\n",
              "      <td>0.068662</td>\n",
              "      <td>0.021170</td>\n",
              "      <td>0.058759</td>\n",
              "      <td>0.065580</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=625, training_loss=12.49685390625, metrics={'train_runtime': 2044.4224, 'train_samples_per_second': 2.446, 'train_steps_per_second': 0.306, 'total_flos': 1.252572266496e+16, 'train_loss': 12.49685390625, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!rm -rf ./models/socraticpretraining_baseline-2025-07-26_215517/"
      ],
      "metadata": {
        "id": "1wwMnjrjPqGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "!pwd\n",
        "!mkdir -p ./models\n",
        "\n",
        "# Get the current time in the US Pacific time zone.\n",
        "timezone_obj = ZoneInfo(\"America/Los_Angeles\")\n",
        "current_time = datetime.now(timezone_obj)\n",
        "current_time = current_time.strftime(\"%Y-%m-%d_%H%M%S\")\n",
        "\n",
        "model_name = \"socraticpretraining_baseline-\" + str(current_time)\n",
        "trainer.save_model(f\"./models/{model_name}\")\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir -p \"/content/drive/MyDrive/DS266/project/models/{model_name}\"\n",
        "!cp -r ./models/{model_name}/* \"/content/drive/MyDrive/DS266/project/models/{model_name}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZDSaWiZLnsb",
        "outputId": "e1f2b519-9bfd-4b7c-d32d-4b9d49b50216"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DS266-ugarcia-bjulve\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    }
  ]
}
