{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "Xh69sEOcl97c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For Preprocessing\n",
        "!pip install -q -U datasets\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import json\n",
        "import random\n",
        "import os\n",
        "from datasets import Dataset, load_dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "waPq3v58qS84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Training\n",
        "\n",
        "!pip install -q -U torch torchvision torchaudio fastai\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U transformers\n",
        "!pip install -q -U peft\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U tokenizers\n",
        "!pip install -q -U evaluate\n",
        "!pip install -q -U rouge_score\n",
        "!pip install -q -U loralib einops xformers\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "import bitsandbytes\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    PeftConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        ")\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    BitsAndBytesConfig,\n",
        ")"
      ],
      "metadata": {
        "id": "pAHIvOeYeE9v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5795e1b-15e1-43d5-a262-fd7178e4f2c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m107.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.3/235.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m106.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m126.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m558.8/558.8 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.1/117.1 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility.\n",
        "RANDOM_SEED = 33\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "#tf.random.set_seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "torch.cuda.manual_seed(RANDOM_SEED)\n",
        "torch.cuda.manual_seed_all(RANDOM_SEED)"
      ],
      "metadata": {
        "id": "wifjrl9dKWBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "zwKWovoamBuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prep for download.\n",
        "%cd /content/\n",
        "!rm -rf DS266-ugarcia-bjulve\n",
        "!git clone https://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@github.com/bjulve-ischool/DS266-ugarcia-bjulve.git\n",
        "%cd DS266-ugarcia-bjulve\n",
        "!ls .\n",
        "\n",
        "train_file = 'data/v1-3/train.jsonl'\n",
        "dev_file = 'data/v1-3/dev.jsonl'\n",
        "test_file = 'data/v1-3/test.jsonl'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMbBByFGl8oq",
        "outputId": "6f1426d2-08de-40e2-f38c-93371c473ba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'DS266-ugarcia-bjulve'...\n",
            "remote: Enumerating objects: 32, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 32 (delta 8), reused 4 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (32/32), 3.34 MiB | 6.14 MiB/s, done.\n",
            "Resolving deltas: 100% (8/8), done.\n",
            "/content/DS266-ugarcia-bjulve\n",
            "Baseline_Model_Evaluation.ipynb  QFS_Datasets.ipynb\n",
            "data\t\t\t\t README.md\n",
            "EDA2.ipynb\t\t\t Socratic_FT_Data_Augmentation.ipynb\n",
            "EDA.ipynb\t\t\t Socratic_Pretrained_Sampler.ipynb\n",
            "outputs\t\t\t\t T5Gemma_Sampler.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper to load the data into memory.\n",
        "def load_data(file_path):\n",
        "  with open(file_path) as f:\n",
        "      lines = f.read().split(\"\\n\")[:-1]\n",
        "\n",
        "  document_question_response = []\n",
        "  for line in lines:\n",
        "      data = json.loads(line)\n",
        "      # Remove extra white space. Since the tokenizer is subword\n",
        "      # and not sentence, then the newlines will not likely affect\n",
        "      # the word embedding underlying meaning.\n",
        "      document = \" \".join(data[\"document\"].split())\n",
        "      questions = data[\"questions\"]\n",
        "      for question in questions:\n",
        "          question_text = \" \".join(question[\"question_text\"].split())\n",
        "          responses = question[\"responses\"]\n",
        "          for response in responses:\n",
        "              response_text = \" \".join(response[\"response_text\"].split())\n",
        "              document_question_response.append((document, question_text, response_text))\n",
        "\n",
        "  return document_question_response\n",
        "\n",
        "\n",
        "# Get the data. Preserve the original splits.\n",
        "train_triplets = load_data(train_file)\n",
        "dev_triplets = load_data(dev_file)\n",
        "test_triplets =  load_data(test_file)\n",
        "print(\"Train:\", len(train_triplets))\n",
        "print(\"Dev:\", len(dev_triplets))\n",
        "print(\"Test:\", len(test_triplets))\n",
        "\n",
        "# Create a HF dataset. Shuffle the order\n",
        "# before returning it.\n",
        "def make_dataset(triplets):\n",
        "    documents, questions, responses = zip(*triplets)\n",
        "    documents = list(documents)\n",
        "    questions = list(questions)\n",
        "    responses = list(responses)\n",
        "\n",
        "    dataset = Dataset.from_dict({\"document\": documents, \"question\": questions, \"response\": responses})\n",
        "    return dataset.shuffle(seed=RANDOM_SEED)\n",
        "\n",
        "train_dataset = make_dataset(train_triplets)\n",
        "dev_dataset = make_dataset(dev_triplets)\n",
        "test_dataset = make_dataset(test_triplets)\n",
        "\n",
        "# Print a sample.\n",
        "random_sample = random.choice(train_dataset)\n",
        "random_document, random_question, random_response = random_sample[\"document\"], random_sample[\"question\"], random_sample[\"response\"]\n",
        "print(\"\\nRANDOM SAMPLE:\\n\")\n",
        "print(f\"\\033[1mDocument:\\033[0m {random_document[:50]}\", \"\\n\")\n",
        "print(f\"\\033[1mQuestion:\\033[0m {random_question}\", \"\\n\")\n",
        "print(f\"\\033[1mResponse:\\033[0m {random_response}\", \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5puHhhOmRGK",
        "outputId": "90e676f6-da2d-4c0a-eadb-888e29e48d92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 1000\n",
            "Dev: 500\n",
            "Test: 1040\n",
            "\n",
            "RANDOM SAMPLE:\n",
            "\n",
            "\u001b[1mDocument:\u001b[0m THE MAN OUTSIDE By EVELYN E. SMITH Illustrated by  \n",
            "\n",
            "\u001b[1mQuestion:\u001b[0m What is the relationship between Martin and Ives? \n",
            "\n",
            "\u001b[1mResponse:\u001b[0m Cousin Ives enters Martin’s life when he is a little older, and is the third descendant to accompany him as his guardian. Out of all his descendants to assume guardianship, Martin forms the closest relationship with Ives. Rather than seeing Martin as a responsibility and duty, Ives sees Martin as an individual and seeks ways to connect and encourage his passions. For one, Ives buys a yacht named The Interregnum to which the pair take upon themselves to explore the current world in. They traveled across the waters and inland to see both the civilized and uncivilized world, with Martin taking it all in. When it was just the two of them, their relationship progressed further. Ives began to open up about the future world that he and his descendants come from and explain the nuances of the social order that rules. Ives is the first to explicitly and honestly describe the feudal and privileged social class that Martin’s descendants take part in, only due to their fortunate ancestry. Additionally, Ives is the only cousin to admit the potential truth in Conrad’s intentions, noting the dilemma between achieving moral good and selfishing maintaining their own good life. Martin even comments his confidence in Ives being able to see the obvious flaw in the cousins’ plans. However, during one winter, Ives fell ill to a severe chill and passed away before his own birth. After Ives’ death, Martin relently voyages across oceans and soon as they and the cousins blur, he begins to live detachedly. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation (Socratic FT samples)"
      ],
      "metadata": {
        "id": "52yHhk3fvxRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First get a subset of the original training examples. Make sure to\n",
        "# stratify it according to the length of the responses, ideally we\n",
        "# will select from the full breadth of the response (summary) length\n",
        "# distribution.\n",
        "\n",
        "df = train_dataset.to_pandas()\n",
        "df['response_length'] = pd.cut(df['response'].apply(len),\n",
        "                                bins=3,\n",
        "                                labels=['short', 'medium', 'long'])\n",
        "df['response_count'] = df['response'].apply(len)\n",
        "grouped_by_length = df.groupby('response_length', observed=True)\n",
        "\n",
        "# Note that the frac value here is based on our use case: the Pagnoni\n",
        "# paper suggested augmenting only 25% of your training set.\n",
        "stratified_sample_proportion = grouped_by_length.apply(lambda x: x.sample(frac=0.25), include_groups = False)\n",
        "stratified_sample_proportion = stratified_sample_proportion.reset_index(drop=True)\n",
        "\n",
        "# Get the remaining samples. We will use these for data augmentation.\n",
        "remaining_samples = df[~df.index.isin(stratified_sample_proportion.index)]\n",
        "\n",
        "\n",
        "# Set back to HF DataSet.\n",
        "stratified_sample_proportion = stratified_sample_proportion.drop('response_count', axis=1)\n",
        "train_dataset_stratified = Dataset.from_pandas(stratified_sample_proportion)\n",
        "\n",
        "remaining_samples = remaining_samples.drop('response_count', axis=1)\n",
        "train_dataset_remaining = Dataset.from_pandas(remaining_samples)\n",
        "\n",
        "print(\"Stratified Samples:\", train_dataset_stratified.num_rows, \"\\n\")\n",
        "print(\"Remaining samples:\", train_dataset_remaining.num_rows)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWsaMNQtvw1o",
        "outputId": "97544d39-df5d-4a26-9fb1-f5febad7a041"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stratified Samples: 250 \n",
            "\n",
            "Remaining samples: 750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, create the classes and functions to generate the augmented samples. These\n",
        "# will have special masking tokens that need to be preprocessed accordingly before\n",
        "# tokenization.\n",
        "from sortedcontainers import SortedList\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "import evaluate\n",
        "rouge = evaluate.load('rouge')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Create this data structure to process each document's\n",
        "# sentences in one pass (one loop). See step 1 below.\n",
        "class ScoredSentence(object):\n",
        "    def __init__(self, index, score, sentence):\n",
        "        self.index = index\n",
        "        self.score = score\n",
        "        self.sentence = sentence\n",
        "    def __lt__(self, other):\n",
        "        return self.score < other.score\n",
        "    def __repr__(self):\n",
        "        s = self.sentence\n",
        "        if len(s) > 5:\n",
        "            s = s[:5] + \"...\"\n",
        "        return f\"ScoredSentence(index={self.index}, score={self.score}, sentence='{s}')\"\n",
        "\n",
        "class TopScoredSentences(object):\n",
        "    def __init__(self, m):\n",
        "        self.m = m\n",
        "\n",
        "        # Sorted in reverse order.\n",
        "        sort_scored_sentences = lambda scored_sentence: -1 * scored_sentence.score\n",
        "        self.scored_sentences = SortedList([], key=sort_scored_sentences)\n",
        "    def __repr__(self):\n",
        "        lst = \"\"\n",
        "        for scored_sentence in self.scored_sentences:\n",
        "          lst += f\"{scored_sentence}, \"\n",
        "        lst.rstrip(\",\")\n",
        "        return f\"TopScoredSentences([{lst}])\"\n",
        "    def __len__(self):\n",
        "        return len(self.scored_sentences)\n",
        "    def get_indexes(self):\n",
        "      indexes = []\n",
        "      for scored_sentence in self.scored_sentences:\n",
        "        indexes.append(scored_sentence.index)\n",
        "      indexes.sort()\n",
        "      return indexes\n",
        "    def maintain_top_m(self, ScoredSentence):\n",
        "      # m is the number of sentences you want to keep in\n",
        "      # scored_sentences. The top m scores will be kept.\n",
        "      # If scored_sentences length is >= m, then if ScoredSentence\n",
        "      # is > the last item in the list, then pop off the last\n",
        "      # item and add the new ScoredSentence. Otherwise, if\n",
        "      # scored_sentences length < m, then there's room so just\n",
        "      # add the new ScoredSentence.\n",
        "      if len(self.scored_sentences) >= self.m:\n",
        "        if self.scored_sentences[-1] < ScoredSentence:\n",
        "          self.scored_sentences.pop()\n",
        "          self.scored_sentences.add(ScoredSentence)\n",
        "      else:\n",
        "        self.scored_sentences.add(ScoredSentence)\n",
        "    def get_pseudo_summary(self, truncate=False, truncate_length=256):\n",
        "      pseudo_summary = \"\"\n",
        "      for scored_sentence in self.scored_sentences:\n",
        "        pseudo_summary += scored_sentence.sentence + \" \"\n",
        "      pseudo_summary = pseudo_summary.rstrip()\n",
        "\n",
        "      # Needed for the Pagnoni model which has limited input tokens.\n",
        "      if truncate:\n",
        "        tokens = word_tokenize(pseudo_summary)\n",
        "        num_tokens = len(tokens)\n",
        "        truncated = tokens[:truncate_length] if num_tokens > truncate_length else tokens\n",
        "        reconstructed_text = \" \".join(truncated)\n",
        "        pseudo_summary = reconstructed_text\n",
        "      return pseudo_summary\n",
        "\n",
        "# Now generate the masked document dataset. This will be used for\n",
        "# fine tuning of the model. Note that the Pagnoni paper suggested\n",
        "# augmenting only 25% of your training samples so as not to bias\n",
        "# your model to generate questions. For the first test, we are\n",
        "# going to have 1000 total training samples as in the original\n",
        "# set, so we will need to generate 250 augmented examples.\n",
        "\n",
        "# Algorithm:\n",
        "# 1. Select the most salient sentences from the input document.\n",
        "#     - Used the PEGASUS-style Gap Sentence Generation (GSG) approach to select sentences.\n",
        "#     - Ensure a Gap Sentence Ratio (GSR) of 45%, meaning 45% of the sentences of a document\n",
        "#       will appear in the pseudo summary.\n",
        "#     - Following the Pagnoni paper suggestion, 80% of the selected sentences will appear\n",
        "#       masked in the document, and 20% will remain unmasked. This is to encourage the model\n",
        "#       to copy information at times from the input to the summary.\n",
        "#     - Also following the Pagnoni paper, will truncate the unmasked document text. The paper\n",
        "#       suggested 512 tokens, but in our case we will just grab the first 10% of the input\n",
        "#       text as they are short stories on the order of 5k-10k words.\n",
        "#     - At the end of step 1, we should have the following objects for each sample:\n",
        "#         + Unmasked document.\n",
        "#         + Masked document.\n",
        "#         + Pseudo summary.\n",
        "# 2. Generate questions for each sentence in the pseudo summary.\n",
        "#     - Per Pagnoni et al., use MixQG to generate questions, with the pseudo summary as the\n",
        "#       context and the sentence as the answer. This will encourage the model to consider\n",
        "#       relevant questions given the specific context.\n",
        "#     - With the questions, create new samples to add to the training set until the desired number is reached.\n",
        "#         + Will need to generate 250 new samples.\n",
        "#         + Each sample will consist of the masked document as the input, and a [question<qsep>pseudo_summary]\n",
        "#           string as the output.\n",
        "#     - At the end of step 2, we should have the following objects for each sample:\n",
        "#         + Unmasked document.\n",
        "#         + Masked document.\n",
        "#         + Pseudo summary.\n",
        "#         + A question.\n",
        "\n",
        "# 1. Get salient sentences.\n",
        "def get_sentences(text):\n",
        "  return nltk.sent_tokenize(text)\n",
        "\n",
        "def truncate_tokens(text, max):\n",
        "  tokens = word_tokenize(text)\n",
        "  num_tokens = len(tokens)\n",
        "  truncated = tokens[:max] if num_tokens > max else tokens\n",
        "  reconstructed_text = \" \".join(truncated)\n",
        "  reconstructed_text_length = len(reconstructed_text)\n",
        "  truncated_text = text[:reconstructed_text_length] if reconstructed_text_length < len(text) else text\n",
        "  return truncated_text\n",
        "\n",
        "def compute_rouge1(sentence, text):\n",
        "    rouge_scores = rouge.compute(predictions=[sentence], references=[text])\n",
        "    return rouge_scores[\"rouge1\"]\n",
        "\n",
        "def select_salient_sentences(sentences, text, metric, gsr=.45):\n",
        "  # Compute how many sentences to select.\n",
        "  if type(gsr) == int:\n",
        "    pseudo_summary_sentence_count = gsr\n",
        "  else:\n",
        "    pseudo_summary_sentence_count = int(len(sentences) * gsr)\n",
        "\n",
        "  # Next, for each sentence, assign a metric\n",
        "  # score for the sentence against the rest of\n",
        "  # the text. This score will be used to maintain\n",
        "  # the top m scoring sentences from the document.\n",
        "  top_scored_sentences = TopScoredSentences(pseudo_summary_sentence_count)\n",
        "  i = 0\n",
        "  for sentence in sentences:\n",
        "    score = metric(sentence, text)\n",
        "    top_scored_sentences.maintain_top_m(ScoredSentence(i, score, sentence))\n",
        "    i += 1\n",
        "\n",
        "  return top_scored_sentences\n",
        "\n",
        "def generate_masked_document(unmasked_document_sentences, top_scored_sentences):\n",
        "  pseudo_summary_indexes = top_scored_sentences.get_indexes()\n",
        "\n",
        "  # Per the Pagnoni paper, only select 80% of these pseudo summary\n",
        "  # sentences to mask in the document. Leave about 20% overlap between\n",
        "  # the masked document and the pseudo summary to encourage the model\n",
        "  # to copy.\n",
        "  num_items_to_select = int(len(pseudo_summary_indexes) * .8)\n",
        "  if num_items_to_select > len(pseudo_summary_indexes):\n",
        "        num_items_to_select = len(pseudo_summary_indexes)\n",
        "  selected_indexes = random.sample(pseudo_summary_indexes, num_items_to_select)\n",
        "\n",
        "  selected_elements = [unmasked_document_sentences[i] for i in range(len(unmasked_document_sentences)) if i not in selected_indexes]\n",
        "  concatenated_text = \" \".join(selected_elements) # Concatenates into a single string\n",
        "  return concatenated_text\n",
        "\n",
        "# 2. Generate a salient question.\n",
        "\n",
        "# MixQG Question Generation system.\n",
        "\n",
        "from transformers import pipeline\n",
        "mixQG = pipeline(\"text2text-generation\", model='Salesforce/mixqg-base', tokenizer='Salesforce/mixqg-base')\n",
        "\n",
        "# Remember that the DataSet format is: DataSet({ document:[], question:[], response:[]})\n",
        "# For these augmented samples:\n",
        "#   - Document is the masked document text.\n",
        "#   - Question is the generated question from mixQG.\n",
        "#   - Response is a string of the form: [question<qsep>pseudo_summary]\n",
        "def generate_question_pseudo_summary_sample(masked_document_text, top_scored_sentences):\n",
        "  context = top_scored_sentences.get_pseudo_summary()\n",
        "  # Grab the most salient sentence to be the answer for the MixQG model. This is the first\n",
        "  # item in the sorted list since it is sorted by score in descending order.\n",
        "  answer = top_scored_sentences.scored_sentences[0].sentence\n",
        "  question = mixQG(f\"{answer} \\\\n {context}\")[0][\"generated_text\"]\n",
        "  return (\n",
        "      \"[Ask&Answer][Mask]\" + masked_document_text,\n",
        "      question,\n",
        "      f\"{question}[QSep]{context}\")\n",
        "\n",
        "# Putting it all together.\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def socratic_augment(dataset):\n",
        "  augmented_dict = {\"document\": [], \"question\": [], \"response\": []}\n",
        "  total = dataset.num_rows\n",
        "  print(\"Socratic Augment Samples:\", total)\n",
        "  pbar = tqdm(dataset)\n",
        "  for sample in pbar:\n",
        "    pbar.set_description_str(\"Truncate tokens\")\n",
        "    document_text = truncate_tokens(sample[\"document\"], 450)\n",
        "    pbar.set_description_str(\"Get sentences\")\n",
        "    document_sentences = get_sentences(document_text)\n",
        "    dsl = len(document_sentences)\n",
        "    dtl = len(document_text)\n",
        "    pbar.set_description_str(f\"Select by salience score (s={dsl}, t={dtl})\")\n",
        "    top_scored_sentences = select_salient_sentences(document_sentences, document_text, compute_rouge1)\n",
        "    pbar.set_description_str(\"Generate mask document\")\n",
        "    masked_document_text = generate_masked_document(document_sentences, top_scored_sentences)\n",
        "    pbar.set_description_str(\"Make pseudo summary samples\")\n",
        "    doc, q, r = generate_question_pseudo_summary_sample(masked_document_text, top_scored_sentences)\n",
        "    augmented_dict[\"document\"].append(doc)\n",
        "    augmented_dict[\"question\"].append(q)\n",
        "    augmented_dict[\"response\"].append(r)\n",
        "  return Dataset.from_dict(augmented_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360,
          "referenced_widgets": [
            "dea6074a6bdd49cf8deac45c2fa47b50",
            "82130533257a4beda3cfeeac139f1d54",
            "d80804d52ef04778aa73fd5675101927",
            "0ad41bfd0d604bb5ad0cab672213fadc",
            "7b9fd059dff44ab1bdeeeff066a09f36",
            "d9cef16349e547cd806227307de4e380",
            "fe9fefe7f7c14838a7f5731c8871c9fe",
            "2bdb8fdc0e8a451cb4465f487fc1903f",
            "44305ac3d9a34ac395868581a22c02ed",
            "81942f3220224d13842cf63e355d3887",
            "38b6e48216e14462ab5a9e8a7702f9c7",
            "1e2bdd6472e74587bab7931dd064d09e",
            "790c03a97c334f28b556ade66585f43e",
            "58cda1874299460aa914c8ed2f4f7eb1",
            "359e28391f1c4300aac21550c34953e4",
            "77db026d58a640878fcea0f19a0b43e9",
            "790d4a54a7874f0db55c6a3fa707787f",
            "9f3452afa1ae490586ed891f10d36e6c",
            "08c1a670d9d849139e66705d609124ea",
            "24ac6655cd6c403a81a06d5c924b7471",
            "d4cb0a3dcb5843329ed53910997eebd0",
            "1ced3b0563134d3b92547e54f3d848d4",
            "fa1be01107814c959cbdb54cb454e418",
            "aea7208d980c422b826737bcdc4b976d",
            "aea75afc45a24002adc7be7ea00de45d",
            "e9001e7b8c9a4c60b0ea63bac82b7e9f",
            "615fa381c2d948c4b49f8a2bad61f6cd",
            "72e1ed039a5c4078b0a77ae9dba3227b",
            "cd37e34c2ea742c29b0505979fb4616e",
            "b0e4639b3e4d49e082c0642033354752",
            "e8a6a8ac02fd4354b280557c7f0f9f1e",
            "077edac74c854a0590f9641461df6ed7",
            "a7598906393541aeae881d4a83ad11b2",
            "c78687f29f3644eeb7ff7ff19655e000",
            "4d4adb74900645debde52a2fbc6afcf7",
            "da32ea6626f744f99560da360eb5f87f",
            "606c8fe3f6cd4051aed0fb64605e9b89",
            "0156e0279bf24a32b1b1cdc9f5d8789a",
            "ef5cfe0d33b14441a20f167bfc9241d1",
            "584a49670d9042cc8ef61eb1230a38a3",
            "a3569ee30a4d4fcea8d81408eb17a253",
            "ca9e59531f194a99b0fc3ffeb2cbc860",
            "035a002384d6423f83188d1166f7f72c",
            "adc9452cc915423db7d6956ecebd0515",
            "4891cf3de96449efba1cb669c0bfdca7",
            "242f7d7b2aca477a8aa092fc404ed030",
            "c74f7567f0f04e56a3fcdc0607e6e9a2",
            "91695f3f72fa4a8999404de915911888",
            "c6040ba1f83f4a658b753572e628ab49",
            "0cce89e6b65b4d42b49228b5e51cb0fe",
            "5088eb698f6f48f7a81fe04cb6df3b85",
            "b57eb54a89df4b0dab723d572765b5e0",
            "ef4d01b271f84e42b41c4c676f431f8c",
            "808d473b302b4a05890b1afe4c073701",
            "5bc48f406510424f89f30e0860ba1776",
            "c4990d49d95e489fa591ea7732964956",
            "9c3570a7ef674f1eb479a4c45c7c3db4",
            "884b0e081ee84d07a8b9b1979b137190",
            "a03ba66be8cd4850aa9a32380d47b74a",
            "4ec6297121044318b04a66d31866c15b",
            "27a428aafee14feb9aaf2f84b0e5b146",
            "dc947058f32045c1bdd466a1be3ea441",
            "0b9f670048aa4b538874a7d1adfc8cd6",
            "e1b314f0157b47ed9cd6e3bf2ae70d51",
            "a462cc42c6f6450c943173481fb880dd",
            "17555100d7b74d3c81864c25a39ee9c5",
            "09b61c0041dd40f6a8ec575419a9d4d0",
            "c8860ec93dcb44e2b1bb03668d7dde92",
            "853ce35bd2eb4986a651471bfd48b9b9",
            "faaa1abdc70f47c49ed4f88ce4dc2829",
            "4bd5f13fb1cd447bb8c76db6b94a7242",
            "a0f0bee0006e45e69fc10d55c4983ff3",
            "2edf51bc8d7341c98ded69a00d782723",
            "9c3287c9dc6d43049e6fc1a1b9a639be",
            "729403f80124412a935bae7478815277",
            "e52d3258a82b46b4a4eb3e6b6120b183",
            "00c0d9f771fc42b3996197a08e426ba3",
            "b31484f025dc43b69df609767072028c",
            "6da5fff2e0cc4be693899e9de5995b1a",
            "3392740340494450a6698e8677d47a4f",
            "522662c984444eb98ae198f89f50dca5",
            "e140c5b98add4f7ebfa59e38f80d6e4b",
            "910d2105891940d79b09de11b783113c",
            "267d9a82b82448e18452751b0bbf28b4",
            "d7b563f5087a46818a35fa87457e1d83",
            "492193339cf149b98c85e83737e34291",
            "da1c7ed5584843f8aa6122164affb59a",
            "6b264859aa2f4ec9a8f83571ab2d678e"
          ]
        },
        "id": "biSj_-WPwsKp",
        "outputId": "66cccee9-cfab-46e4-f0b2-f298dd99f3c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dea6074a6bdd49cf8deac45c2fa47b50"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e2bdd6472e74587bab7931dd064d09e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa1be01107814c959cbdb54cb454e418"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c78687f29f3644eeb7ff7ff19655e000"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4891cf3de96449efba1cb669c0bfdca7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4990d49d95e489fa591ea7732964956"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09b61c0041dd40f6a8ec575419a9d4d0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b31484f025dc43b69df609767072028c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import concatenate_datasets, Dataset\n",
        "\n",
        "# Generate the augmented samples from the stratified 250\n",
        "# that we sampled.\n",
        "train_dataset_stratified_augmented = socratic_augment(train_dataset_stratified)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "acb147a1e9a742d79249b1c7fc8f1677",
            "354dc927c97e4ff597d299119c268970",
            "061a8dc1e7f346c1a76a95ec278b90df",
            "4633bfbddaaa4b248ba1d38542bd0ce1",
            "edbaeac96b164bddab4e2574c1e7d3dd",
            "e99776029353479a8e22fe7abac46318",
            "feeec6c91b414f65939441a6a151fe65",
            "2d782d606f8c47b197cf501a661dd3ef",
            "e2eb8f10120349d688de1d95b28cc57d",
            "febe33f3a4ac497a8c6f277a99a6000a",
            "d6f45e10b2594061ab2c8e3c901f4a8b"
          ]
        },
        "id": "Qd0Hh-CtxQkk",
        "outputId": "52217567-ba38-4e91-d706-7e44b25ae4fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Socratic Augment Samples: 250\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/250 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "acb147a1e9a742d79249b1c7fc8f1677"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# For dev. Sample code to add special tokens.\n",
        "# from nltk.tokenize import word_tokenize\n",
        "\n",
        "# text = train_dataset_stratified_augmented[0][\"response\"]\n",
        "# print(text)\n",
        "\n",
        "# test_tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/squality-socratic-books-30M\")\n",
        "\n",
        "# special_tokens_dict = {\"additional_special_tokens\": [\"[Ask&Answer]\", \"[Mask]\", \"[QSep]\"]}\n",
        "# num_added_toks = test_tokenizer.add_special_tokens(special_tokens_dict)\n",
        "# print(f\"Added {num_added_toks} tokens to the tokenizer.\")\n",
        "\n",
        "# tokens = test_tokenizer.tokenize(text)\n",
        "# print(len(test_tokenizer))\n",
        "# #print(len(socratic_tokenizer_quantized))\n",
        "# qsep_index = test_tokenizer.all_special_tokens.index(\"[QSep]\")\n",
        "# print(test_tokenizer.all_special_tokens)\n",
        "# print(test_tokenizer.all_special_ids)\n",
        "# print(tokens)\n",
        "# print(qsep_index, test_tokenizer.all_special_ids[qsep_index])\n",
        "# encoded = test_tokenizer(text)\n",
        "# print(encoded)\n",
        "# find_i = encoded[\"input_ids\"].index(test_tokenizer.all_special_ids[qsep_index])\n",
        "# print(find_i)\n",
        "# encoded[\"input_ids\"][find_i] = -100\n",
        "# print(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsHVvcalZAqe",
        "outputId": "501c76a1-d585-4778-b789-4258f21f2355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What was the reason for the short man standing next to the pair?[QSep]There was a short man standing next to the pair—young, as most men and women were in that time, thanks to the science which could stave off decay, though not death—but with no other apparent physical virtue, for plastic surgery had not fulfilled its bright promise of the twentieth century. Everyone in the room was aware of the big young man, and most of the humans present were resentful, for he handled himself consciously and arrogantly, as if his appearance alone were enough to make him superior to anyone. So did the light-haired girl at his side, and so did the nondescript man in the gray suit who was watching them from a booth in the corner. Now he was not only a rather ugly little man, but also a rather ridiculous one—or at least he felt he was, which was what mattered. \"You must allow me to pay your cleaning bill,\" Gabe said, taking out his wallet and extracting several credit notes without seeming to look at them. Even the girl with him was growing restless, for she was accustomed to adulation herself, and next to Gabriel Lockard she was almost ordinary-looking. As for the extraterrestrials—it was a free bar—they were merely amused, since to them all men were pathetically and irredeemably hideous. Bodyguard By CHRISTOPHER GRIMM Illustrated by CAVAT [Transcriber's Note: This etext was produced from Galaxy Science Fiction February 1956. The drink he had been raising to his lips splashed all over his clothing; the glass shattered at his feet. Extensive research did not uncover any evidence that the U.S. copyright on this publication was renewed.]\n",
            "Added 3 tokens to the tokenizer.\n",
            "50277\n",
            "50274\n",
            "['<s>', '</s>', '<unk>', '<pad>', '<mask>', '[Ask&Answer]', '[Mask]', '[QSep]']\n",
            "[0, 2, 3, 1, 50264, 50274, 50275, 50276]\n",
            "['What', 'Ġwas', 'Ġthe', 'Ġreason', 'Ġfor', 'Ġthe', 'Ġshort', 'Ġman', 'Ġstanding', 'Ġnext', 'Ġto', 'Ġthe', 'Ġpair', '?', '[QSep]', 'There', 'Ġwas', 'Ġa', 'Ġshort', 'Ġman', 'Ġstanding', 'Ġnext', 'Ġto', 'Ġthe', 'Ġpair', 'âĢĶ', 'young', ',', 'Ġas', 'Ġmost', 'Ġmen', 'Ġand', 'Ġwomen', 'Ġwere', 'Ġin', 'Ġthat', 'Ġtime', ',', 'Ġthanks', 'Ġto', 'Ġthe', 'Ġscience', 'Ġwhich', 'Ġcould', 'Ġst', 'ave', 'Ġoff', 'Ġdecay', ',', 'Ġthough', 'Ġnot', 'Ġdeath', 'âĢĶ', 'but', 'Ġwith', 'Ġno', 'Ġother', 'Ġapparent', 'Ġphysical', 'Ġvirtue', ',', 'Ġfor', 'Ġplastic', 'Ġsurgery', 'Ġhad', 'Ġnot', 'Ġfulfilled', 'Ġits', 'Ġbright', 'Ġpromise', 'Ġof', 'Ġthe', 'Ġtwentieth', 'Ġcentury', '.', 'ĠEveryone', 'Ġin', 'Ġthe', 'Ġroom', 'Ġwas', 'Ġaware', 'Ġof', 'Ġthe', 'Ġbig', 'Ġyoung', 'Ġman', ',', 'Ġand', 'Ġmost', 'Ġof', 'Ġthe', 'Ġhumans', 'Ġpresent', 'Ġwere', 'Ġresent', 'ful', ',', 'Ġfor', 'Ġhe', 'Ġhandled', 'Ġhimself', 'Ġconsciously', 'Ġand', 'Ġarrog', 'antly', ',', 'Ġas', 'Ġif', 'Ġhis', 'Ġappearance', 'Ġalone', 'Ġwere', 'Ġenough', 'Ġto', 'Ġmake', 'Ġhim', 'Ġsuperior', 'Ġto', 'Ġanyone', '.', 'ĠSo', 'Ġdid', 'Ġthe', 'Ġlight', '-', 'haired', 'Ġgirl', 'Ġat', 'Ġhis', 'Ġside', ',', 'Ġand', 'Ġso', 'Ġdid', 'Ġthe', 'Ġnond', 'esc', 'ript', 'Ġman', 'Ġin', 'Ġthe', 'Ġgray', 'Ġsuit', 'Ġwho', 'Ġwas', 'Ġwatching', 'Ġthem', 'Ġfrom', 'Ġa', 'Ġbooth', 'Ġin', 'Ġthe', 'Ġcorner', '.', 'ĠNow', 'Ġhe', 'Ġwas', 'Ġnot', 'Ġonly', 'Ġa', 'Ġrather', 'Ġugly', 'Ġlittle', 'Ġman', ',', 'Ġbut', 'Ġalso', 'Ġa', 'Ġrather', 'Ġridiculous', 'Ġone', 'âĢĶ', 'or', 'Ġat', 'Ġleast', 'Ġhe', 'Ġfelt', 'Ġhe', 'Ġwas', ',', 'Ġwhich', 'Ġwas', 'Ġwhat', 'Ġmattered', '.', 'Ġ\"', 'You', 'Ġmust', 'Ġallow', 'Ġme', 'Ġto', 'Ġpay', 'Ġyour', 'Ġcleaning', 'Ġbill', ',\"', 'ĠGabe', 'Ġsaid', ',', 'Ġtaking', 'Ġout', 'Ġhis', 'Ġwallet', 'Ġand', 'Ġextracting', 'Ġseveral', 'Ġcredit', 'Ġnotes', 'Ġwithout', 'Ġseeming', 'Ġto', 'Ġlook', 'Ġat', 'Ġthem', '.', 'ĠEven', 'Ġthe', 'Ġgirl', 'Ġwith', 'Ġhim', 'Ġwas', 'Ġgrowing', 'Ġrestless', ',', 'Ġfor', 'Ġshe', 'Ġwas', 'Ġaccustomed', 'Ġto', 'Ġad', 'ulation', 'Ġherself', ',', 'Ġand', 'Ġnext', 'Ġto', 'ĠGabriel', 'ĠLock', 'ard', 'Ġshe', 'Ġwas', 'Ġalmost', 'Ġordinary', '-', 'looking', '.', 'ĠAs', 'Ġfor', 'Ġthe', 'Ġextrater', 'rest', 'ri', 'als', 'âĢĶ', 'it', 'Ġwas', 'Ġa', 'Ġfree', 'Ġbar', 'âĢĶ', 'they', 'Ġwere', 'Ġmerely', 'Ġamused', ',', 'Ġsince', 'Ġto', 'Ġthem', 'Ġall', 'Ġmen', 'Ġwere', 'Ġpat', 'hetically', 'Ġand', 'Ġir', 'red', 'eem', 'ably', 'Ġhideous', '.', 'ĠBody', 'guard', 'ĠBy', 'ĠCHRIST', 'OP', 'HER', 'ĠGR', 'IM', 'M', 'ĠIllustrated', 'Ġby', 'ĠCA', 'V', 'AT', 'Ġ[', 'Trans', 'c', 'riber', \"'s\", 'ĠNote', ':', 'ĠThis', 'Ġe', 'text', 'Ġwas', 'Ġproduced', 'Ġfrom', 'ĠGalaxy', 'ĠScience', 'ĠFiction', 'ĠFebruary', 'Ġ1956', '.', 'ĠThe', 'Ġdrink', 'Ġhe', 'Ġhad', 'Ġbeen', 'Ġraising', 'Ġto', 'Ġhis', 'Ġlips', 'Ġspl', 'ashed', 'Ġall', 'Ġover', 'Ġhis', 'Ġclothing', ';', 'Ġthe', 'Ġglass', 'Ġshattered', 'Ġat', 'Ġhis', 'Ġfeet', '.', 'ĠExt', 'ensive', 'Ġresearch', 'Ġdid', 'Ġnot', 'Ġuncover', 'Ġany', 'Ġevidence', 'Ġthat', 'Ġthe', 'ĠU', '.', 'S', '.', 'Ġcopyright', 'Ġon', 'Ġthis', 'Ġpublication', 'Ġwas', 'Ġrenewed', '.]']\n",
            "7 50276\n",
            "{'input_ids': [0, 2264, 21, 5, 1219, 13, 5, 765, 313, 2934, 220, 7, 5, 1763, 116, 50276, 970, 21, 10, 765, 313, 2934, 220, 7, 5, 1763, 578, 26840, 6, 25, 144, 604, 8, 390, 58, 11, 14, 86, 6, 2446, 7, 5, 2866, 61, 115, 1690, 4097, 160, 30512, 6, 600, 45, 744, 578, 4297, 19, 117, 97, 5890, 2166, 21484, 6, 13, 4136, 3012, 56, 45, 20218, 63, 4520, 4198, 9, 5, 41566, 3220, 4, 7632, 11, 5, 929, 21, 2542, 9, 5, 380, 664, 313, 6, 8, 144, 9, 5, 5868, 1455, 58, 31379, 2650, 6, 13, 37, 7521, 1003, 35561, 8, 46553, 12106, 6, 25, 114, 39, 2772, 1937, 58, 615, 7, 146, 123, 10295, 7, 1268, 4, 407, 222, 5, 1109, 12, 29542, 1816, 23, 39, 526, 6, 8, 98, 222, 5, 29470, 10338, 36423, 313, 11, 5, 12339, 3235, 54, 21, 2494, 106, 31, 10, 13939, 11, 5, 2797, 4, 978, 37, 21, 45, 129, 10, 1195, 11355, 410, 313, 6, 53, 67, 10, 1195, 10861, 65, 578, 368, 23, 513, 37, 1299, 37, 21, 6, 61, 21, 99, 26179, 4, 22, 1185, 531, 1157, 162, 7, 582, 110, 8143, 1087, 60, 29757, 26, 6, 602, 66, 39, 14952, 8, 37213, 484, 1361, 2775, 396, 29626, 7, 356, 23, 106, 4, 1648, 5, 1816, 19, 123, 21, 1197, 36844, 6, 13, 79, 21, 21090, 7, 2329, 11264, 2864, 6, 8, 220, 7, 8719, 11647, 1120, 79, 21, 818, 7945, 12, 3690, 4, 287, 13, 5, 40595, 7110, 1069, 1536, 578, 405, 21, 10, 481, 2003, 578, 10010, 58, 8315, 36530, 6, 187, 7, 106, 70, 604, 58, 10512, 31667, 8, 10209, 2050, 33386, 4735, 42396, 4, 13048, 12984, 870, 33051, 5733, 21770, 8837, 3755, 448, 25596, 30, 5267, 846, 2571, 646, 19163, 438, 44260, 18, 6068, 35, 152, 364, 29015, 21, 2622, 31, 5325, 4662, 35320, 902, 24649, 4, 20, 4076, 37, 56, 57, 3282, 7, 39, 14638, 11743, 9512, 70, 81, 39, 5296, 131, 5, 4049, 17306, 23, 39, 1730, 4, 19188, 17355, 557, 222, 45, 20489, 143, 1283, 14, 5, 121, 4, 104, 4, 4857, 15, 42, 5362, 21, 7867, 21838, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "15\n",
            "{'input_ids': [0, 2264, 21, 5, 1219, 13, 5, 765, 313, 2934, 220, 7, 5, 1763, 116, -100, 970, 21, 10, 765, 313, 2934, 220, 7, 5, 1763, 578, 26840, 6, 25, 144, 604, 8, 390, 58, 11, 14, 86, 6, 2446, 7, 5, 2866, 61, 115, 1690, 4097, 160, 30512, 6, 600, 45, 744, 578, 4297, 19, 117, 97, 5890, 2166, 21484, 6, 13, 4136, 3012, 56, 45, 20218, 63, 4520, 4198, 9, 5, 41566, 3220, 4, 7632, 11, 5, 929, 21, 2542, 9, 5, 380, 664, 313, 6, 8, 144, 9, 5, 5868, 1455, 58, 31379, 2650, 6, 13, 37, 7521, 1003, 35561, 8, 46553, 12106, 6, 25, 114, 39, 2772, 1937, 58, 615, 7, 146, 123, 10295, 7, 1268, 4, 407, 222, 5, 1109, 12, 29542, 1816, 23, 39, 526, 6, 8, 98, 222, 5, 29470, 10338, 36423, 313, 11, 5, 12339, 3235, 54, 21, 2494, 106, 31, 10, 13939, 11, 5, 2797, 4, 978, 37, 21, 45, 129, 10, 1195, 11355, 410, 313, 6, 53, 67, 10, 1195, 10861, 65, 578, 368, 23, 513, 37, 1299, 37, 21, 6, 61, 21, 99, 26179, 4, 22, 1185, 531, 1157, 162, 7, 582, 110, 8143, 1087, 60, 29757, 26, 6, 602, 66, 39, 14952, 8, 37213, 484, 1361, 2775, 396, 29626, 7, 356, 23, 106, 4, 1648, 5, 1816, 19, 123, 21, 1197, 36844, 6, 13, 79, 21, 21090, 7, 2329, 11264, 2864, 6, 8, 220, 7, 8719, 11647, 1120, 79, 21, 818, 7945, 12, 3690, 4, 287, 13, 5, 40595, 7110, 1069, 1536, 578, 405, 21, 10, 481, 2003, 578, 10010, 58, 8315, 36530, 6, 187, 7, 106, 70, 604, 58, 10512, 31667, 8, 10209, 2050, 33386, 4735, 42396, 4, 13048, 12984, 870, 33051, 5733, 21770, 8837, 3755, 448, 25596, 30, 5267, 846, 2571, 646, 19163, 438, 44260, 18, 6068, 35, 152, 364, 29015, 21, 2622, 31, 5325, 4662, 35320, 902, 24649, 4, 20, 4076, 37, 56, 57, 3282, 7, 39, 14638, 11743, 9512, 70, 81, 39, 5296, 131, 5, 4049, 17306, 23, 39, 1730, 4, 19188, 17355, 557, 222, 45, 20489, 143, 1283, 14, 5, 121, 4, 104, 4, 4857, 15, 42, 5362, 21, 7867, 21838, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add these back to the original 750 to get back 1000 samples in our training set.\n",
        "train_dataset_original_plus_augmented = concatenate_datasets([train_dataset_remaining, train_dataset_stratified_augmented])\n",
        "\n",
        "print(\"Combined Samples:\", train_dataset_original_plus_augmented.num_rows, \"\\n\")\n",
        "print(\"Augmented Sample:\", train_dataset_original_plus_augmented[-1])\n",
        "\n",
        "# Lastly, shuffle the dataset again.\n",
        "train_dataset_original_plus_augmented.shuffle(seed=RANDOM_SEED)\n"
      ],
      "metadata": {
        "id": "VUjoH3pzTajx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef65fcca-e906-4485-bb17-c76649e08bf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined Samples: 1000 \n",
            "\n",
            "Augmented Sample: {'document': '[Ask&Answer][Mask]Extensive research did not uncover any evidence that the U.S. copyright on this publication was renewed.] Major Winship, after receiving the message, discussed precautions with the three other Americans. \"Is Pinov,\" came the reply. \"Help?\" \" Nyet ,\" said Major Winship, exhausting his Russian. \"Count down. Progress. When—boom?\" \"Is Pinov,\" came the reply. \"Boom! Boom!\" said Major Winship in exasperation. \"Boom!\" said Pinov happily. \"When?\" \"Boom—boom!\" said Pinov. \"Oh, nuts.\" Major Winship cut out the circuit. \"The one that doesn\\'t speak English.\" \"He\\'s done it deliberately,\" said Capt. Wilkins, the eldest of the four Americans. No one bothered to respond. Ultimately, Lt. Chandler said, \"This is a little ridiculous. Rap if you want me.\" He sat transfixed for several minutes. \"Ah, it\\'s all Russian. Jabbering away. I can\\'t tell a thing that\\'s going on.\" \"Static?\" \"Nope.\" \"We\\'ll get static on these things.\" Major Winship shifted restlessly. \"My reefer\\'s gone on th', 'question': 'What did the four of them do?', 'response': 'What did the four of them do?[QSep]Next morning, before the sunlight exploded, the four of them donned their space suits and went and sat outside the dome, waiting. Extensive research did not uncover any evidence that the U.S. copyright on this publication was renewed.] \"Will you please request the general to keep us informed on the progress of the countdown?\" A moth\\'s wing of dust would, perhaps, rise and settle beyond the horizon: no more. [Transcriber\\'s Note: This etext was produced from Worlds of If Science Fiction, September 1962. General Finogenov notified Major Winship that the underground blast was scheduled for the following morning. \"They\\'ve got Pinov on emergency watch this morning,\" he explained to the other Americans. Black pools of shadows lay in harsh contrast, their edges drawn with geometric precision. The Winning of the Moon BY KRIS NEVILLE The enemy was friendly enough. Major Winship, after receiving the message, discussed precautions with the three other Americans. In the airless void of the moon, the blast itself would be silent. They sat for a while in silence while the shadows evaporated. Trouble was—their friendship was as dangerous as their hate! The sun rose with its bright, silent clap of radiance. \"How are we going to know when it\\'s over?\" I can\\'t tell a thing that\\'s going on.\" Major Winship attempted unsuccessfully to communicate with Base Gagarin. One by one they clicked on their cooling systems. Ultimately, Lt. Chandler said, \"This is a little ridiculous. I\\'m going to switch over to their channel. A small infinity seemed to pass very slowly. \" Nyet ,\" said Major Winship, exhausting his Russian.', 'response_length': None, '__index_level_0__': None}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['document', 'question', 'response', 'response_length', '__index_level_0__'],\n",
              "    num_rows: 1000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "!pwd\n",
        "!mkdir -p ./datasets\n",
        "\n",
        "# Get the current time in the US Pacific time zone.\n",
        "timezone_obj = ZoneInfo(\"America/Los_Angeles\")\n",
        "current_time = datetime.now(timezone_obj)\n",
        "current_time = current_time.strftime(\"%Y-%m-%d_%H%M%S\")\n",
        "\n",
        "dataset_name = \"models_socraticpretraining_augmented_train-\" + str(current_time) + \".json\"\n",
        "train_dataset_original_plus_augmented.to_json(f\"./datasets/{dataset_name}\")\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir -p \"/content/drive/MyDrive/DS266/project/datasets\"\n",
        "!cp ./datasets/{dataset_name} \"/content/drive/MyDrive/DS266/project/datasets/{dataset_name}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "ea29d171bfae465789d717e084a06c1b",
            "b3d713dc79a54c45be11f4cc19d31783",
            "518cae36cf6146e9b1d644c2ce0498b9",
            "99f82a1a5c234693bb57fd9bb7e8d0fe",
            "01c2d7a5fc2a4019acc24d884cb75e9b",
            "ec824b27eb444cefa7208d4a36f6b04a",
            "93224143daf947a6be87bcfbd6f02223",
            "4e75bf5f198b40a9a2422fed8994c455",
            "7cc429d96875474496db018482b0de25",
            "6de09f455fe548839d3d3ffcc9401356",
            "913b02283a9a471d977de6da7b4b6c8b"
          ]
        },
        "id": "hOjcHEQ_z6RQ",
        "outputId": "61697322-ad72-482c-84e7-2a1c49600fbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DS266-ugarcia-bjulve\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea29d171bfae465789d717e084a06c1b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment to test loading.\n",
        "recover_dataset = load_dataset(\"json\", data_files=f\"/content/drive/MyDrive/DS266/project/datasets/{dataset_name}\", split=\"train\")\n",
        "print(dataset_name, \"\\n\")\n",
        "print(recover_dataset)\n",
        "\n",
        "for i in range(2):\n",
        "  sample = recover_dataset[i]\n",
        "  print()\n",
        "  print(\"D:\", sample[\"document\"][:10], \"| Q:\", sample[\"question\"], \"| R:\", sample[\"response\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzGtAtog2ppV",
        "outputId": "c3008a7d-7a98-46f6-e398-980c97305703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models_socraticpretraining_augmented_train-2025-07-28_092936.json \n",
            "\n",
            "Dataset({\n",
            "    features: ['document', 'question', 'response', 'response_length', '__index_level_0__'],\n",
            "    num_rows: 1000\n",
            "})\n",
            "\n",
            "D: CALL HIM N | Q: What pattern does Stevenson notice in the crimes that makes him suspicious? | R: In all three incidents that take place in the story, the criminals were stopped and caught by the police. They all seemed to be mysteriously burned in one way or another: the tires on the car melted off, Higgins' hands were burned by the rifle, and the jackets and weapons of the gang members seemed to have the same effect. Additionally, all three events were tagged by \"The Scorpion\": the words were branded on the car, the rifle, and the jackets.\n",
            "\n",
            "D: CALL HIM N | Q: What is the relationship between Stevenson and Hanks? | R: Hanks is the Precinct Captain, while Stevenson works under him as a Detective-Sergeant. Hanks and Stevenson share a good working relationship; however, Hank gets annoyed whenever Stevenson brings up his theories about “The Scorpion” and thinks of them as nonsense. He refutes every point realistically, saying that Higgins burned the words onto the rifle himself. When Stevenson brings up the earlier robbery incident, Hanks refuses to accept those observations and says Stevenson’s points are similar to that of a comic book. During the schoolyard incident, Hank is extremely annoyed at Stevenson trying to connect more points to ‘The Scorpion’ and tells him that the children just had a silly brawl. He also warns Stevenson to stop thinking about these foolish ideas and return to doing his job in the precinct.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "1f4jbSQPeEk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the pretrained model and prepare it for QLoRA.\n",
        "# We'll use the quantized version of the model for\n",
        "# PEFT.\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    load_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "socratic_checkpoint_name = \"Salesforce/squality-socratic-books-30M\"\n",
        "socratic_model_quantized = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    socratic_checkpoint_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0})\n",
        "socratic_tokenizer_quantized = AutoTokenizer.from_pretrained(socratic_checkpoint_name)\n",
        "socratic_model_config_quantized = AutoConfig.from_pretrained(socratic_checkpoint_name)\n",
        "\n",
        "# Add special tokens for Socratic FT.\n",
        "special_tokens = [\"[Ask&Answer]\", \"[Mask]\", \"[QSep]\"]\n",
        "num_added_tokens = socratic_tokenizer_quantized.add_special_tokens({'additional_special_tokens': special_tokens})\n",
        "print(f\"Added {num_added_tokens} tokens to the tokenizer.\")\n",
        "\n",
        "# Resize the model accordingly.\n",
        "print(\"Resized model vocab:\", socratic_model_config_quantized.vocab_size, len(socratic_tokenizer_quantized))\n",
        "socratic_model_quantized.resize_token_embeddings(max(\n",
        "    len(socratic_tokenizer_quantized),\n",
        "    socratic_model_config_quantized.vocab_size))\n",
        "\n",
        "# Prepare for QLoRA.\n",
        "socratic_model_quantized = prepare_model_for_kbit_training(socratic_model_quantized)\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"k_proj\", \"v_proj\", \"q_proj\", \"o_proj\"],\n",
        "    trainable_token_indices={'embed_tokens': socratic_tokenizer_quantized.convert_tokens_to_ids(special_tokens)},\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_2_SEQ_LM\",\n",
        "    modules_to_save=[\"lm_head\"]\n",
        ")\n",
        "\n",
        "socratic_model_quantized = get_peft_model(socratic_model_quantized, config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377,
          "referenced_widgets": [
            "1764baf51c8747e0b406a6befce97dce",
            "b59d5a701bc4408f8dad8586dff435be",
            "6ecd881ea8fb4e1d9b0d2dab6bf7bcee",
            "28a7605be2f34c078c4dc8a84843a3c4",
            "6899a11b3b654480b2f735a61ef6ecb7",
            "06be7c8d336a49d8ab0fb3fe2484642e",
            "33f4c791d556451e972c60c3b3bd033d",
            "9b57c9f5a2054f958217715a80bd34d0",
            "b275396ab4474c06a72a20d5218f29fe",
            "eb293a52af9c462799d8a69b104c4a71",
            "6cf74f550e0d49dabece17ea3cfde5c0",
            "3b53ca49b9a34ed0bfcefeeceaccb3f5",
            "84a589142569415b860d7098f0552903",
            "643e3bce7242430ab5f1ff24dad7351b",
            "547768a865e44deea6ec97b2ad364428",
            "5c481063c2f344fb8de865e6a254cc4b",
            "fb2586da9aaa4e888dc96f86e8d71ec4",
            "3d27e9de18b6483182964f6432c286df",
            "1c592d90d7824029a28dcb3d07794301",
            "daedba7f297d42c3b617399332f578b5",
            "767bcde8627b4adab6eb9bd57eaf7a60",
            "11e1c794dbb44dba97371a56d6253124",
            "32f2bde7dc67485bb24e18e6c0b8e43f",
            "993f779f0c3747799be4372953a24df8",
            "08dd9f543ae445bab7a993c1e3aeac41",
            "028b92b56ff54c07b88cc4dff2faccbf",
            "35d589dc557c439d95d80a5623e47c6b",
            "1144291ecfd64cfb84eb58f6ea47c45e",
            "cc4f4497d36b41f792921fe879fc33e2",
            "4576692865334274ae7adeabf7a21b9d",
            "835a496eb2944b92b80b02cb4f1ecff0",
            "5a5049f1a1d14ae984ad257cd49cfbdf",
            "a14f236f2a9248e0a9dc4802995c1d2f",
            "119985ebae29405f89cb9b8af67aae4d",
            "7d50ca4912464048814adb11f0c697eb",
            "44687d0f1ddb41a189cc1d53aec0d37c",
            "a9c6b38c4e86434b82e36c4fdb935131",
            "fe0f416293d84c66ae09c9442ef19874",
            "e5bea8acd6ff49e29d4cb37d3e8f39b7",
            "16bf18179cfa426eab58e1b2ba1432f5",
            "91464a98c28443cbbc9939d06175fca9",
            "ab13a8877a0f480c93764d5bc980b6f1",
            "dc78b64376c146b4b92f926a6fc5c50e",
            "2707c7e1983f4cc8bb3330ed994235aa",
            "5a044ffd60a44a269f717bc2bb3f3c92",
            "148a810fcdb347f58e7da5b77be19fd8",
            "f2af408571d5484381eb258da539a4c9",
            "8bce2da5b4b04f7cbedc4d504c48b921",
            "8238e92306764069ab6ffd656cdec781",
            "e9a335830a764e61a28e56152a14ebd3",
            "ed6cb5240ace435f96414b7a2f73d694",
            "317965f1014646538f2a2ff5014551c1",
            "bfd0d362c58c464fbfa1f79cc4700f84",
            "e5a4b2e8a1094417bed3a90925e411b8",
            "f8df1c0035e14d65ae514e1bd19fd357",
            "c7465ebe0ded4430862b7b8671e1f411",
            "303fdda8dc534654b678163e84470873",
            "4f7e55e61cb142c686ba393a6f40b095",
            "e4898b9a26fc4fe1b8a1eab943645d20",
            "1cbadbd04fd646cd80677ef79ce06a12",
            "a83ab99a0ad844fba743fc69babe3f3e",
            "b00fee66cfa9490aab9a08ec20d949cc",
            "99bf5b17d1c14a71901944dae86e8bd3",
            "0dd613db2948407984a9e43751f249c6",
            "118fa11fd1824bbfa457cd4e2f59e405",
            "7110d5850371437f9dd0812065fe4107",
            "4fdb869404d3436685b7c360d7829df6",
            "2eb321b39cad4464a057ca3ffdff87d6",
            "1ee0929c8e714e9fad661b471e2d6ec1",
            "4a30dbce29db4795bacb07b4fc6a4f3c",
            "fe75c90e5577446ab2865b8657219013",
            "a89e66242b204dcb9195a45b1221a04b",
            "311d41f167794ad49b47766785eb0d4d",
            "5ab66ea03b414572bf2e3b8cca98a771",
            "37d15f682a3f4c2e942ee3a627c7cbaf",
            "3297cdb656c543d4b50bff99db773a83",
            "83e7005fd9cd4680bf512ee03988a0a9",
            "bfd053b4733848b7838f47724ffa48f6",
            "456d403156144b7da9281b7f25c48499",
            "c2cec253ca114bc09da32687f17883ac",
            "6d3a7b8d3e964d57899f2631e19948ad",
            "0f2e45cb5a1a4b2da697b5a596bdf131",
            "1f974bca5e534cb1a883c69c459cc636",
            "c63d1ff1aba44b00ad3188431c6f6f52",
            "97247b23d5e94bef86b217ef37210b91",
            "170bf4326f9e4e39886cbd2259851b1b",
            "b8716ce47de24473b4b4cd24bc980da8",
            "28e14a0d35464945bf56c11316f4bf6d",
            "f2da551bdb1442a48479b918070d6fa8",
            "06351468580d42f6b41aee764578f61e",
            "02a2d007578c4a7a83c30a2af7f2a891",
            "b05ebdc25f324bea8b061f9c28784f20",
            "3bb04816ec644d13a5b944d45ee52dfd",
            "179ebc68930d468b9bea8c2e29aba863",
            "b24218de3f4f4f439a9a2424091b4c0f",
            "47f5d59d432c4e38a0a7ee9e43828d99",
            "b2cb240c1d394659855b510a929be297",
            "fd1a298b4a564965bfa977d08995e9c8",
            "473443dd5b8a4e9f9efc5374b7941361"
          ]
        },
        "id": "WIBil0nBz4ZK",
        "outputId": "0af07c88-9e6c-4c31-8922-c3db1d4e0b2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1764baf51c8747e0b406a6befce97dce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3b53ca49b9a34ed0bfcefeeceaccb3f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "32f2bde7dc67485bb24e18e6c0b8e43f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/384 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "119985ebae29405f89cb9b8af67aae4d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a044ffd60a44a269f717bc2bb3f3c92"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7465ebe0ded4430862b7b8671e1f411"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4fdb869404d3436685b7c360d7829df6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bfd053b4733848b7838f47724ffa48f6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2da551bdb1442a48479b918070d6fa8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 3 tokens to the tokenizer.\n",
            "Resized model vocab: 50274 50277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the training and eval datasets and prep them for fine tuning.\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = socratic_tokenizer_quantized.model_max_length\n",
        "print(f\"Max sequence length: {MAX_SEQUENCE_LENGTH}\", \"\\n\")\n",
        "\n",
        "def make_question_document_pairs(dataset):\n",
        "    question_document_pairs = []\n",
        "    for document, question in zip(dataset[\"document\"], dataset[\"question\"]):\n",
        "        question_document_pairs.append(f\"<ask&answer> {question} <qsep> {document}\")\n",
        "\n",
        "    return question_document_pairs\n",
        "\n",
        "def preprocess_socratic_batch(dataset, tokenizer):\n",
        "    question_document_pairs = make_question_document_pairs(dataset)\n",
        "\n",
        "    input_encoded = tokenizer.batch_encode_plus(\n",
        "        question_document_pairs,\n",
        "        max_length=MAX_SEQUENCE_LENGTH,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    labels_encoded = tokenizer.batch_encode_plus(\n",
        "        dataset[\"response\"],\n",
        "        max_length=MAX_SEQUENCE_LENGTH,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    # Need to discount the [QSep] token so it doesn't affect the\n",
        "    # loss function.\n",
        "    qsep_index = socratic_tokenizer_quantized.all_special_tokens.index(\"[QSep]\")\n",
        "    qsep_token_id = socratic_tokenizer_quantized.all_special_ids[qsep_index]\n",
        "    for label_ids in labels_encoded['input_ids']:\n",
        "        label_ids[label_ids == qsep_token_id] = -100\n",
        "\n",
        "    return {'input_ids': input_encoded['input_ids'],\n",
        "            'labels': labels_encoded['input_ids']}\n",
        "\n",
        "train_encoded = train_dataset_original_plus_augmented.map(\n",
        "    preprocess_socratic_batch,\n",
        "    batched=True,\n",
        "    fn_kwargs={\n",
        "      'tokenizer': socratic_tokenizer_quantized\n",
        "})\n",
        "\n",
        "val_encoded = dev_dataset.map(\n",
        "    preprocess_socratic_batch,\n",
        "    batched=True,\n",
        "    fn_kwargs={\n",
        "      'tokenizer': socratic_tokenizer_quantized\n",
        "})\n",
        "\n",
        "print()\n",
        "print(\"Train encoded:\", train_encoded, \"\\n\")\n",
        "print(\"Val encoded:\", val_encoded, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327,
          "referenced_widgets": [
            "d28458ab593547d68468fae8ac4ec1a4",
            "ec59a70253504771bc35f9a7e7a9b38d",
            "a685e8711d25441098f54c7d26de9f69",
            "bf1bc663f86840cea0b8770be2457758",
            "dd448fa8350c4a7a81da81473c3b6382",
            "1cda7c3c6ab244a4adcc017afdb8965e",
            "0f9564c4d8e84dc2b3eb7b1f627e6545",
            "71639c9b94c9411487264461bc819fb1",
            "1324ab9d069e47fb90b9a0d140435068",
            "f3c3f637154344f3bb8403a4cb7ce7bc",
            "9a5cdd4e25b14408b7f8f50e166cc006",
            "e36577ffcb164bdaaa9c9e74e3ab3870",
            "d4dfd5751f0e4e27b4e1fcb12b3d5337",
            "f3a79424da7b4b92ad637d0729c7103e",
            "5f67c31ec26f4fcd97256e74aff70cde",
            "6f7438e55c9c4f3a94da473aaed17cb7",
            "d0869f34d58f4c5c85b85c9278588615",
            "0033c809a5964207a1c692a9ff99f540",
            "0c8b3872fb1148e186eee8389177d7ca",
            "edda94b3245a4447ae67f31a5ac916ef",
            "7e4a4c48241742f8a76100b410cfc388",
            "81a41afb78d54ce49b9035c98175bf35"
          ]
        },
        "id": "0tZ-fFLIsuOz",
        "outputId": "147ada42-7914-4ff3-ebf1-f6998234722a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max sequence length: 1024 \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d28458ab593547d68468fae8ac4ec1a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e36577ffcb164bdaaa9c9e74e3ab3870"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train encoded: Dataset({\n",
            "    features: ['document', 'question', 'response', 'response_length', '__index_level_0__', 'input_ids', 'labels'],\n",
            "    num_rows: 1000\n",
            "}) \n",
            "\n",
            "Val encoded: Dataset({\n",
            "    features: ['document', 'question', 'response', 'input_ids', 'labels'],\n",
            "    num_rows: 500\n",
            "}) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_encoded))\n",
        "random_training_sample = random.choice(train_encoded)\n",
        "print(random_training_sample[\"input_ids\"])\n",
        "print(random_training_sample[\"labels\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qa-35YLiNGO7",
        "outputId": "8fb98966-5ce7-471f-9870-dc55e8a2eeb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n",
            "[0, 50269, 653, 21, 5, 1219, 13, 5, 765, 313, 2934, 220, 7, 5, 1763, 116, 1437, 50266, 1437, 50274, 50275, 40884, 17355, 557, 222, 45, 20489, 143, 1283, 14, 5, 121, 4, 104, 4, 4857, 15, 42, 5362, 21, 7867, 21838, 520, 8642, 4854, 16, 5861, 1455, 6, 1116, 768, 10, 313, 16, 7919, 7, 33, 10, 809, 12984, 4, 20, 39341, 21, 14, 37, 56, 7, 109, 24, 1003, 1666, 8, 39, 809, 74, 45, 11866, 328, 20, 313, 23, 5, 2003, 21, 20135, 19222, 6, 8, 37, 1467, 24, 4, 7632, 11, 5, 929, 21, 2542, 9, 5, 380, 664, 313, 6, 8, 144, 9, 5, 5868, 1455, 58, 31379, 2650, 6, 13, 37, 7521, 1003, 35561, 8, 46553, 12106, 6, 25, 114, 39, 2772, 1937, 58, 615, 7, 146, 123, 10295, 7, 1268, 4, 29757, 4021, 39, 3124, 1810, 11, 65, 9, 39, 21734, 18693, 4, 22, 31535, 6, 9896, 60, 29757, 26, 40154, 5846, 4, 22, 3684, 127, 7684, 4, 370, 531, 905, 162, 907, 47, 10, 5010, 72, 91, 18371, 4075, 7, 5, 33080, 4, 22, 21518, 9, 5, 276, 13, 127, 2598, 12, 397, 259, 72, 20, 11355, 313, 35454, 5134, 32628, 352, 23, 39, 38221, 20580, 19, 10, 21543, 33779, 12359, 30, 5, 1052, 4, 22, 11773, 6, 33, 2512, 10, 92, 3235, 15, 162, 72, 370, 115, 304, 65, 21, 14754, 4, 178, 14, 6, 567, 15, 299, 9, 8719, 11647, 1120, 18, 8694, 2772, 6, 21, 350, 203, 4, 20, 11355, 313, 2738, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "[0, 2264, 21, 5, 1219, 13, 5, 765, 313, 2934, 220, 7, 5, 1763, 116, -100, 970, 21, 10, 765, 313, 2934, 220, 7, 5, 1763, 578, 26840, 6, 25, 144, 604, 8, 390, 58, 11, 14, 86, 6, 2446, 7, 5, 2866, 61, 115, 1690, 4097, 160, 30512, 6, 600, 45, 744, 578, 4297, 19, 117, 97, 5890, 2166, 21484, 6, 13, 4136, 3012, 56, 45, 20218, 63, 4520, 4198, 9, 5, 41566, 3220, 4, 7632, 11, 5, 929, 21, 2542, 9, 5, 380, 664, 313, 6, 8, 144, 9, 5, 5868, 1455, 58, 31379, 2650, 6, 13, 37, 7521, 1003, 35561, 8, 46553, 12106, 6, 25, 114, 39, 2772, 1937, 58, 615, 7, 146, 123, 10295, 7, 1268, 4, 407, 222, 5, 1109, 12, 29542, 1816, 23, 39, 526, 6, 8, 98, 222, 5, 29470, 10338, 36423, 313, 11, 5, 12339, 3235, 54, 21, 2494, 106, 31, 10, 13939, 11, 5, 2797, 4, 978, 37, 21, 45, 129, 10, 1195, 11355, 410, 313, 6, 53, 67, 10, 1195, 10861, 65, 578, 368, 23, 513, 37, 1299, 37, 21, 6, 61, 21, 99, 26179, 4, 22, 1185, 531, 1157, 162, 7, 582, 110, 8143, 1087, 60, 29757, 26, 6, 602, 66, 39, 14952, 8, 37213, 484, 1361, 2775, 396, 29626, 7, 356, 23, 106, 4, 1648, 5, 1816, 19, 123, 21, 1197, 36844, 6, 13, 79, 21, 21090, 7, 2329, 11264, 2864, 6, 8, 220, 7, 8719, 11647, 1120, 79, 21, 818, 7945, 12, 3690, 4, 287, 13, 5, 40595, 7110, 1069, 1536, 578, 405, 21, 10, 481, 2003, 578, 10010, 58, 8315, 36530, 6, 187, 7, 106, 70, 604, 58, 10512, 31667, 8, 10209, 2050, 33386, 4735, 42396, 4, 13048, 12984, 870, 33051, 5733, 21770, 8837, 3755, 448, 25596, 30, 5267, 846, 2571, 646, 19163, 438, 44260, 18, 6068, 35, 152, 364, 29015, 21, 2622, 31, 5325, 4662, 35320, 902, 24649, 4, 20, 4076, 37, 56, 57, 3282, 7, 39, 14638, 11743, 9512, 70, 81, 39, 5296, 131, 5, 4049, 17306, 23, 39, 1730, 4, 19188, 17355, 557, 222, 45, 20489, 143, 1283, 14, 5, 121, 4, 104, 4, 4857, 15, 42, 5362, 21, 7867, 21838, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning"
      ],
      "metadata": {
        "id": "DpbWte_zCyj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training args and other parameters.\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"outputs\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    optim=\"paged_adamw_8bit\", #used with QLoRA\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=5,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to='none',\n",
        "    label_names=[\"labels\"]\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=socratic_tokenizer_quantized,\n",
        "    model=socratic_model_quantized)\n",
        "\n",
        "metric = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "\n",
        "    # decode preds and labels\n",
        "    labels = np.where(labels != -100, labels, socratic_tokenizer_quantized.pad_token_id)\n",
        "    decoded_preds = socratic_tokenizer_quantized.batch_decode(preds, skip_special_tokens=True)\n",
        "    decoded_labels = socratic_tokenizer_quantized.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # rougeLSum expects newline after each sentence\n",
        "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    return result\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=socratic_model_quantized,\n",
        "    args=training_args,\n",
        "    train_dataset=train_encoded,\n",
        "    eval_dataset=val_encoded,\n",
        "    processing_class=socratic_tokenizer_quantized,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jb6UTBGnCsRy",
        "outputId": "5041ca6a-c694-494b-8b1e-3950a834cf45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model.\n",
        "os.environ['WANDB_MODE'] = 'disabled'\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "1AtBVdZEFnHz",
        "outputId": "96877bb4-92ee-44f8-e37a-8bc4f61c1db0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [625/625 14:01, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Rougelsum</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>9.517300</td>\n",
              "      <td>9.487814</td>\n",
              "      <td>0.071341</td>\n",
              "      <td>0.021392</td>\n",
              "      <td>0.060741</td>\n",
              "      <td>0.068141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>5.361100</td>\n",
              "      <td>4.730658</td>\n",
              "      <td>0.076797</td>\n",
              "      <td>0.023324</td>\n",
              "      <td>0.063846</td>\n",
              "      <td>0.072565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.062300</td>\n",
              "      <td>1.512324</td>\n",
              "      <td>0.079199</td>\n",
              "      <td>0.024260</td>\n",
              "      <td>0.065942</td>\n",
              "      <td>0.074802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.306400</td>\n",
              "      <td>1.003995</td>\n",
              "      <td>0.081072</td>\n",
              "      <td>0.025080</td>\n",
              "      <td>0.067210</td>\n",
              "      <td>0.076498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.956200</td>\n",
              "      <td>0.947386</td>\n",
              "      <td>0.081070</td>\n",
              "      <td>0.024393</td>\n",
              "      <td>0.067094</td>\n",
              "      <td>0.076504</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=625, training_loss=5.054167267608642, metrics={'train_runtime': 842.1414, 'train_samples_per_second': 5.937, 'train_steps_per_second': 0.742, 'total_flos': 1.252600578048e+16, 'train_loss': 5.054167267608642, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!rm -rf ./models/socraticpretraining_baseline-2025-07-26_215517/"
      ],
      "metadata": {
        "id": "1wwMnjrjPqGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "!pwd\n",
        "!mkdir -p ./models\n",
        "\n",
        "# Get the current time in the US Pacific time zone.\n",
        "timezone_obj = ZoneInfo(\"America/Los_Angeles\")\n",
        "current_time = datetime.now(timezone_obj)\n",
        "current_time = current_time.strftime(\"%Y-%m-%d_%H%M%S\")\n",
        "\n",
        "model_name = \"socraticpretraining_augmented-\" + str(current_time)\n",
        "trainer.save_model(f\"./models/{model_name}\")\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir -p \"/content/drive/MyDrive/DS266/project/models/{model_name}\"\n",
        "!cp -r ./models/{model_name}/* \"/content/drive/MyDrive/DS266/project/models/{model_name}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZDSaWiZLnsb",
        "outputId": "e894e937-4eb8-470a-ea32-3c4315bfc4d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DS266-ugarcia-bjulve\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:252: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    }
  ]
}
